<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="这里记录了我在编程时遇到的一些问题，和解决这些问题的一些方法。还有一些是我对编程的一些看法观点，以及学习语言时的一些笔记和心得体会。"><title>梯度下降算法 (Gradient Descent) | DeepCode</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><meta name="generator" content="Hexo 4.2.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">梯度下降算法 (Gradient Descent)</h1><a id="logo" href="/.">DeepCode</a><p class="description">高岸为谷，深谷为陵</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/tags/"><i class="fa fa-tag"> 标签</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">梯度下降算法 (Gradient Descent)</h1><div class="post-meta">Dec 30, 2019<span> | </span><span class="category"><a href="/categories/%E6%8A%80%E6%9C%AF/">技术</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 1.3k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 4</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>已知了代价函数：<br>$$ J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=0}^m(h_\theta(x^{(i)})-y^{(i)})^2 $$</p>
<p>我们需要一个算法来最小化 $J(\theta_0, \theta_1)$，而梯度下降算法可以解决这个问题。</p>
<p>梯度下降算法不仅可以应用于多种函数的求解，不仅限于线性回归问题。<br>梯度下降算法可以解决更一般的问题，例如 $J(\theta_0,\theta_1,\ldots,\theta_n)$ 求解该代价函数的最小值。</p>
<h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><p>回到线性回归上来，还是使用 $J(\theta_0, \theta_1)$ 来说明。</p>
<ul>
<li>将$\theta_0, \theta_1$设置一个初始值，可以是任意值，但通常会设置成0；</li>
<li>不断的改变$\theta_0, \theta_1$的值，让$J(\theta_0, \theta_1)$的值一直减小，直到找到$J$的最小值或局部最小值。</li>
</ul>
<p><strong>工作流程</strong></p>
<p><img src="/images/ml_5.jpg" alt="" title="流程1"></p>
<p>这是一个随 $\theta_0, \theta_1$ 变化而导致 $J$ 变化的图表。我们希望最小化这个函数，根据流程，先初始化一个$\theta_0, \theta_1$的值。</p>
<p>把这个图像想像成一座上，初始化的试过就是把你放在山的某处，现在你要下到山的最下面，也就是山谷底。<br>现在把自己旋转360度，寻找一个最快下山的方向，然后迈出一步，接着重复上次的过程继续找一个最快下山的方向迈出一步，重复上面的步骤，迈出一步又一步，直到一个局部最低点的位置。</p>
<p>下降的轨迹<br><img src="/images/ml_6.gif" alt="" title="动态1"></p>
<p>当初始位置向右偏移一点，下降的轨迹就不一样了。<br><img src="/images/ml_7.gif" alt="" title="动态2"></p>
<p>这样我们得到了两个局部最低点，当初始位置不同时会得到不同的局部最优解，这是梯度下降算法的一个特点。</p>
<p>取得下降方向的方法就是取代价函数的导数（即一个函数的切线），切线的斜率就是那个点的导数。当斜率趋向于0时就说明代价函数的值在下降。而每一步的步长则是由学习率（learning rate）$\alpha$ 来表示。</p>
<p>例如：上图中每个 “星” 之间的距离就是由参数 $\alpha$ 来确定，当 $\alpha$ 较大时，步长就较大，同样的，$\alpha$较小时，步长就会变小；前进的方向则是由 $J(\theta_0, \theta_1)$ 出的偏导数来决定。根据不同的起点，可能会在不同的终点结束。</p>
<h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>梯度下降算法的定义是：</p>
<p><em>重复下面的步骤直到$J(\theta_0, \theta_1)$收敛</em><br>$$ \theta_j:=\theta_j−\alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1), j \epsilon \left(0, 1\right) $$</p>
<p>$j$ 表示索引，此例中可取0, 1</p>
<p><strong>注意：</strong><br>当改变参数 $\theta_0, \theta_1, \ldots$ 时，需要注意的一点就是要同时改变这些参数，这对于获得正确的代价函数是非常重要的。</p>
<p>$\color{green}{下面这个例子是正确的}$：<br>$temp0:=\theta_0−\alpha \frac{\partial}{\partial \theta_0} J(\theta_0, \theta_1)$<br>$temp1:=\theta_1−\alpha \frac{\partial}{\partial \theta_1} J(\theta_0, \theta_1)$<br>$\theta_0:=temp0$<br>$\theta_1:=temp1$<br>这样的一个步骤是正确的。</p>
<p>$\color{red}{下面这个例子是错误的}$：<br>$temp0:=\theta_0−\alpha \frac{\partial}{\partial \theta_0} J(\theta_0, \theta_1)$<br>$\theta_0:=temp0$<br>$temp1:=\theta_1−\alpha \frac{\partial}{\partial \theta_1} J(\theta_0, \theta_1)$<br>$\theta_1:=temp1$</p>
<p>用一个例子来说明梯度下降算法：</p>
<p>有一个单变量线性回归的预测函数，同时将$\theta_0=0$,这样预测函数就是：$h_\theta(x) = \theta_1x$<br>这样梯度下降算法就是重复计算:<br>$$ \theta_1:=\theta_1−\alpha \frac{\partial}{\partial \theta_1} J(\theta_1) $$</p>
<p><img src="/images/ml_8.jpg" alt=""></p>
<p>红线就代表的代价函数的导数，即在这一点的斜率，上图的点上我们可以知道 $\frac{\partial}{\partial \theta_1} J(\theta_1) \geq 0$，<br>代入式子中，我们可以得到$\theta_1$在变小，向左靠拢。</p>
<p><img src="/images/ml_9.jpg" alt=""></p>
<p>当初始 $\theta_1$ 在左边，它的斜率是负数，我们可以得到$\theta_1$在变大，向右靠拢。</p>
<p>当接近J函数的最底部时，$\frac{\partial}{\partial \theta_1} J(\theta_1)$ 接近0。当到最理想的情况下，$\frac{\partial}{\partial \theta_1} J(\theta_1) = 0$，在这张情况下我们可以得到：$\theta_1:=\theta_1−\alpha*0$。即$\theta_1$ 不会再变化。</p>
<p>由上图可知，当学习率$\alpha$ 过小时，我们需要经过很多步才能到达最低点，耗时会更长。而当$\alpha$ 过大时，可能会越过最低点，导致无法收敛</p>
<p><img src="/images/ml_10.jpg" alt=""></p>
<p>总结一下：当越接近局部最小值的地方时，偏导数 $\frac{\partial}{\partial \theta_1} J(\theta_1)$ 就越小，在$\alpha$不变的情况下，$\theta_1$ 下降的步子也就越来越小，当达到局部最优时，$\theta_1$就不变了，因此，我们不需要减小$\alpha$的值。</p>
<h2 id="应用到线性回归问题上"><a href="#应用到线性回归问题上" class="headerlink" title="应用到线性回归问题上"></a>应用到线性回归问题上</h2><p>已知线性回归的预测函数：<br>$$ h_\theta(x) = \theta_0 + \theta_1x $$</p>
<p>将预测函数代入到代价函数中并应用数学求导，可得到：</p>
<p>当j=0时:<br>$$\theta_0:=\theta_0 - \alpha \frac{1}{m} \sum_{i=0}^m(h_\theta(x_i)-y_i)$$<br>当j=1时：<br>$$\theta_1:=\theta_1 - \alpha \frac{1}{m} \sum_{i=0}^m((h_\theta(x_i)-y_i)x_i)$$</p>
<p>在线性回归问题中，我们从一个假设的$\theta_0, \theta_1$开始，重复应用这些梯度下降方程，之后我们的预测函数就会越来越精确。</p>
</div><div class="tags"><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a href="/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/">监督学习</a><a href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">线性回归</a></div><div class="post-nav"><a class="pre" href="/2020/01/03/tech/machine_learning/multiple_features_for_linear_regression.html">多变量线性回归问题</a><a class="next" href="/2019/12/29/tech/machine_learning/linear_regression_cost_function.html">线性回归代价函数 (Cost Function)</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8A%80%E6%9C%AF/">技术</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/%E9%A9%AC%E6%8B%89%E6%9D%BE/" style="font-size: 10px;">马拉松</a> <a href="/tags/HTTP/" style="font-size: 10px;">HTTP</a> <a href="/tags/mysql/" style="font-size: 11.25px;">mysql</a> <a href="/tags/javascript/" style="font-size: 18.75px;">javascript</a> <a href="/tags/bootstrap/" style="font-size: 10px;">bootstrap</a> <a href="/tags/HTML/" style="font-size: 11.25px;">HTML</a> <a href="/tags/vue/" style="font-size: 10px;">vue</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 20px;">机器学习</a> <a href="/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 16.25px;">监督学习</a> <a href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" style="font-size: 12.5px;">逻辑回归</a> <a href="/tags/%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">非监督学习</a> <a href="/tags/%E7%AE%97%E6%B3%95%E4%BC%98%E5%8C%96/" style="font-size: 12.5px;">算法优化</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" style="font-size: 13.75px;">线性回归</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" style="font-size: 12.5px;">神经网络</a> <a href="/tags/SVM/" style="font-size: 11.25px;">SVM</a> <a href="/tags/PHP/" style="font-size: 10px;">PHP</a> <a href="/tags/python/" style="font-size: 17.5px;">python</a> <a href="/tags/pandas/" style="font-size: 13.75px;">pandas</a> <a href="/tags/django/" style="font-size: 10px;">django</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="font-size: 12.5px;">数据结构</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020/05/24/tech/python/structure/str-search-and-kmp.html">字符串朴素匹配算法和KMP算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/03/tech/python/structure/josephus-solve.html">约瑟夫（Josephos）问题以及解法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/31/tech/python/structure/link-intro.html">链表介绍及python的实现</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/09/tech/machine_learning/learn_large_datasets.html">在大数据下应用机器学习算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/08/tech/machine_learning/mini_batch_gradient_descent.html">小批量梯度下降算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/08/tech/machine_learning/stochastic_gradient_descent.html">随机梯度下降算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/08/tech/machine_learning/recommender_system.html">推荐系统和协同过滤算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/07/tech/machine_learning/anomaly_detection.html">异常检测（Anomaly Detection）</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/06/tech/machine_learning/dimensionality_reduction.html">PCA降维算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/06/tech/machine_learning/k_means_intro.html">K均值算法介绍</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://www.jianshu.com/u/f133ca80001f" title="爱吃鱼的夏侯莲子" target="_blank">爱吃鱼的夏侯莲子</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2020 <a href="/." rel="nofollow">DeepCode</a>&nbsp;|&nbsp;<a href="http://www.beian.miit.gov.cn/" target="_blank" rel="noopener">苏ICP备20021898号-1</a>&nbsp;|&nbsp;<img class="nofancybox" src="../images/beian/icon.png" style="margin-bottom: -4px;"/><a href="http://www.beian.gov.cn/" target="_blank" rel="noopener">苏公网安备 32059002003042号</a><div class="ss"> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>