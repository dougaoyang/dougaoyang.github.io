---
layout: post
title: 线性回归代价函数 (Cost Function)
date: 2019-12-29 14:00
categories:
- 技术
description: 了解代价函数的作用，以及通过最小化代价函数来得到最优的预测函数...
mathjax: true
---

回到房价的例子：

单变量线性回归的预测函数：
$$ h_\theta(x) = \theta_0 + \theta_1x $$

要得出这个函数就要知道$\theta_0$ 和 $\theta_1$这两个值。

而想要让预测 $h_\theta(x)$ 更加准确，就需要 $h_\theta(x)$ 和 真实值 $y$ 之间的差距越小，所以我们需要做的就是得到一个 $\theta_0$ 和 $\theta_1$ 让预测函数的到的值与 $y$ 之间的差异最小，这样就得到了一个满意的预测函数了。

我们可以用平方差公式来表示两组数之间的差异度，方程为：

$$ J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=0}^m(h_\theta(x^{(i)})-y^{(i)})^2 $$

>PS: 此处平方差公式原本应当是除以 m,这里除以 2m 只为了以后在数学上计算方便，这里除以 m 和 除以 2m 对获得最小的代价函数值没有影响。

$J(\theta_0, \theta_1)$被成为代价函数（cost function），这是回归问题中最常使用的方法.

现在要做的就是得到使 $J(\theta_0, \theta_1)$ 最小的 $\theta_0$ 和 $\theta_1$

## 最小化$J(\theta_0, \theta_1)$

为了更好的理解最小化的过程，先假设 $\theta_0$ = 0，这样就简化了预测函数$h_\theta(x) = \theta_1x$

将 $h_\theta(x)$ 带入 代价函数， 我们可以得到：
$$ J(\theta_1) = \frac{1}{2m} \sum_{i=0}^m(\theta_1x^{(i)}-y^{(i)})^2 $$

假设我们只有一组数据样本$(x^{(1)}, y^{(1)}) = (1, 2)$，继续简化代价函数：
$$ J(\theta_1) = \frac{1}{2} (\theta_1-2)^2 $$

![J随$\theta_1$变化][1]

当使用不同的$\theta_1$ 带入公式时，我们可以的到这样的一个曲线图。从图中可以清楚的看到，当$\theta_1=2$时 $J$ 的返回值最小。

当$\theta_0$ 不为0时，我们就要根据$\theta_0$ 和 $\theta_1$两个变量来确定 $J$，这在图表上是一个三维的表现

![J随$\theta_0$, $\theta_1$变化][2]


除了通过图表直观的看出最小值，从数学上获得一个函数的最小值可以使用的方法就是求这个函数的导数，当导数为0时，就得到的函数的最小值。


[1]: /images/ml_3.jpg "模拟数据"
[2]: /images/ml_4.jpg "三维"
