---
layout: post
title: 特征值缩放
date: 2020-01-03 22:00
categories:
- 技术
tags:
- 机器学习
- 算法优化
description: 了解特征值缩放的两种方法，以及特征值缩放对梯度下降算法的帮助。
mathjax: true
---

下面用房价的例子来说明一下梯度下降算法。

**假设训练集中有四个训练样本：**
$x=\left[ \begin{matrix} 2104 & 5 & 45 \\\ 1416 & 3 & 40 \\\ 1534 & 3 & 30 \\\ 852 & 2 & 36 \end{matrix} \right], y=\left[ \begin{matrix} 460 \\\ 232 \\\ 315 \\\ 178 \end{matrix}\right]$

**假设预测函数为：**
$ h_\theta(x) = \theta_0x_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3 $
代价函数为：
$ J(\theta) = \frac{1}{2m} \sum_{i=0}^m(h_\theta(x^{(i)})-y^{(i)})^2 $

**梯度下降算法为：**
重复直到$J(\theta)$收敛 {
$ \theta_j:=\theta_j - \alpha \frac{1}{m} \sum_{i=0}^m((h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}), j \epsilon \left(0, 1, 2,...,n\right) $
}

**设置初始值：** $\theta_0=0,\theta_1=0,\theta_2=0,\theta_3=0$; 学习率$\alpha=0.01$
**初始的梯度值为** $J(\theta)=4.9542\times10^{4}$

进行**第一步梯度下降**：
$\theta_0:=0-0.01\times((0-460)\times1+(0-232)\times1+(0-315)\times1+(0-178)\times1)／4 = 2.9625$
$\theta_1:=0-0.01\times((0-460)\times2104+(0-232)\times1416+(0-315)\times1534+(0-178)\times852)／4 = 4828.045$
$\theta_2:=0-0.01\times((0-460)\times5+(0-232)\times3+(0-315)\times3+(0-178)\times2)／4 = 10.7425$
$\theta_3:=0-0.01\times((0-460)\times45+(0-232)\times40+(0-315)\times30+(0-178)\times36)／4 = 114.595$

算得**代价函数值为**$J(\theta)=3.9024\times10^{10}$。

比较两次的代价函数值，发现梯度下降后反而上升了，这里的原因是什么。

## 原因说明

在上面的例子中，
$x_1$ 的范围是从 852 到 2104， $x_2$的范围是从2 到 5，两个特征值的范围差距很大，这样会导致在梯度下降时，需要花费更多的时间，才能收敛到最小值，在这期间梯度值还会来回波动。

这就要求每项特征值都要在大致相同的范围内。这样梯度下降会很快到达最低点，完成收敛。

理想情况下，我们可以让每个特征值都修改到
$-1\leq x_i \leq1$ 或者 $-0.5\leq x_i \leq 0.5$ 

一般情况下有两种办法可以达到目的，是**特征缩放(feature scaling)**和**均值归一化(mean normalization)**

**特征缩放(feature scaling)**
特征缩放就是将特征值除以该特征值的范围（最大值减去最小值）
$$ x_i := \frac{x_i}{x_i(max) - x_i(min)} $$

应用到上面的例子，可以得到修改后的特征值矩阵：
$$ x=\left[ \begin{matrix} 1.6805 & 1.67 & 3 \\\ 1.131 & 1 & 2.67 \\\ 1.2252 & 1 & 2 \\\ 0.6805 & 0.67 & 2.4 \end{matrix} \right] $$

**均值归一化(mean normalization)**
均值归一化用特征值减去这些特征值的平均数，然后用减去的值除以该特征值的范围
$$ x_i := \frac{x_i - \mu_i}{x_i(max) - x_i(min)} $$

$\mu_i$即为平均数
应用到上面的例子，可以得到修改后的特征值矩阵：
$$ x=\left[ \begin{matrix} 0.5012 & 0.5833 & 0.4833 \\\ -0.0483 & 0 & 0.15 \\\ 0.0459 & 0 & -0.5167 \\\ -0.4988 & -0.3077 & -0.1167 \end{matrix} \right] $$

这样就得到了范围相近的特征值。注意，这两种方法得到的结果不同。

下面将归一化后的特征值代入到开始的例子中去，经过一次梯度下降后，可以的到:
$J(\theta)= 4.8571\times10^{4}$

计算过程和上方类似，这里不在列出，可以看到梯度值下降了，梯度下降算法是有效的。
当然这边我们的数据集只有4个样本，这只是举例所用，从中我们可以看出特征值缩放的作用，可以帮助我们更好的实现梯度下降算法。

