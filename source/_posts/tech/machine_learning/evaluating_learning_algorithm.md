---
layout: post
title: 评估学习算法
date: 2020-01-26 22:00
categories:
- 技术
tags:
- 交叉验证
description: 了解如何选择学习算法，以及对学习算法的优化，在使用学习曲线的情况下，判断高偏差和高方差，以及对其的解决办法。
mathjax: true
---

在使用学习算法解决机器学习问题时，可能预测函数的误差很小，但是这个学习算法不确定是不是准确的，因为可能出现过拟合的情况。

因为仅仅用一个训练集来判断学习算法是否准确是不行的，为此我们需要将原始数据集拆分成三个：
- 训练集 (Training set) 占60%
- 交叉验证集 (Cross validation set) 占20%
- 测试集 (Test set) 占20%

## 学习模型的选择

例如在线性回归问题中，训练集有一个特征值$x$，在选择模型时，选择几次项：
1 .  $h_\theta(x)=\theta_0+\theta_1x$
2 .  $h_\theta(x)=\theta_0+\theta_1x+\theta_2x^2$
3 .  $h_\theta(x)=\theta_0+\theta_1x+...+\theta_3x^3$
...
10 . $h_\theta(x)=\theta_0+\theta_1x+...+\theta_{10}x^{10}$

首先分成三个样本集：
训练集：$(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), ..., (x^{(m)},y^{(m)})$
交叉验证集：$(x_{cv}^{(1)},y_{cv}^{(1)}), (x_{cv}^{(2)},y_{cv}^{(2)}), ..., (x_{cv}^{(m_{cv})},y_{cv}^{(m_{cv})})$
测试集：$(x_{test}^{(1)},y_{test}^{(1)}), (x_{test}^{(2)},y_{test}^{(2)}), ..., (x_{test}^{(m_{test})},y_{test}^{(m_{test})})$

分开的三个样本集上的误差：
$J_{train}(\Theta)=\frac{1}{2m} \sum_{i=0}^m(h_\theta(x^{(i)})-y^{(i)})^2$
$J_{cv}(\Theta)=\frac{1}{2m_{cv}} \sum_{i=0}^{m_{cv}}(h_\theta(x_{cv}^{(i)})-y_{cv}^{(i)})^2$
$J_{test}(\Theta)=\frac{1}{2m_{test}} \sum_{i=0}^{m_{test}}(h_\theta(x_{test}^{(i)})-y_{test}^{(i)})^2$

选择模型的步骤：

- 将上面十个模型在训练集上优化每个模型的参数$\theta$；
- 在交叉验证集上找出一个误差最小的模型；
- 在测试集上用$J_{test}(\Theta)$来估计泛化误差。

这样的操作是的测试集与训练集，交叉验证集完全分离开，互不影响。在测试集上的泛化误差比较准确。


## 偏差（Bias）和方差（Variance）

当一个算法在使用时并不理想，会有两种情况，高偏差（欠拟合）或高方差（过拟合）。

还是上面的线性回归例子。通过选择不同模型，比较它们在训练集和交叉验证集上的表现结果：

![][1]

上图中，x轴是多项式的次数，可以看作是函数的复杂程度，y轴是代价函数的值。

**在训练集上：**当函数简单时，代价函数值很大，预测函数效果不好，随着多项式的增加，函数复杂度的增加，拟合效果越来越小，代价趋向于零。
**在交叉验证集上：**当函数简单时，代价函数值很大，预测函数效果不好，多项式在增加到一定程度前，代价一直在下降，当下降到一定程度是，代价函数又会上升，拟合效果又变差了。

**高偏差（Bias）：**$J_{train}(\Theta)$和$J_{cv}(\Theta)$都很高，而且两者非常接近，$J_{train}(\Theta)\approx J_{cv}(\Theta)$
**高方差（Variance）：**$J_{train}(\Theta)$很小，$J_{cv}(\Theta)$远远大于$J_{train}(\Theta)$

在实际使用时，需要在中间找一个平衡点，避免高偏差和高方差


## 正则化

通过正则化能够解决过拟合的问题，但是它在欠拟合也就是高偏差情况下效果并不明显。

上面的线性回归例子，选择一个模型：
$$h_\theta(x)=\theta_0+\theta_1x+\theta_2x^2+\theta_3x^3+\theta_4x^4$$
它的正则化后的代价函数：
$$ J(\theta)= \frac{1}{2m} \sum_{i=0}^m (h_\theta(x^{(i)})-y^{(i)})^2 + \frac{\lambda}{2m} \sum_{j=1}^n\theta_j^2 $$

*PS: 上面的$J_{train}(\Theta),J_{cv}(\Theta),J_{test}(\Theta)$定义里面没有加上正则化，所以上面的训练集，交叉验证集，测试集都是在没有正则化的情况的误差*

现在我们要做的是先选择一系列的$\lambda$值，比如:
0, 0.01, 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1.28, 2.56, 5.12, 10.24
通过正则化得出优化的参数$\theta$。

然后使用这些优化参数得出$J_{train}(\Theta),J_{cv}(\Theta)$
$J_{train}(\Theta)=\frac{1}{2m} \sum_{i=0}^m(h_\theta(x^{(i)})-y^{(i)})^2$
$J_{cv}(\Theta)=\frac{1}{2m_{cv}} \sum_{i=0}^{m_{cv}}(h_\theta(x_{cv}^{(i)})-y_{cv}^{(i)})^2$
**注意，这里的误差都没有加上正则化项目**

比较每个$\lambda$值下的代价函数的曲线：

![][2]

**在训练集上：** 随着$\lambda$的增大，代价函数的值越来越大，效果越来越差。
**在交叉验证集上：** 先随着$\lambda$在一定范围下增大，代价函数的值在减小，超过一个程度后，代价函数会增大。

当$\lambda$为0，或很小时，是高方差；当$\lambda$过大时，是高偏差。

同样的需要找一个$\lambda$使得代价函数在训练集和交叉验证集上平衡的。

## 学习曲线（Learning Curves）

画出随着样本集数量的增加，代价函数的值的变化曲线。
训练集和交叉验证集的误差：
$J_{train}(\Theta)=\frac{1}{2m} \sum_{i=0}^m(h_\theta(x^{(i)})-y^{(i)})^2$
$J_{cv}(\Theta)=\frac{1}{2m_{cv}} \sum_{i=0}^{m_{cv}}(h_\theta(x_{cv}^{(i)})-y_{cv}^{(i)})^2$

当样本集数量很少时，训练集的误差很小，因为两三个样本总能够用一个直线达到不错的拟合效果，但是在交叉验证集上体现的并不好。
随着样本数量的增大，训练集的误差增大，而在交叉验证集上的体现会改善，误差会减小。

**高偏差**
在高偏差情况下，随着样本数量的增大，训练集和验证集的误差趋向于相等，但是都会偏高，与预期效果不符。
![高偏差学习曲线][3]

**高方差**
在高方差下，随着样本增加，训练集的误差会增加，但是增加的幅度很小；在验证集上，误差会减小，但是仍然大于训练集误差。
![高方差学习曲线][4]

可以看出，在高方差下，增加样本数量会有助于算法的优化。但是在高偏差下，增加样本数量并没有多少用处。

通过以上方法，在解决机器学习问题中，有以下方法可以帮助优化算法：
- 获取更多样本：优化高方差
- 减少特征值的数量：优化高方差
- 添加特征值：优化高偏差
- 添加多项式次数，复杂的预测函数：优化高偏差
- 减小$\lambda$：解决高偏差
- 增加$\lambda$：解决高方差


[1]: /images/ml_19.jpg
[2]: /images/ml_20.jpg
[3]: /images/ml_21.jpg
[4]: /images/ml_22.jpg

