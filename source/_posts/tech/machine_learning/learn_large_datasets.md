---
layout: post
title: 在大数据下应用机器学习算法
date: 2020-02-09 10:00
categories:
- 技术
tags:
- 机器学习
description: 大量的数据对机器学习算法的研究很有帮助，当我们直到算法有较高的方差（variance）时，增加m会有助于改善算法。
mathjax: true
---

大量的数据对机器学习算法的研究很有帮助，当我们直到算法有较高的方差（variance）时，增加m会有助于改善算法。
但是当m很大时，比如m=100,000,000时，在这种情况下，在计算时就会消耗更多的成本。

### 使用随机梯度下降或小批量梯度下降
在使用批量梯度下降时，假设有1亿的样本，就要对1亿的样本求和，这样的成本太高了，使用随机梯度下降，每次对一个样本，或者小批量梯度下降，对几十个样本求和，可以大大减小计算，加快速度。

### 映射约减（Map Reduce），并行化处理
在设计网站时，当用户量增加时，一台服务器不足以承载这么多用户，我们会增加服务器，用负载均衡来解决。
同样的在解决大量数据的机器学习问题上，一台机器有时也不足以完成任务。

以线性回归为例，假设有400个样本，它的批量梯度下降的内部迭代为：
$$ \theta_j:=\theta_j−\alpha \left( \frac{1}{400} \sum_{i=1}^{400}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \right) $$

同时有四台机器，将样本分为四份：
机器1： $x^{(1)},x^{(2)},...,x^{(100)}$；
机器2： $x^{(101)},x^{(102)},...,x^{(200)}$；
机器3： $x^{(201)},x^{(202)},...,x^{(300)}$；
机器4： $x^{(301)},x^{(302)},...,x^{(400)}$。

分别将梯度下降大括号里的求和分成4在四个机器上运算：
机器1：$temp_j^{(1)}=\sum_{i=1}^{100}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$
机器2：$temp_j^{(2)}=\sum_{i=1}^{200}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$
机器3：$temp_j^{(3)}=\sum_{i=1}^{300}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$
机器4：$temp_j^{(4)}=\sum_{i=1}^{400}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$

然后将4份数据汇总到另一台机器上做求和运算。
$$\theta_j := \theta_j - \alpha \dfrac{1}{400}(temp_j^{(1)} + temp_j^{(2)} + temp_j^{(3)} + temp_j^{(4)})$$
这样能将原来的速度提升解决4倍，这是一个很大的提升。

将类似的求和运算分摊到多个机器上，它就是映射约减。
在线性回归和逻辑回归问题上很容易实现，而在神经网络上，也可以在多台机器上运行正向传播和反向传播，然后将结果汇总到主服务器上，加快运行速度。

处理使用多台计算机实现并行化，一台多核处理器的计算机也能实现类似的效果。

### 在线学习（Online Learning）

在一个大型网站中，有不断的用户访问，数据源保持不断，这时就可以使用在线学习系统在解决一些问题。

与通常的机器学习问题使用固定的数据集来训练算法不同，在线学习每次处理单独的样本，在利用单独的样本更新完参数$\theta$，改善算法性能后就丢弃该样本。这与随机梯度下降的处理方式有类似之处，区别就是在线学习不会使用一个固定的数据集。

算法过程：
一直迭代下去 {
　　获取单独的数据$(x,y)$;
　　通过该样本更新$\theta$ {
　　　　$ \theta_j:=\theta_j−\alpha (h_\theta(x)-y)x_j $
　　}
}

这种做法的优点在于可以减少数据的存储空间和计算速度，在大型网站中，数据是海量的，要集中处理时占用的硬盘和内存空间非常大，也非常耗时。而当有连续的数据时，就可以使用在线学习来处理。
另一个优点是可以适应业务不同发展的需求和不同时段用户的需求，因为参数$\theta$会随着输入样本的不同而改变。

