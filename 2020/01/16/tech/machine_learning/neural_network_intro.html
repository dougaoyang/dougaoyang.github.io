<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="这里记录了我在编程时遇到的一些问题，和解决这些问题的一些方法。还有一些是我对编程的一些看法观点，以及学习语言时的一些笔记和心得体会。"><title>神经网络算法介绍 | DeepCode</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><meta name="generator" content="Hexo 4.2.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">神经网络算法介绍</h1><a id="logo" href="/.">DeepCode</a><p class="description">高岸为谷，深谷为陵</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/tags/"><i class="fa fa-tag"> 标签</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">神经网络算法介绍</h1><div class="post-meta">Jan 16, 2020<span> | </span><span class="category"><a href="/categories/%E6%8A%80%E6%9C%AF/">技术</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 1.4k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 6</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>在解决分类问题时，可以用逻辑回归算法，但当解决复杂的非线性分类器时，这并不是一个好的选择。<br>如果用逻辑回归来解决，首先要构造一个包含很多非线性项的逻辑回归函数。使用逻辑回归会构造一个s型函数$g$。当多项式足够多，足够复杂，会有一个非常扭曲的决策边界。这可能会出现过拟合的情况。<br>另一个问题，复杂的非线性分类器会包含有很多的特征项，要包含所有的特征项时很困难的事情，而且计算成本过大。</p>
<p>比如要识别一个图像是不是汽车，就要检测图像的每一个像素，这是一个非常大的计算量。因此神经网络是一个很好的选择。</p>
<h2 id="神经网络模型"><a href="#神经网络模型" class="headerlink" title="神经网络模型"></a>神经网络模型</h2><img width="50%" src="/images/ml_16.jpg" alt="逻辑单元">

<p>这是一个最简单的神经网络的模型。左侧的是三个特征值的输入，右侧是一个输出，这是一个二元问题的神经网络模型。</p>
<p>一般在处理神经网络时，和逻辑回归一样，需要添加一个$x_0$的默认特征项。在神经网络里，这称之为偏置单位<br>$$ x=\left[ \begin{matrix}<br>x_0 \\ x_1 \\ x_2 \\ x_3<br>\end{matrix} \right], \theta=\left[ \begin{matrix}<br>\theta_0 \\ \theta_1 \\ \theta_2 \\ \theta_3<br>\end{matrix} \right]$$<br>还有关于$h(\theta)$的函数<br>$$ h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}} $$<br>在神经网络里这个成为激励函数，这是神经网络的术语，它是和逻辑回归里相同的函数。在这种情况下，激励函数的参数$\theta$称之为权重。</p>
<p>看一个复杂一点的模型：</p>
<img width="50%" src="/images/ml_17.jpg" alt="多层神经网络">

<p>上图中，第一层是<strong>输入层</strong>，然后进入第二层，最后输出预测函数，也就是第三层，<strong>输出层</strong>。<br>输入层和输出层之间的节点层，称之为<strong>隐藏层</strong>。上图中有一个隐藏层。<br>隐藏层中的$a_i^{(j)}$是第$j$层的第$i$个单元。和输入层一样，在计算时会添加一个偏置单元$a_0^{(j)}=1$<br>$$ a^{(j)}=\left[ \begin{matrix}<br>a_0^{(j)} \\ a_1^{(j)} \\ a_2^{(j)} \\  a_3^{(j)}<br>\end{matrix} \right]$$</p>
<p>$\theta^{(j)}$是从第$j$层到第$j+1$层的映射参数矩阵。</p>
<h2 id="计算过程"><a href="#计算过程" class="headerlink" title="计算过程"></a>计算过程</h2><p>隐藏层的激活节点和输出层的输出函数计算过程如下：<br>$ a_0^{(2)} = 1 $<br>$ a_1^{(2)} = g(\theta_{10}^{(1)}x_0+\theta_{11}^{(1)}x_1+\theta_{12}^{(1)}x_2+\theta_{13}^{(1)}x_3) $<br>$ a_2^{(2)} = g(\theta_{20}^{(1)}x_0+\theta_{21}^{(1)}x_1+\theta_{22}^{(1)}x_2+\theta_{23}^{(1)}x_3) $<br>$ a_3^{(2)} = g(\theta_{30}^{(1)}x_0+\theta_{31}^{(1)}x_1+\theta_{32}^{(1)}x_2+\theta_{33}^{(1)}x_3) $<br>$ h_\theta(x) = a_1^{(3)} = g(\theta_{10}^{(2)}a_0^{(2)}+\theta_{11}^{(2)}a_1^{(2)}+\theta_{12}^{(2)}a_2^{(2)}+\theta_{13}^{(2)}a_3^{(2)})$</p>
<p>$\theta^{(j)}$是一个矩阵，关于它的维度：<br>在第$j$层，该层有$s_j$个单元，而第$j+1$层有$s_k$个单元，则$\theta^{(j)}$会是一个$s_k\times s_j + 1$维矩阵</p>
<p>举个例子，假如有一个三层的神经网络，第一层有2个输入特征值，第二层是3个单元，最后第三层输出1个预测结果：<br>$\theta^{(1)}$s是一个$3\times 3$的矩阵<br>$\theta^{(2)}$s是一个$1\times 4$的矩阵</p>
<p>为了方便，将激励函数中的参数用变量$z$替换：<br>$ a_0^{(2)} = 1 $<br>$ a_1^{(2)} = g(z_1^{(2)}) $<br>$ a_2^{(2)} = g(z_2^{(2)}) $<br>$ a_3^{(2)} = g(z_3^{(2)}) $</p>
<p>其中$z$为：<br>$ z_k^{(2)}= \theta_{k,0}^{(1)}x_0 + \theta_{k,1}^{(1)}x_1 + … + \theta_{k,n}^{(1)}x_n $</p>
<h3 id="向量形式"><a href="#向量形式" class="headerlink" title="向量形式"></a>向量形式</h3><p>用向量的形式来表示：</p>
<p>$x=\left[ \begin{matrix}<br>x_0 \\ x_1 \\ x_2 \\ x_3<br>\end{matrix} \right], z^{(2)}= \left[ \begin{matrix}<br>z_1^{(2)} \\ z_2^{(2)} \\ z_2^{(2)}<br>\end{matrix} \right]$</p>
<p>$z^{(2)} = \theta^{(1)}x$<br>$a^{(2)} = g(z^{(2)})$</p>
<p>第三层：</p>
<p>$z^{(3)} = \theta^{(2)}z^{(2)}$</p>
<p>类推到通常情况：<br>$z^{(j)} = \theta^{(j-1)}a^{(j-1)}$<br>$a^{(j)} = g(z^{(j)})$<br>最后一步<br>$h_\theta(x) = a^{(j+1)} = g(z^{(j+1)})$</p>
<h2 id="举例说明"><a href="#举例说明" class="headerlink" title="举例说明"></a>举例说明</h2><h3 id="1-预测“与”-AND"><a href="#1-预测“与”-AND" class="headerlink" title="1.预测“与” AND"></a>1.预测“与” AND</h3><p>$$ \left[ \begin{matrix}<br>x_0 \\ x_1 \\ x_2<br>\end{matrix} \right] → \left[ \begin{matrix}<br>g(z^{(2)})<br>\end{matrix} \right] → h_\theta(x) $$</p>
<p>其中$x_0 = 1$</p>
<p>我们要计算“与”，其中$x1,x2 \in [0,1]$，$y=x_1$ &amp;&amp; $x_2$<br>设置$\theta^{(1)} = \left[ \begin{matrix}<br>-30 &amp; 20 &amp; 20<br>\end{matrix} \right]$</p>
<p>通过上面的公式<br>$h_\theta(x) = \theta^{(1)}x = g(-30+20x_1+20x_2)$</p>
<p>$x_1=0,x_2=0$，$h_\theta(x)=g(-30)\approx 0$<br>$x_1=0,x_2=1$，$h_\theta(x)=g(-10)\approx 0$<br>$x_1=1,x_2=0$，$h_\theta(x)=g(-10)\approx 0$<br>$x_1=1,x_2=1$，$h_\theta(x)=g(10)\approx 1$</p>
<p>满足”与“的逻辑。</p>
<h3 id="2-预测“或”-OR"><a href="#2-预测“或”-OR" class="headerlink" title="2.预测“或” OR"></a>2.预测“或” OR</h3><p>与 预测“AND”的神经网络模型一样，我们只是调整一下：$\theta^{(1)} = \left[ \begin{matrix}<br>-10 &amp; 20 &amp; 20<br>\end{matrix} \right]$</p>
<p>$h_\theta(x) = \theta^{(1)}x = g(-10+20x_1+20x_2)$</p>
<p>$x_1=0,x_2=0$，$h_\theta(x)=g(-10)\approx 0$<br>$x_1=0,x_2=1$，$h_\theta(x)=g(10)\approx 1$<br>$x_1=1,x_2=0$，$h_\theta(x)=g(10)\approx 1$<br>$x_1=1,x_2=1$，$h_\theta(x)=g(30)\approx 1$</p>
<p>满足”或“的逻辑。</p>
<h3 id="3-预测“异或”-XOR"><a href="#3-预测“异或”-XOR" class="headerlink" title="3.预测“异或” XOR"></a>3.预测“异或” XOR</h3><p>$$ \left[ \begin{matrix}<br>x_0 \\ x_1 \\ x_2<br>\end{matrix} \right] → \left[ \begin{matrix}<br>a_0^{(2)} \\ a_1^{(2)} \\ a_2^{(2)}<br>\end{matrix} \right] → \left[ \begin{matrix}<br>g(z^{(3)})<br>\end{matrix} \right] → h_\theta(x) $$</p>
<p>其中$x_0=1,a_0^{(2)}=1$</p>
<p>$\theta^{(1)} = \left[ \begin{matrix}<br>-10 &amp; 20 &amp; 20 \\ 40 &amp; -30 &amp; -30<br>\end{matrix} \right], \theta^{(2)} = \left[ \begin{matrix}<br>-30 &amp; 20 &amp; 20<br>\end{matrix} \right] $</p>
<p>$a^{(2)} = \theta^{(1)}x$，$h_\theta(x)=a^{(3)}=\theta^{(2)}a^{(2)}$</p>
<p>$x_1=0,x_2=0$，$a^{(2)}=\left[ \begin{matrix}<br>1 \\ g(-10)\\ g(40)<br>\end{matrix} \right]\approx\left[ \begin{matrix}<br>1 \\ 0 \\ 1<br>\end{matrix} \right]$，$h_\theta(x)=g(-10)\approx 0$</p>
<p>$x_1=0,x_2=1$，$a^{(2)}=\left[ \begin{matrix}<br>1 \\ g(10) \\ g(10)<br>\end{matrix} \right]\approx\left[ \begin{matrix}<br>1 \\ 1 \\ 1<br>\end{matrix} \right]$，$h_\theta(x)=g(10)\approx 1$</p>
<p>$x_1=1,x_2=0$，$a^{(2)}=\left[ \begin{matrix}<br>1 \\ g(10) \\ g(10)<br>\end{matrix} \right]\approx\left[ \begin{matrix}<br>1 \\ 1 \\ 1<br>\end{matrix} \right]$，$h_\theta(x)=g(10)\approx 1$</p>
<p>$x_1=1,x_2=1$，$a^{(2)}=\left[ \begin{matrix}<br>1 \\ g(30) \\ g(-20)<br>\end{matrix} \right]\approx\left[ \begin{matrix}<br>1 \\ 1 \\ 0<br>\end{matrix} \right]$，$h_\theta(x)=g(-10)\approx 0$</p>
<p>这符合“异或”的逻辑。</p>
</div><div class="tags"><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a></div><div class="post-nav"><a class="pre" href="/2020/01/18/tech/machine_learning/neural_network_cost_function.html">神经网络代价函数和反向传播算法</a><a class="next" href="/2020/01/15/tech/machine_learning/one_vs_all.html">分类算法中多元分类</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8A%80%E6%9C%AF/">技术</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/%E9%A9%AC%E6%8B%89%E6%9D%BE/" style="font-size: 10px;">马拉松</a> <a href="/tags/HTTP/" style="font-size: 10px;">HTTP</a> <a href="/tags/mysql/" style="font-size: 11.25px;">mysql</a> <a href="/tags/javascript/" style="font-size: 18.75px;">javascript</a> <a href="/tags/bootstrap/" style="font-size: 10px;">bootstrap</a> <a href="/tags/HTML/" style="font-size: 11.25px;">HTML</a> <a href="/tags/vue/" style="font-size: 10px;">vue</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 20px;">机器学习</a> <a href="/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 16.25px;">监督学习</a> <a href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" style="font-size: 12.5px;">逻辑回归</a> <a href="/tags/%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">非监督学习</a> <a href="/tags/%E7%AE%97%E6%B3%95%E4%BC%98%E5%8C%96/" style="font-size: 12.5px;">算法优化</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" style="font-size: 13.75px;">线性回归</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" style="font-size: 12.5px;">神经网络</a> <a href="/tags/SVM/" style="font-size: 11.25px;">SVM</a> <a href="/tags/PHP/" style="font-size: 10px;">PHP</a> <a href="/tags/python/" style="font-size: 17.5px;">python</a> <a href="/tags/pandas/" style="font-size: 13.75px;">pandas</a> <a href="/tags/django/" style="font-size: 10px;">django</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="font-size: 12.5px;">数据结构</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020/05/24/tech/python/structure/str-search-and-kmp.html">字符串朴素匹配算法和KMP算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/03/tech/python/structure/josephus-solve.html">约瑟夫（Josephos）问题以及解法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/31/tech/python/structure/link-intro.html">链表介绍及python的实现</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/09/tech/machine_learning/learn_large_datasets.html">在大数据下应用机器学习算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/08/tech/machine_learning/mini_batch_gradient_descent.html">小批量梯度下降算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/08/tech/machine_learning/stochastic_gradient_descent.html">随机梯度下降算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/08/tech/machine_learning/recommender_system.html">推荐系统和协同过滤算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/07/tech/machine_learning/anomaly_detection.html">异常检测（Anomaly Detection）</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/06/tech/machine_learning/dimensionality_reduction.html">PCA降维算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/06/tech/machine_learning/k_means_intro.html">K均值算法介绍</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://www.jianshu.com/u/f133ca80001f" title="爱吃鱼的夏侯莲子" target="_blank">爱吃鱼的夏侯莲子</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2020 <a href="/." rel="nofollow">DeepCode</a>&nbsp;|&nbsp;<a href="http://www.beian.miit.gov.cn/" target="_blank" rel="noopener">苏ICP备20021898号-1</a>&nbsp;|&nbsp;<img class="nofancybox" src="../images/beian/icon.png" style="margin-bottom: -4px;"/><a href="http://www.beian.gov.cn/" target="_blank" rel="noopener">苏公网安备 32059002003042号</a><div class="ss"> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>