<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="这里记录了我在编程时遇到的一些问题，和解决这些问题的一些方法。还有一些是我对编程的一些看法观点，以及学习语言时的一些笔记和心得体会。"><title>神经网络代价函数和反向传播算法 | DeepCode</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><meta name="generator" content="Hexo 4.2.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">神经网络代价函数和反向传播算法</h1><a id="logo" href="/.">DeepCode</a><p class="description">高岸为谷，深谷为陵</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/tags/"><i class="fa fa-tag"> 标签</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">神经网络代价函数和反向传播算法</h1><div class="post-meta">Jan 18, 2020<span> | </span><span class="category"><a href="/categories/%E6%8A%80%E6%9C%AF/">技术</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 745</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 3</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><img width="50%" src="/images/ml_18.jpg" alt="多层神经网络">

<p>定义一些参数：<br>$L$ = 神经网络的层数<br>$s_l$ = $l$层的单元数，但不包括偏差单元<br>$K$ = 输出层的单元数，二元分类是1，大于二的就是k</p>
<p>逻辑回归的代价函数为：<br>$$ J(\theta)=-\frac{1}{m} \sum_{i=1}^m [y^{(i)} \log(h_\theta(x^{(i)})) + (1-y^{(i)}) \log(1-h_\theta(x^{(i)}))] + \frac{\lambda}{2m} \sum_{j=1}^n \theta_j^2 $$</p>
<p>神经网络的代价函数：<br>$h_\theta(x^{(i)})$ 是一个K维向量。$(h_\theta(x))_i$是输出向量的第i个单元。</p>
<p>$$ J(\theta)=-\frac{1}{m} \sum_{i=1}^{m}\sum_{k=1}^{K} [y_k^{(i)}\log((h_\theta(x^{(i)}))^k) + (1-y_k^{(i)})\log(1-(h_\theta(x^{(i)}))^k) ] + \frac{\lambda}{2m} \sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}(\theta_{ji}^{(l)})^2 $$</p>
<p><em>PS：$(h_\theta(x^{(i)}))^k$ 这里的$k$应该是下标，这样写的原因是markdown解析不出来，囧～～</em></p>
<p>左侧两个求和的是将输出的每个单元的回归代价相加，右侧的三次和是将所有的$\theta$单元的平方相加</p>
<h2 id="最小化-J-theta"><a href="#最小化-J-theta" class="headerlink" title="最小化$J(\theta)$"></a>最小化$J(\theta)$</h2><p>想要计算神经网络的代价函数$J(\theta)$的最小化，需要计算$\frac{\partial}{\partial \theta_{i,j}^{(l)}} J(\theta)$</p>
<p>在神经网络算法中，最小化代价函数一般是使用<strong>反向传播算法（Backpropagation）</strong>来计算</p>
<p>定义一个参数：<br>$a_j^{(l)}$ = 第$l$层，第$j$个节点的激励函数返回值。<br>$\delta_j^{(l)}$ = 第$l$层，第$j$个节点的误差。</p>
<h3 id="计算过程"><a href="#计算过程" class="headerlink" title="计算过程"></a>计算过程</h3><ol>
<li><h4 id="初始化-Delta-i-j-l-0-，每一层的每一个元素都是0；"><a href="#初始化-Delta-i-j-l-0-，每一层的每一个元素都是0；" class="headerlink" title="初始化$\Delta_{i,j}^{(l)}=0$，每一层的每一个元素都是0；"></a>初始化$\Delta_{i,j}^{(l)}=0$，每一层的每一个元素都是0；</h4></li>
<li><h4 id="设置-a-1-x-i-；"><a href="#设置-a-1-x-i-；" class="headerlink" title="设置 $a^{(1)}=x^{(i)}$；"></a>设置 $a^{(1)}=x^{(i)}$；</h4></li>
<li><h4 id="正向计算每层的单元-a-l-，-l-2-3-…-L"><a href="#正向计算每层的单元-a-l-，-l-2-3-…-L" class="headerlink" title="正向计算每层的单元$a^{(l)}$，$l$=2,3,…,L"></a>正向计算每层的单元$a^{(l)}$，$l$=2,3,…,L</h4></li>
</ol>
<p>最上面的神经网络模型为例：<br>$a^{(1)}=x$<br>$z^{(2)}=\Theta^{(1)}a^{(1)}$<br>$a^{(2)}=g(z^{(2)})$，添加$a_0^{(2)}=1$<br>$z^{(3)}=\Theta^{(2)}a^{(2)}$<br>$a^{(3)}=g(z^{(3)})$，添加$a_0^{(3)}=1$<br>$z^{(4)}=\Theta^{(3)}a^{(3)}$<br>$a^{(4)}=h_\theta(x)=g(z^{(4)})$</p>
<ol start="4">
<li><h4 id="计算L层（输出层）误差-delta-L-a-L-y-i"><a href="#计算L层（输出层）误差-delta-L-a-L-y-i" class="headerlink" title="计算L层（输出层）误差$\delta^{(L)}=a^{(L)}-y^{(i)}$"></a>计算L层（输出层）误差$\delta^{(L)}=a^{(L)}-y^{(i)}$</h4></li>
</ol>
<p>$y^{(i)}$是训练集的真实输出值。上例中，$\delta^{(4)}=a^{(4)}-y^{(i)}$</p>
<ol start="5">
<li><h4 id="计算L-1-L-2-…-2层误差，-delta-L-1-delta-L-2-…-delta-2-；"><a href="#计算L-1-L-2-…-2层误差，-delta-L-1-delta-L-2-…-delta-2-；" class="headerlink" title="计算L-1,L-2,…,2层误差，$\delta^{(L-1)},\delta^{(L-2)},…,\delta^{(2)}$；"></a>计算L-1,L-2,…,2层误差，$\delta^{(L-1)},\delta^{(L-2)},…,\delta^{(2)}$；</h4></li>
</ol>
<p>L层之前的层的误差计算公式<br>$\delta^{(l)}=(\Theta^{(l)})^T\delta^{(l+1)} \cdot*g\prime(z^{(l)}),l \in [2,3,…L-1]$</p>
<p>其中$g\prime(z^{(l)})=a^{(l)} \cdot* (1-a^{(l)})$<br>因此误差计算公式等于：<br>$$ \delta^{(l)}=(\Theta^{(l)})^T\delta^{(l+1)} \cdot* a^{(l)} \cdot* (1-a^{(l)}),l \in [2,3,…L-1] $$<br>这里没有第一层误差，因为第一层就是输入值，不存在误差。</p>
<ol start="6">
<li><h4 id="计算-Delta-，-Delta-i-j-l-Delta-i-j-l-a-j-i-delta-i-l-1-，用向量方式表示-Delta-l-Delta-l-delta-l-1-a-l-T-，其中-l-in-1-2-3-…-L-1"><a href="#计算-Delta-，-Delta-i-j-l-Delta-i-j-l-a-j-i-delta-i-l-1-，用向量方式表示-Delta-l-Delta-l-delta-l-1-a-l-T-，其中-l-in-1-2-3-…-L-1" class="headerlink" title="计算$\Delta$，$\Delta_{i,j}^{(l)} := \Delta_{i,j}^{(l)} + a_j^{(i)}\delta_i^{(l+1)}$，用向量方式表示$\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T $，其中$l \in [1,2,3,…,L-1]$"></a>计算$\Delta$，$\Delta_{i,j}^{(l)} := \Delta_{i,j}^{(l)} + a_j^{(i)}\delta_i^{(l+1)}$，用向量方式表示$\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T $，其中$l \in [1,2,3,…,L-1]$</h4></li>
<li><h4 id="在所有的样本集中，重复2-6步骤，计算出-Delta-l"><a href="#在所有的样本集中，重复2-6步骤，计算出-Delta-l" class="headerlink" title="在所有的样本集中，重复2-6步骤，计算出$\Delta^{(l)}$;"></a>在所有的样本集中，重复2-6步骤，计算出$\Delta^{(l)}$;</h4></li>
<li><h4 id="这里要加上正则化的过程："><a href="#这里要加上正则化的过程：" class="headerlink" title="这里要加上正则化的过程："></a>这里要加上正则化的过程：</h4></li>
</ol>
<p>$D_{i,j}^{(l)} = \frac{1}{m}\Delta_{i,j}^{(l)}, j=0$<br>$D_{i,j}^{(l)} = \frac{1}{m}(\Delta_{i,j}^{(l)} + \lambda\Theta_{i,j}^{(l)}), j\neq 0$</p>
<p>最终获得的代价函数的导数$\frac{\partial}{\partial \theta_{i,j}^{(l)}} J(\Theta)=D_{i,j}^{(l)}$</p>
</div><div class="tags"><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a></div><div class="post-nav"><a class="pre" href="/2020/01/20/tech/machine_learning/neural_network_backpropagation.html">神经网络的反向传播算法的使用</a><a class="next" href="/2020/01/16/tech/machine_learning/neural_network_intro.html">神经网络算法介绍</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8A%80%E6%9C%AF/">技术</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/%E9%A9%AC%E6%8B%89%E6%9D%BE/" style="font-size: 10px;">马拉松</a> <a href="/tags/HTTP/" style="font-size: 10px;">HTTP</a> <a href="/tags/mysql/" style="font-size: 11.25px;">mysql</a> <a href="/tags/javascript/" style="font-size: 18.75px;">javascript</a> <a href="/tags/bootstrap/" style="font-size: 10px;">bootstrap</a> <a href="/tags/HTML/" style="font-size: 11.25px;">HTML</a> <a href="/tags/vue/" style="font-size: 10px;">vue</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 20px;">机器学习</a> <a href="/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 16.25px;">监督学习</a> <a href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" style="font-size: 12.5px;">逻辑回归</a> <a href="/tags/%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">非监督学习</a> <a href="/tags/%E7%AE%97%E6%B3%95%E4%BC%98%E5%8C%96/" style="font-size: 12.5px;">算法优化</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" style="font-size: 13.75px;">线性回归</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" style="font-size: 12.5px;">神经网络</a> <a href="/tags/SVM/" style="font-size: 11.25px;">SVM</a> <a href="/tags/PHP/" style="font-size: 10px;">PHP</a> <a href="/tags/python/" style="font-size: 17.5px;">python</a> <a href="/tags/pandas/" style="font-size: 13.75px;">pandas</a> <a href="/tags/django/" style="font-size: 10px;">django</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="font-size: 12.5px;">数据结构</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020/05/24/tech/python/structure/str-search-and-kmp.html">字符串朴素匹配算法和KMP算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/03/tech/python/structure/josephus-solve.html">约瑟夫（Josephos）问题以及解法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/31/tech/python/structure/link-intro.html">链表介绍及python的实现</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/09/tech/machine_learning/learn_large_datasets.html">在大数据下应用机器学习算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/08/tech/machine_learning/mini_batch_gradient_descent.html">小批量梯度下降算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/08/tech/machine_learning/stochastic_gradient_descent.html">随机梯度下降算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/08/tech/machine_learning/recommender_system.html">推荐系统和协同过滤算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/07/tech/machine_learning/anomaly_detection.html">异常检测（Anomaly Detection）</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/06/tech/machine_learning/dimensionality_reduction.html">PCA降维算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/06/tech/machine_learning/k_means_intro.html">K均值算法介绍</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://www.jianshu.com/u/f133ca80001f" title="爱吃鱼的夏侯莲子" target="_blank">爱吃鱼的夏侯莲子</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2020 <a href="/." rel="nofollow">DeepCode</a>&nbsp;|&nbsp;<a href="http://www.beian.miit.gov.cn/" target="_blank" rel="noopener">苏ICP备20021898号-1</a>&nbsp;|&nbsp;<img class="nofancybox" src="../images/beian/icon.png" style="margin-bottom: -4px;"/><a href="http://www.beian.gov.cn/" target="_blank" rel="noopener">苏公网安备 32059002003042号</a><div class="ss"> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>