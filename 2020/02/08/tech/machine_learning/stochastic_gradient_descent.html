<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="这里记录了我在编程时遇到的一些问题，和解决这些问题的一些方法。还有一些是我对编程的一些看法观点，以及学习语言时的一些笔记和心得体会。"><title>随机梯度下降算法 | DeepCode</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><meta name="generator" content="Hexo 4.2.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">随机梯度下降算法</h1><a id="logo" href="/.">DeepCode</a><p class="description">高岸为谷，深谷为陵</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/tags/"><i class="fa fa-tag"> 标签</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">随机梯度下降算法</h1><div class="post-meta">Feb 8, 2020<span> | </span><span class="category"><a href="/categories/%E6%8A%80%E6%9C%AF/">技术</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 1.2k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 4</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>以线性回归为例：<br>预测函数为：<br>$$ h_\theta(x) = \theta^Tx $$<br>代价函数：<br>$$ J_{train}(\theta) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2 $$</p>
<p>重复：{<br>　　$ \theta_j:=\theta_j−\alpha \left( \frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \right) $<br>}</p>
<p>当数据量过大时，梯度下降的算法会变得很慢，因为要对所有的数据进行求和。因为每次重复梯度下降都是所有数据全部求和，所以梯度下降算法又称之为<strong>批量梯度下降（Batch Gradient Descent）</strong></p>
<h2 id="概念说明"><a href="#概念说明" class="headerlink" title="概念说明"></a>概念说明</h2><p>随机梯度下降在每一次迭代中，不用考虑全部的样本，只需要考虑一个训练样本。</p>
<p>针对一个样本，它的代价函数：<br>$$ cost(\theta, (x^{(i)},y^{(i)})) = \frac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2 $$<br>而针对所有样本的代价函数可以看作是对每个样本代价函数的平均：<br>$$ J_{train}(\theta) = \frac{1}{m}\sum_{i=1}^m cost(\theta, (x^{(i)},y^{(i)})) $$</p>
<p>随机梯度下降算法如下：<br>第一步，先随机打乱训练集样本。<br>第二步，进行梯度下降：<br>重复 {<br>　　循环所有样本 for i=1,2,3,…,m {<br>　　　　$ \theta_j:=\theta_j−\alpha (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}  $<br>　　}<br>}</p>
<p>一开始随机打乱数据是为了对样本集的访问是随机的，会让梯度下降的速度快一点。</p>
<p>该算法一次训练一个样本，对它的代价函数进行一小步梯度下降，修改参数$\theta$，使得它对该样本的拟合会好一点；然后再对下一个样本进行运算，直到扫描完所有的训练样本，最后外部在迭代这个过程。</p>
<p>跟批量梯度下降算法不同的是，随机梯度下降不需要等到所有样本求和来得到梯度项，而是在对每个样本就可以求出梯度项，在对每个样本扫描的过程中就已经在优化参数了。</p>
<p>在梯度下降过程中，批量梯度下降的过程趋向于一条直线，直接收敛到全局最小值；而随机梯度下降不太可能收敛到全局最小值，而是随机地在其周围震荡，但通常会很接近最小值。</p>
<p>随机梯度下降通常需要经过1-10次外部循环才能接近全局最小值。</p>
<h2 id="判断收敛"><a href="#判断收敛" class="headerlink" title="判断收敛"></a>判断收敛</h2><p>在批量梯度下降中，要判断是否收敛，需要在每一次迭代算法后计算$J_{train}$的值，根据值的变化来判断收敛。<br>在执行随机梯度下降时，不需要计算所有的样本的代价函数，只用在对某个样本进行梯度下降前计算该样本的代价函数$cost(\theta, (x^{(i)},y^{(i)})) $，为了判断是否收敛，可以计算多次迭代后$cost(\theta, (x^{(i)},y^{(i)})) $的平均值，例如1000次迭代，在每次更新$\theta$前，计算最后1000次的的cost的平均值。</p>
<p>选择每隔多少次计算成本函数对梯度下降的过程也有影响：<br><img width="40%" src="/images/ml44.png" alt=""><br>上图中蓝色曲线是每1000次迭代，红色的是每隔5000次迭代。<br>因为随机梯度下降时会出现震荡，当迭代次数少时发现下降的曲线起伏很多，而迭代次数变大时，曲线就会变得平滑许多。缺点是每隔5000个计算，会增加计算成本。</p>
<p>增加迭代次数可以判断算法是否正确：<br><img width="40%" src="/images/ml45.png" alt=""><br>上图蓝色的是1000个迭代次数，通过这条曲线，不能很好的判断成本函数是否在下降，这时就需要添加迭代次数，当增加到5000次，则可以通过平滑的曲线判断，当下滑曲线是红色的时，说明算法是有效的，代价函数值在下降；当是紫色的曲线时，可以看到是一个平坦的线，这时判断算法可能出现问题了。</p>
<p>在随机梯度下降中，学习率$\alpha$也会影响算法，当学习率减小时，下降曲线的震荡就会变小，而且会收敛到一个更好的解：<br><img width="40%" src="/images/ml46.png" alt=""><br>图中红色的曲线时学习率更小的一个，可以看到震荡变小，且下降到一个更小的值。</p>
<p>当看到曲线是上升的时候，可以尝试减小学习率看看效果。</p>
<p>在随机梯度下降中，如果想要收敛到全剧最小值，需要随着时间的变化减小学习率$\alpha$的值：<br>$$ \alpha = \frac{const1}{iterNumber + const2} $$<br>学习率等于一个常数除以迭代次数加另一个常数，随着迭代次数增大，学习率会减小；但这会造成常数1和常数2的选择问题。</p>
</div><div class="tags"><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post-nav"><a class="pre" href="/2020/02/08/tech/machine_learning/mini_batch_gradient_descent.html">小批量梯度下降算法</a><a class="next" href="/2020/02/08/tech/machine_learning/recommender_system.html">推荐系统和协同过滤算法</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8A%80%E6%9C%AF/">技术</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/%E9%A9%AC%E6%8B%89%E6%9D%BE/" style="font-size: 10px;">马拉松</a> <a href="/tags/HTTP/" style="font-size: 10px;">HTTP</a> <a href="/tags/mysql/" style="font-size: 11.25px;">mysql</a> <a href="/tags/javascript/" style="font-size: 18.75px;">javascript</a> <a href="/tags/bootstrap/" style="font-size: 10px;">bootstrap</a> <a href="/tags/HTML/" style="font-size: 11.25px;">HTML</a> <a href="/tags/vue/" style="font-size: 10px;">vue</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 20px;">机器学习</a> <a href="/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 16.25px;">监督学习</a> <a href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" style="font-size: 12.5px;">逻辑回归</a> <a href="/tags/%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">非监督学习</a> <a href="/tags/%E7%AE%97%E6%B3%95%E4%BC%98%E5%8C%96/" style="font-size: 12.5px;">算法优化</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" style="font-size: 13.75px;">线性回归</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" style="font-size: 12.5px;">神经网络</a> <a href="/tags/SVM/" style="font-size: 11.25px;">SVM</a> <a href="/tags/PHP/" style="font-size: 10px;">PHP</a> <a href="/tags/python/" style="font-size: 17.5px;">python</a> <a href="/tags/pandas/" style="font-size: 13.75px;">pandas</a> <a href="/tags/django/" style="font-size: 10px;">django</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="font-size: 12.5px;">数据结构</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020/05/24/tech/python/structure/str-search-and-kmp.html">字符串朴素匹配算法和KMP算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/03/tech/python/structure/josephus-solve.html">约瑟夫（Josephos）问题以及解法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/31/tech/python/structure/link-intro.html">链表介绍及python的实现</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/09/tech/machine_learning/learn_large_datasets.html">在大数据下应用机器学习算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/08/tech/machine_learning/mini_batch_gradient_descent.html">小批量梯度下降算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/08/tech/machine_learning/stochastic_gradient_descent.html">随机梯度下降算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/08/tech/machine_learning/recommender_system.html">推荐系统和协同过滤算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/07/tech/machine_learning/anomaly_detection.html">异常检测（Anomaly Detection）</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/06/tech/machine_learning/dimensionality_reduction.html">PCA降维算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/06/tech/machine_learning/k_means_intro.html">K均值算法介绍</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://www.jianshu.com/u/f133ca80001f" title="爱吃鱼的夏侯莲子" target="_blank">爱吃鱼的夏侯莲子</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2020 <a href="/." rel="nofollow">DeepCode</a>&nbsp;|&nbsp;<a href="http://www.beian.miit.gov.cn/" target="_blank" rel="noopener">苏ICP备20021898号-1</a>&nbsp;|&nbsp;<img class="nofancybox" src="../images/beian/icon.png" style="margin-bottom: -4px;"/><a href="http://www.beian.gov.cn/" target="_blank" rel="noopener">苏公网安备 32059002003042号</a><div class="ss"> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>