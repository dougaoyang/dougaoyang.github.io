<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="这里记录了我在编程时遇到的一些问题，和解决这些问题的一些方法。还有一些是我对编程的一些看法观点，以及学习语言时的一些笔记和心得体会。"><title>PCA降维算法 | DeepCode</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><meta name="generator" content="Hexo 4.2.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">PCA降维算法</h1><a id="logo" href="/.">DeepCode</a><p class="description">高岸为谷，深谷为陵</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/tags/"><i class="fa fa-tag"> 标签</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">PCA降维算法</h1><div class="post-meta">Feb 6, 2020<span> | </span><span class="category"><a href="/categories/%E6%8A%80%E6%9C%AF/">技术</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 1.6k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 6</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>降维是机器学习中很重要的一种思想。在机器学习中经常会碰到一些高维的数据集，它们会占用计算机的内存和硬盘空间，而且在运算时会减缓速度。<br>降维能够使得数据量被压缩，加快运算速度，减小储存空间，以及方便可视化的观察数据特点。</p>
<p><em>PS：在降维中，我们减少的是特征种类而不是样本数量，样本数量m不变，特征值数量n会减少。</em></p>
<h2 id="主成分分析算法（PCA）"><a href="#主成分分析算法（PCA）" class="headerlink" title="主成分分析算法（PCA）"></a>主成分分析算法（PCA）</h2><p>一种常用的降维算法是主成分分析算法（Principal Component Analysis），简称<strong>PCA</strong>。</p>
<p>PCA是通过找到一个低维的线或面，然后将数据投影到线或面上去，然后通过减少投影误差（即每个特征到投影的距离的平均值）来实现降维。</p>
<img width="40%" src="/images/ml_32.jpg" alt="">

<p>上图是一个包含二维特征值的样本集。黑色的叉代表样本，红色的线表示找到的低维的线，绿色的叉则是样本投影在线上的位置。而它们的投影距离就是PCA算法所需要考虑的。</p>
<p>通过上图可以看出PCA算法就是找出一个线，在数学上就是一个向量，使得其他样本投影到该向量上的距离最小。</p>
<p>推而广之：<br>一般情况下，将特征值的维度从n降到k，就是找到k个向量$u^{(1)},u^{(2)},…,u^{(k)}$，使得样本在这些向量上的投影最小。</p>
<p>例如，2维降到1维，就是找到1个向量，即一条线；3维降到2维，就是找到2向量，即一个平面。</p>
<h3 id="PCA和线性回归的区别"><a href="#PCA和线性回归的区别" class="headerlink" title="PCA和线性回归的区别"></a>PCA和线性回归的区别</h3><ul>
<li>在线性回归中，最小化的是没有样本到预测线的平方误差，它们之间的距离是垂直距离。</li>
<li>在PCA中，最小化的是投影距离，或者说是正交距离。</li>
</ul>
<h3 id="算法过程"><a href="#算法过程" class="headerlink" title="算法过程"></a>算法过程</h3><p><strong>数据处理</strong></p>
<p>假设有m个样本集：$x^{(1)},x^{(2)},…,x^{(m)}$<br>下面需要对数据做一下特征值缩放或者均值归一化。<br>先计算出平均值，然后用样本值减去平均值。<br>$\mu_i=\frac{1}{m}\sum_{i=1}^m x_j^{(i)}$<br>然后用$\frac{x_j^{(i)}-\mu_i}{s_j}$ 替换$ x_j^{(i)}$，$s_j$可以是数据最大值最小值的范围或者标准差。</p>
<p><strong>算法部分</strong></p>
<ol>
<li>计算协方差矩阵（covariance matrix）<br>$$\Sigma = \frac{1}{m}\sum^m_{i=1}(x^{(i)})(x^{(i)})^T$$</li>
</ol>
<p><em>PS：假设$x^{(i)}$为一个n维向量（n * 1）,$(x^{(i)})(x^{(i)})^T$则是一个n维方阵。</em></p>
<ol start="2">
<li><p>计算协方差矩阵的特征向量（eigenvectors）<br>可以通过奇异值分解（SVD）来获得：</p>
<figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="meta">U,S,V</span>] = svd(Sigma);</span><br></pre></td></tr></table></figure>
<p>我们需要的就是矩阵U，他是一个n维方阵$U \in \mathbb{R}^{n \times n}$，它的每一列就是我们需要的向量：<br>$$ U = \left[ \begin{matrix} u^{(1)} &amp; u^{(2)} &amp; … &amp; u^{(n)} \end{matrix} \right]$$</p>
</li>
<li><p>获取Ureduce矩阵<br>当要从n降维到k，从矩阵U中取前k个向量就可以了：<br>$$Ureduce = \left[ \begin{matrix} u^{(1)} &amp; u^{(2)} &amp; … &amp; u^{(k)} \end{matrix} \right]$$</p>
</li>
<li><p>计算投影后的矩阵<br>$$ z^{(i)} = Ureduce^T \cdot x^{(i)} $$</p>
</li>
</ol>
<p><em>PS：$Ureduce^T$ 是一个  k×n 维的矩阵，$x^{(i)}$ 是一个n×1维的向量，两者点乘后就是一个z×1的向量，这样就得到了我们需要的。</em></p>
<h3 id="回到原来的维度"><a href="#回到原来的维度" class="headerlink" title="回到原来的维度"></a>回到原来的维度</h3><p>使用$Ureduce$矩阵可以降维：<br>$$ z^{(i)} = Ureduce^T \cdot x^{(i)} $$<br>那么要回到原来的维度上去就需要：<br>$$ x_{approx}^{(i)} = Ureduce \cdot z^{(i)} $$</p>
<p><em>这里我们只能得到原来的近似值</em></p>
<h3 id="选择降维的维度"><a href="#选择降维的维度" class="headerlink" title="选择降维的维度"></a>选择降维的维度</h3><p>$x_{approx}^{(i)}$与$ x^{(i)}$ 近似相等，两者之间的差就是投影误差，或平均平方映射误差：<br>$$ \frac{1}{m}\sum_{i=1}^m || x^{(i)} - x_{approx}^{(i)} ||^2 $$</p>
<p>数据的总变差（total variation），即样本的长度平方的均值：<br>$$ \frac{1}{m}\sum_{i=1}^m || x^{(i)} ||^2 $$</p>
<p>选择维度k的最小值的方法：<br>$$ \frac{\frac{1}{m}\sum_{i=1}^m || x^{(i)} - x_{approx}^{(i)} ||^2}{\frac{1}{m}\sum_{i=1}^m || x^{(i)} ||^2} \leq 0.01 $$</p>
<p>表示平方投影误差除以总变差的值小于0.01，用PCA的语言称之为<strong>保留了99%的差异性</strong>。<br><em>PS：这个值是可以变化的，可以是95%，90%，85%等等。</em></p>
<p><strong>使用循环验证的办法：</strong><br> 初始化$k=1$，然后计算出$Ureduce$，通过$Ureduce$计算出$z^{(1)},z^{(2)},…,z^{(m)} $和$x_{approx}^{(1)},x_{approx}^{(2)},…,x_{approx}^{(m)}$，然后通过上方的公式计算出值是不是小于0.01。<br>如果不是，增加k值，直到获得最小的k值满足条件。</p>
<p><strong>快捷办法</strong></p>
<figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="meta">U,S,V</span>] = svd(Sigma);</span><br></pre></td></tr></table></figure>
<p>通过奇异值分解的到的矩阵$S$是一个n维的对角矩阵：<br>$$ S = \left[ \begin{matrix} s_{11} &amp; 0  &amp; … &amp; 0 \\ 0 &amp; s_{22} &amp; … &amp; 0 \\ … &amp; … &amp; … &amp; … \\ 0 &amp; 0 &amp; … &amp; s_{nn}  \end{matrix} \right] $$</p>
<p>通过这个矩阵可以来计算：<br>$$ 1 - \frac{\sum_{i=1}^k S_{ii}}{\sum_{i=1}^n S_{ii}}  \leq 0.01 $$<br>也可以用下面的式子：<br>$$ \frac{\sum_{i=1}^k S_{ii}}{\sum_{i=1}^n S_{ii}}  \geq 0.99 $$<br>这种方法就非常快捷高效。</p>
<h2 id="使用PCA的建议"><a href="#使用PCA的建议" class="headerlink" title="使用PCA的建议"></a>使用PCA的建议</h2><p>我们在训练集上通过PCA获得矩阵$Ureduce$，在交叉验证集和测试集上就不能再使用PCA来计算矩阵了，而是直接用训练集里的矩阵来映射交叉验证集和测试集上的数据。</p>
<p>PCA最常用的就是压缩数据，加速算法的学习，或者可视化数据。</p>
<ul>
<li>压缩数据<br>将数据压缩到合适的维度，用来加速算法。</li>
<li>数据可视化<br>将数据的维度降到2或3维以便画出图像。</li>
</ul>
<p><strong>PCA的错误用法，用来防止算法过拟合</strong><br>算法过拟合的原因之一是算法过于复杂，特征值的维度过高，使用PCA可以降低维度，看起来会有效，但是实际上效果很差。防止算法过拟合还是使用正则化的方法来实现。</p>
<p>还有一个注意点。就是在设计一个机器学习算法时，不用一开始就考虑降维，先在不使用PCA的条件下设计算法，当算法出现问题，例如，算法计算过慢，占用大量内存…，之后当确定需要使用PCA的时候再继续使用。</p>
</div><div class="tags"><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a href="/tags/%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/">非监督学习</a></div><div class="post-nav"><a class="pre" href="/2020/02/07/tech/machine_learning/anomaly_detection.html">异常检测（Anomaly Detection）</a><a class="next" href="/2020/02/06/tech/machine_learning/k_means_intro.html">K均值算法介绍</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8A%80%E6%9C%AF/">技术</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/%E9%A9%AC%E6%8B%89%E6%9D%BE/" style="font-size: 10px;">马拉松</a> <a href="/tags/HTTP/" style="font-size: 10px;">HTTP</a> <a href="/tags/mysql/" style="font-size: 11.25px;">mysql</a> <a href="/tags/javascript/" style="font-size: 18.75px;">javascript</a> <a href="/tags/bootstrap/" style="font-size: 10px;">bootstrap</a> <a href="/tags/HTML/" style="font-size: 11.25px;">HTML</a> <a href="/tags/vue/" style="font-size: 10px;">vue</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 20px;">机器学习</a> <a href="/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 16.25px;">监督学习</a> <a href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" style="font-size: 12.5px;">逻辑回归</a> <a href="/tags/%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">非监督学习</a> <a href="/tags/%E7%AE%97%E6%B3%95%E4%BC%98%E5%8C%96/" style="font-size: 12.5px;">算法优化</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" style="font-size: 13.75px;">线性回归</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" style="font-size: 12.5px;">神经网络</a> <a href="/tags/SVM/" style="font-size: 11.25px;">SVM</a> <a href="/tags/PHP/" style="font-size: 10px;">PHP</a> <a href="/tags/python/" style="font-size: 17.5px;">python</a> <a href="/tags/pandas/" style="font-size: 13.75px;">pandas</a> <a href="/tags/django/" style="font-size: 10px;">django</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="font-size: 12.5px;">数据结构</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020/05/24/tech/python/structure/str-search-and-kmp.html">字符串朴素匹配算法和KMP算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/03/tech/python/structure/josephus-solve.html">约瑟夫（Josephos）问题以及解法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/31/tech/python/structure/link-intro.html">链表介绍及python的实现</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/09/tech/machine_learning/learn_large_datasets.html">在大数据下应用机器学习算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/08/tech/machine_learning/mini_batch_gradient_descent.html">小批量梯度下降算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/08/tech/machine_learning/stochastic_gradient_descent.html">随机梯度下降算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/08/tech/machine_learning/recommender_system.html">推荐系统和协同过滤算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/07/tech/machine_learning/anomaly_detection.html">异常检测（Anomaly Detection）</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/06/tech/machine_learning/dimensionality_reduction.html">PCA降维算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/06/tech/machine_learning/k_means_intro.html">K均值算法介绍</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://www.jianshu.com/u/f133ca80001f" title="爱吃鱼的夏侯莲子" target="_blank">爱吃鱼的夏侯莲子</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2020 <a href="/." rel="nofollow">DeepCode</a>&nbsp;|&nbsp;<a href="http://www.beian.miit.gov.cn/" target="_blank" rel="noopener">苏ICP备20021898号-1</a>&nbsp;|&nbsp;<img class="nofancybox" src="../images/beian/icon.png" style="margin-bottom: -4px;"/><a href="http://www.beian.gov.cn/" target="_blank" rel="noopener">苏公网安备 32059002003042号</a><div class="ss"> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>