<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>DeepCode</title>
  
  <subtitle>高岸为谷，深谷为陵</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://codeeper.com/"/>
  <updated>2020-05-24T23:27:11.000Z</updated>
  <id>https://codeeper.com/</id>
  
  <author>
    <name>Dougaoyang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>字符串朴素匹配算法和KMP算法</title>
    <link href="https://codeeper.com/2020/05/24/tech/python/structure/str-search-and-kmp.html"/>
    <id>https://codeeper.com/2020/05/24/tech/python/structure/str-search-and-kmp.html</id>
    <published>2020-05-23T16:00:00.000Z</published>
    <updated>2020-05-24T23:27:11.000Z</updated>
    
    <content type="html"><![CDATA[<p>字符串的匹配在平常的编码过程中非常常用，在编程语言中通常是调用一个内置函数就可以实现字符串的匹配，当不让使用内置的函数，而是自己编写一个函数来实现匹配的功能，应该如何来写呢？</p><p>今天介绍两个算法，朴素匹配算法，和无回溯匹配算法中的KMP算法。</p><h2 id="朴素匹配算法"><a href="#朴素匹配算法" class="headerlink" title="朴素匹配算法"></a>朴素匹配算法</h2><p>朴素匹配算法就是按照常识来，最容易理解的逐个字符匹配。<br>从待匹配字符串中的某个下标<code>i</code>开始，匹配字符串从<code>0</code>开始，逐个匹配。当有不匹配的字符时，重新从<code>i+1</code>下标开始重复上次的匹配过程。</p><img width="50%" src="/images/struct/5.jpg" alt="图(1) 朴素匹配算法"><p>下面用代码实现一下：<br><code>t</code>表示待匹配字符串，<code>p</code>表示用来匹配的字符串。</p><p><strong>$p_i$ 表示p字符串的第i个下标的字符。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_match</span><span class="params">(t, p)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    t 目标字符串</span></span><br><span class="line"><span class="string">    p 匹配字符串</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    m, n = len(p), len(t)</span><br><span class="line">    i, j = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> i &lt; m <span class="keyword">and</span> j &lt; n:</span><br><span class="line">        <span class="keyword">if</span> p[i] == t[j]: <span class="comment"># 字符相同，考虑下一字符</span></span><br><span class="line">            i, j = i + <span class="number">1</span>, j + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            i, j = <span class="number">0</span>, j - i + <span class="number">1</span>  <span class="comment"># 字符不同，查找字符串重置，目标字符串回溯</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> i == m:</span><br><span class="line">        <span class="keyword">return</span> j - i</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure><p>朴素匹配算法非常简单，容易理解。当然，它的效率也是很低的，造成效率低的原因是在执行过程中会有回溯。当遇到<code>p[i] != t[j]</code>时，匹配字符串下标置0，待匹配字符串的下标回到上一次检查的下一个位置，往回退了<code>j - i + 1</code>个位置。</p><p>这种操作造成的效率很低。最坏的情况是，每次匹配都是到最后一个字符的时候不匹配。例如：<br>&nbsp;&nbsp;待匹配字符串： 0000000000001<br>&nbsp;&nbsp;匹配字符串：   00001<br>这样需要 <code>n-m+1</code>次比较，总的比较次数就是<code>(n-m+1) * m</code>，这样它的复杂度就是：O(n*m)</p><h2 id="KMP算法"><a href="#KMP算法" class="headerlink" title="KMP算法"></a>KMP算法</h2><p>朴素算法的效率低，根源上是把每次匹配都看成的单独的事件，没有利用到之前的匹配信息。而其他改进算法就是利用了之前的匹配信息。</p><p>KMP算法的基本思想就是在匹配中不回溯。</p><img width="60%" src="/images/struct/7.jpg" alt="图(2) KMP算法图解"><p>描述KMP算法，需要借助上图。<br>待匹配字符串<code>t</code>，和匹配字符串<code>p</code>。<br>在匹配过程中，<code>p</code>的第i个字符在和<code>t</code>的第j个字符比较，这时，$t_{j-i}$～$t_{j-1}$和$p_0$～$p_{i-1}$相等，匹配完成。下面要做的步骤可以分为两个：</p><ul><li>当$t_j = p_i$ 时，继续进行下一个字符的比较。</li><li>当$t_j \neq p_i$ 时，这是不需要重置<code>p</code>，而是找到一个位置<code>k</code>($0\leq k &lt; i$)，使得$t_{j-k}$～$t_{j-1}$ 等于 $p_0$～$p_{k-1}$，继续匹配$t_{j}$和$p_{k}$，重复上面的步骤。这样待匹配字符串也不需要回溯到前面去重新匹配。</li></ul><p>KMP算法中的关键认识是：在$p_i$匹配失败时，所有的$p_k$($0\leq k &lt; i$)都已经匹配成功。也就是说，待匹配字符串中的$t_j$之前的i个字符，与匹配字符串中的前i个字符$p_0,p_1,…,p_{i-1}$。<br>通过上面的分析，要找到k，完全可以先不管待匹配字符串，而是研究一下匹配字符串<code>p</code>，通过<code>p</code>找到它前移的位置<code>k</code>。</p><p>得出一个结论：在<code>p</code>中，其中的每一个字符的<code>i</code>都会有其对应的下标<code>k</code>，与待匹配的字符串无关。</p><p>因此，我们可以为匹配字符串设计一个列表来存储每一个<code>i</code>的下标<code>k</code>。假设p的长度为m，用一个长度为m的列表pnext来存储，用表元素<code>pnext[i]</code>来表示i个元素的下标k值。<br>有一种特殊情况：$p_i$匹配失败后，之前所做的匹配都没有用，需要从头开始匹配，用$p_0$与$t_{j+1}$比较。在这种特殊情况下可以在<code>pnext[i]</code>中存入-1来表示。显然，$p_0$一直为-1。</p><h3 id="KMP主算法"><a href="#KMP主算法" class="headerlink" title="KMP主算法"></a>KMP主算法</h3><p>当假设pnext已经获得了，KMP的算法实现为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kmp_match</span><span class="params">(t, p, pnext)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    KMP匹配，主函数</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    i, j = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    m, n = len(p), len(t)</span><br><span class="line">    <span class="keyword">while</span> i &lt; m <span class="keyword">and</span> j &lt; n:</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">-1</span>: <span class="comment"># -1 匹配下一队字符</span></span><br><span class="line">            i, j = i + <span class="number">1</span>, j + <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> p[i] == t[j]: <span class="comment"># 相等，匹配下一对字符</span></span><br><span class="line">            i, j = i + <span class="number">1</span>, j + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            i = pnext[i] <span class="comment"># 从pnext中拿出下一个字符应该的位置</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> i == m:</span><br><span class="line">        <span class="keyword">return</span> j - i</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure><p>该算法的时间复杂度为O(n)，因为j的循环次数不会超过n，<code>i = pnext[i]</code>的次数不会超过m。</p><h3 id="pnext的实现"><a href="#pnext的实现" class="headerlink" title="pnext的实现"></a>pnext的实现</h3><p>最长相等前后缀</p><p>已知pnext[0]=-1，并且pnext[0]到pnext[i-1]已知的情况下，求解pnext[i]:</p><ol><li>假设<code>pnext[i-1]=k-1</code>，如果$p_i=p_k$，则$p_0,p_1,…,p_i$的最长的匹配相等长度为k，记下<code>pnext[i]=k</code>。</li><li>如果$p_i \neq p_k$，就将k的值设为pnext[k]，即考虑前一个保证匹配的字符串，且更短。</li><li>假如k的值为-1（由第二步造成，一直到不到可以匹配的字符串），那么就将pnext[i]设置为0，将i递增后继续检查。</li></ol><p>构造方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_pnext</span><span class="params">(p)</span>:</span></span><br><span class="line">    i, k, m = <span class="number">0</span>, <span class="number">-1</span>, len(p)</span><br><span class="line"></span><br><span class="line">    pnext = [<span class="number">-1</span>] * m</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> i &lt; m - <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">if</span> k == <span class="number">-1</span> <span class="keyword">or</span> p[i] == p[k]:</span><br><span class="line">            i, k = i + <span class="number">1</span>, k + <span class="number">1</span></span><br><span class="line">            pnext[i] = k</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            k = pnext[k]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> pnext</span><br></pre></td></tr></table></figure><p>举例：匹配字符串为 <code>abbcabcaabbcaa</code>，得到的pnext为：<br><code>[-1, 0, 0, 0, 0, 1, 2, 0, 1, 1, 2, 3, 4, 5]</code></p><h4 id="pnext生成算法的改进"><a href="#pnext生成算法的改进" class="headerlink" title="pnext生成算法的改进"></a>pnext生成算法的改进</h4><p>在pnext的生成中，对pnext[i]的设置可以有些优化。<br>在图(2)中，$p_i \neq t_j$匹配失败，假设pnext[i]=k，如果发现$p_i=p_k$，那么也一定有$p_k \neq t_j$，所以，这种情况下pnext[i]的位置移动到pnext[k]，这一修改减少了一个比较步骤，有可能提高效率。修改后：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_pnext</span><span class="params">(p)</span>:</span></span><br><span class="line">    i, k, m = <span class="number">0</span>, <span class="number">-1</span>, len(p)</span><br><span class="line"></span><br><span class="line">    pnext = [<span class="number">-1</span>] * m</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> i &lt; m - <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">if</span> k == <span class="number">-1</span> <span class="keyword">or</span> p[i] == p[k]:</span><br><span class="line">            i, k = i + <span class="number">1</span>, k + <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> p[i] == p[k]:</span><br><span class="line">                pnext[i] = pnext[k]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                pnext[i] = k</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            k = pnext[k]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> pnext</span><br></pre></td></tr></table></figure><p>举例：匹配字符串为 <code>abbcabcaabbcaa</code>，得到的pnext为：<br><code>[-1, 0, 0, 0, -1, 0, 2, -1, 1, 0, 0, 0, -1, 5]</code></p><p>pnext的复杂度为O(m)，所以整个KMP算法的复杂度O(m+n)，由于m小于n，可以认为复杂度为O(n)，优于朴素算法。</p>]]></content>
    
    <summary type="html">
    
      字符串的匹配在平常的编码过程中非常常用，在编程语言中通常是调用一个内置函数就可以实现字符串的匹配，当不让使用内置的函数，而是自己编写一个函数来实现匹配的功能，应该如何来写呢？
    
    </summary>
    
    
      <category term="技术" scheme="https://codeeper.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="python" scheme="https://codeeper.com/tags/python/"/>
    
      <category term="数据结构" scheme="https://codeeper.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>约瑟夫（Josephos）问题以及解法</title>
    <link href="https://codeeper.com/2020/04/03/tech/python/structure/josephus-solve.html"/>
    <id>https://codeeper.com/2020/04/03/tech/python/structure/josephus-solve.html</id>
    <published>2020-04-02T16:00:00.000Z</published>
    <updated>2020-04-03T06:26:37.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Josephos问题"><a href="#Josephos问题" class="headerlink" title="Josephos问题"></a>Josephos问题</h2><p>假设n个人围坐一圈，现在要求从第k个人开始报数，报到第m个人退出，然后从下一个人开始继续报数并按照同样规则退出，直至所有人退出。要求按照顺序输出退出人的编号。</p><h2 id="基于顺序表的解法"><a href="#基于顺序表的解法" class="headerlink" title="基于顺序表的解法"></a>基于顺序表的解法</h2><p>当确定退出的人后，就将其编号的元素从列表中删除，这样列表就会越来越短，直至0结束。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">josephos_l</span><span class="params">(n, k, m)</span>:</span></span><br><span class="line">    people = list(range(<span class="number">1</span>, n+<span class="number">1</span>))</span><br><span class="line">    i = k<span class="number">-1</span> <span class="comment"># 假设最开始的是k-1下标的退出</span></span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> range(n, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">        <span class="comment"># num表示现有的人数，会随着进行减少；i表示开退出的编号</span></span><br><span class="line">        i = (i + m<span class="number">-1</span>) % num</span><br><span class="line">        print(people.pop(i))</span><br><span class="line"></span><br><span class="line">josephos_l(<span class="number">10</span>, <span class="number">2</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure><p>得出结果为：<code>6 1 7 3 10 9 2 5 8 4</code></p><p>该算法的复杂度是 $O(n^2)$。外出循环体执行n次；内层的pop操作也是需要线性的时间，它的复杂度是n。</p><h2 id="基于循环单链表的解法"><a href="#基于循环单链表的解法" class="headerlink" title="基于循环单链表的解法"></a>基于循环单链表的解法</h2><p>采用循环单链表来实现，顺序报数，可以认为是沿着结点的next引用一直向后，退出后，就删除该结点。</p><p>实现步骤：</p><ol><li>创建一个长度为n的链表，</li><li>循环链表，并删除确定的结点。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Josephos</span><span class="params">(SingleCycleLinkList)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">append</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        <span class="comment"># 重写一下尾部添加的方法</span></span><br><span class="line">        node = Node(item)</span><br><span class="line">        <span class="keyword">if</span> self.is_empty():</span><br><span class="line">            self._head = node</span><br><span class="line">            node.next = node</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            node.next = self._head</span><br><span class="line">            self._rear.next = node</span><br><span class="line">        self._rear = node</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">turn</span><span class="params">(self, m)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            self._rear = self._rear.next</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pop</span><span class="params">(self)</span>:</span></span><br><span class="line">        node = self._rear.next</span><br><span class="line">        <span class="keyword">if</span> self._rear.next == self._rear:</span><br><span class="line">            self._rear = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self._rear.next = self._rear.next.next</span><br><span class="line">        <span class="keyword">return</span> node.elem</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self, n, k, m)</span>:</span></span><br><span class="line">        <span class="comment"># 创建长度为n的链表</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n+<span class="number">1</span>):</span><br><span class="line">            self.append(i)</span><br><span class="line"></span><br><span class="line">        self.turn(k<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">while</span> self._rear:</span><br><span class="line">            self.turn(m<span class="number">-1</span>)</span><br><span class="line">            print(self.pop())</span><br></pre></td></tr></table></figure><p>执行后返回的结果为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">josephos_link = Josephos()</span><br><span class="line">josephos_link.run(<span class="number">10</span>, <span class="number">2</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure><p>结果为：<code>6 1 7 3 10 9 2 5 8 4</code></p><p>它的复杂度可以分为两部分，创建链表的复杂度是$O(n)$，遍历解决问题的复杂度是$O(n*m)$。</p>]]></content>
    
    <summary type="html">
    
      假设n个人围坐一圈，现在要求从第k个人开始报数，报到第m个人退出，然后从下一个人开始继续报数并按照同样规则退出，直至所有人退出。要求按照顺序输出退出人的编号。
    
    </summary>
    
    
      <category term="技术" scheme="https://codeeper.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="python" scheme="https://codeeper.com/tags/python/"/>
    
      <category term="数据结构" scheme="https://codeeper.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>链表介绍及python的实现</title>
    <link href="https://codeeper.com/2020/03/31/tech/python/structure/link-intro.html"/>
    <id>https://codeeper.com/2020/03/31/tech/python/structure/link-intro.html</id>
    <published>2020-03-30T16:00:00.000Z</published>
    <updated>2020-04-03T06:23:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>链表是线性表的一种实现方式，它的基本想法是：</p><ul><li>将表中的元素分别存放在各个独立的存储区内，存储区又称为结点；</li><li>在表中，可以通过任意结点找到与之相关的下一个结点；</li><li>在前一个结点上，通过链接的方式记录与下一个结点的关联。</li></ul><p>当找到组成表结构的第一个结点时，就能按照顺序找到属于这个表的其它结点。</p><h2 id="单链表"><a href="#单链表" class="headerlink" title="单链表"></a>单链表</h2><img width="80%" src="/images/struct/1.jpg" alt="单链表"><p>如上图(a)所示，单链表的结点是一个二元组，elem保存元素的数据，next表示下一个结点的标识。</p><p>为了掌握一个单链表，需要用一个变量来保存这个表的首结点的引用（或标识），该变量可以称之为<em>表头变量<em>或</em>表头指针</em></p><h3 id="基本链表操作"><a href="#基本链表操作" class="headerlink" title="基本链表操作"></a>基本链表操作</h3><ul><li>创建空链表</li><li>删除链表</li><li>判断链表是否为空</li><li>加入元素<ul><li>首端加入</li><li>尾端加入</li><li>中间加入</li></ul></li><li>删除元素<ul><li>首端删除</li><li>尾端删除</li><li>中间删除</li></ul></li><li>遍历链表</li><li>查找元素</li></ul><p>代码实现，里面实现几个典型方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>:</span></span><br><span class="line">    <span class="string">"""结点"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, elem)</span>:</span></span><br><span class="line">        self.elem = elem</span><br><span class="line">        self.next = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SingleLinkList</span>:</span></span><br><span class="line">    <span class="string">"""单链表"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self._head = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_empty</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""链表是否为空"""</span></span><br><span class="line">        <span class="keyword">return</span> self._head == <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">length</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""链表长度"""</span></span><br><span class="line">        <span class="comment"># cur游标，用来移动遍历结点</span></span><br><span class="line">        cur = self._head</span><br><span class="line">        <span class="comment"># count记录数量</span></span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> cur != <span class="literal">None</span>:</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">            cur = cur.next</span><br><span class="line">        <span class="keyword">return</span> count</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">travel</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""遍历整个链表"""</span></span><br><span class="line">        cur = self._head</span><br><span class="line">        <span class="keyword">while</span> cur != <span class="literal">None</span>:</span><br><span class="line">            print(cur.elem, end=<span class="string">" "</span>)</span><br><span class="line">            cur = cur.next</span><br><span class="line">        print(<span class="string">""</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        <span class="string">"""链表头部添加元素，头插法"""</span></span><br><span class="line">        node = Node(item)</span><br><span class="line">        node.next = self._head</span><br><span class="line">        self._head = node</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">append</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        <span class="string">"""链表尾部添加元素, 尾插法"""</span></span><br><span class="line">        node = Node(item)</span><br><span class="line">        <span class="keyword">if</span> self.is_empty():</span><br><span class="line">            self._head = node</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cur = self._head</span><br><span class="line">            <span class="keyword">while</span> cur.next != <span class="literal">None</span>:</span><br><span class="line">                cur = cur.next</span><br><span class="line">            cur.next = node</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert</span><span class="params">(self, pos, item)</span>:</span></span><br><span class="line">        <span class="string">"""指定位置添加元素</span></span><br><span class="line"><span class="string">        :param  pos 从0开始</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> pos &lt;= <span class="number">0</span>:</span><br><span class="line">            self.add(item)</span><br><span class="line">        <span class="keyword">elif</span> pos &gt; (self.length()<span class="number">-1</span>):</span><br><span class="line">            self.append(item)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pre = self._head</span><br><span class="line">            count = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> count &lt; (pos<span class="number">-1</span>):</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">                pre = pre.next</span><br><span class="line">            <span class="comment"># 当循环退出后，pre指向pos-1位置</span></span><br><span class="line">            node = Node(item)</span><br><span class="line">            node.next = pre.next</span><br><span class="line">            pre.next = node</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">remove</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        <span class="string">"""删除结点"""</span></span><br><span class="line">        cur = self._head</span><br><span class="line">        pre = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">while</span> cur != <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> cur.elem == item:</span><br><span class="line">                <span class="comment"># 先判断此结点是否是头结点</span></span><br><span class="line">                <span class="comment"># 头结点</span></span><br><span class="line">                <span class="keyword">if</span> cur == self._head:</span><br><span class="line">                    self._head = cur.next</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    pre.next = cur.next</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                pre = cur</span><br><span class="line">                cur = cur.next</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">search</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        <span class="string">"""查找结点是否存在"""</span></span><br><span class="line">        cur = self._head</span><br><span class="line">        <span class="keyword">while</span> cur != <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> cur.elem == item:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cur = cur.next</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure><p>通过上面的代码，可以说明一下单链表的操作复杂度：</p><ul><li>创建空链表：O(1)</li><li>判断链表是否为空：O(1)</li><li>链表长度：O(n)</li><li>加入元素：<ul><li>首端加入：O(1)</li><li>尾端加入：O(n)</li><li>中间加入：O(n)</li></ul></li><li>删除元素：<ul><li>首端删除：O(1)</li><li>尾端删除：O(n)</li><li>中间删除：O(n)</li></ul></li><li>遍历，查找：O(n)</li></ul><p>上面的单链表实现有个缺点，在尾端操作元素的效率低，只能从表头开始遍历到尾部。<br>在实际中，会添加一个尾部结点的引用，这样在尾部添加，删除元素测复杂度就降为O(1)。</p><h2 id="循环单链表"><a href="#循环单链表" class="headerlink" title="循环单链表"></a>循环单链表</h2><img width="80%" src="/images/struct/2.jpg" alt="循环单链表"><p>单链表的一种变形是循环单链表，它的最后一个结点的next不指向None，而是指向首结点的位置，如上图所示。</p><p>代码实现，部分和单链表类似</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SingleCycleLinkList</span>:</span></span><br><span class="line">    <span class="string">"""单向循环链表"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self._head = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        <span class="string">"""链表头部添加元素，头插法"""</span></span><br><span class="line">        node = Node(item)</span><br><span class="line">        <span class="keyword">if</span> self.is_empty():</span><br><span class="line">            self._head = node</span><br><span class="line">            node.next = node</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cur = self._head</span><br><span class="line">            <span class="keyword">while</span> cur.next != self._head:</span><br><span class="line">                cur = cur.next</span><br><span class="line">            <span class="comment"># 退出循环，cur指向尾结点</span></span><br><span class="line">            node.next = self._head</span><br><span class="line">            self._head = node</span><br><span class="line">            <span class="comment"># cur.next = node</span></span><br><span class="line">            cur.next = self._head</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">append</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        <span class="string">"""链表尾部添加元素, 尾插法"""</span></span><br><span class="line">        node = Node(item)</span><br><span class="line">        <span class="keyword">if</span> self.is_empty():</span><br><span class="line">            self._head = node</span><br><span class="line">            node.next = node</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cur = self._head</span><br><span class="line">            <span class="keyword">while</span> cur.next != self._head:</span><br><span class="line">                cur = cur.next</span><br><span class="line">            <span class="comment"># node.next = cur.next</span></span><br><span class="line">            node.next = self._head</span><br><span class="line">            cur.next = node</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert</span><span class="params">(self, pos, item)</span>:</span></span><br><span class="line">        <span class="string">"""指定位置添加元素</span></span><br><span class="line"><span class="string">        :param  pos 从0开始</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> pos &lt;= <span class="number">0</span>:</span><br><span class="line">            self.add(item)</span><br><span class="line">        <span class="keyword">elif</span> pos &gt; (self.length()<span class="number">-1</span>):</span><br><span class="line">            self.append(item)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pre = self._head</span><br><span class="line">            count = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> count &lt; (pos<span class="number">-1</span>):</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">                pre = pre.next</span><br><span class="line">            <span class="comment"># 当循环退出后，pre指向pos-1位置</span></span><br><span class="line">            node = Node(item)</span><br><span class="line">            node.next = pre.next</span><br><span class="line">            pre.next = node</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">remove</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        <span class="string">"""删除结点"""</span></span><br><span class="line">        <span class="keyword">if</span> self.is_empty():</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        cur = self._head</span><br><span class="line">        pre = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> cur.next != self._head:</span><br><span class="line">            <span class="keyword">if</span> cur.elem == item:</span><br><span class="line">                <span class="comment"># 先判断此结点是否是头结点</span></span><br><span class="line">                <span class="keyword">if</span> cur == self._head:</span><br><span class="line">                    <span class="comment"># 头结点的情况</span></span><br><span class="line">                    <span class="comment"># 找尾结点</span></span><br><span class="line">                    rear = self._head</span><br><span class="line">                    <span class="keyword">while</span> rear.next != self._head:</span><br><span class="line">                        rear = rear.next</span><br><span class="line">                    self._head = cur.next</span><br><span class="line">                    rear.next = self._head</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="comment"># 中间结点</span></span><br><span class="line">                    pre.next = cur.next</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                pre = cur</span><br><span class="line">                cur = cur.next</span><br><span class="line">        <span class="comment"># 退出循环，cur指向尾结点</span></span><br><span class="line">        <span class="keyword">if</span> cur.elem == item:</span><br><span class="line">            <span class="keyword">if</span> cur == self._head:</span><br><span class="line">                <span class="comment"># 链表只有一个结点</span></span><br><span class="line">                self._head = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># pre.next = cur.next</span></span><br><span class="line">                pre.next = self._head</span><br></pre></td></tr></table></figure><p>其它的方法和单链表一样。</p><p>同样的，可以添加一个尾部结点的引用，使得运算量下降。</p><h2 id="双向链表"><a href="#双向链表" class="headerlink" title="双向链表"></a>双向链表</h2><img width="50%" src="/images/struct/3.jpg" alt="带有尾结点引用的双向链表"><p>单链表只有一个方向的链接，从一个方向进行链表的遍历。为了是从两端的插入和删除操作都能高效完成，可以加入另一个方向的链接。如上图所示，就形成了<em>双向链表<em>，或</em>双链表</em></p><p>双向链表的结点除了next引用，还需要一个prev引用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""结点"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        self.elem = item</span><br><span class="line">        self.next = <span class="literal">None</span></span><br><span class="line">        self.prev = <span class="literal">None</span></span><br></pre></td></tr></table></figure><p>双向链表的一些操作要做一些修改：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DoubleLinkList</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""双链表"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self._head = <span class="literal">None</span></span><br><span class="line">        self._rear = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        <span class="string">"""链表头部添加元素，头插法"""</span></span><br><span class="line">        node = Node(item)</span><br><span class="line">        <span class="keyword">if</span> self.is_empty():</span><br><span class="line">            self._rear = node</span><br><span class="line"></span><br><span class="line">        node.next = self._head</span><br><span class="line">        self._head = node</span><br><span class="line">        node.next.prev = node</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">append</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        <span class="string">"""链表尾部添加元素, 尾插法"""</span></span><br><span class="line">        node = Node(item)</span><br><span class="line">        <span class="keyword">if</span> self.is_empty():</span><br><span class="line">            self._head = node</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cur = self._head</span><br><span class="line">            <span class="keyword">while</span> cur.next != <span class="literal">None</span>:</span><br><span class="line">                cur = cur.next</span><br><span class="line">            cur.next = node</span><br><span class="line">            node.prev = cur</span><br><span class="line">        self._rear = node</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert</span><span class="params">(self, pos, item)</span>:</span></span><br><span class="line">        <span class="string">"""指定位置添加元素</span></span><br><span class="line"><span class="string">        :param  pos 从0开始</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> pos &lt;= <span class="number">0</span>:</span><br><span class="line">            self.add(item)</span><br><span class="line">        <span class="keyword">elif</span> pos &gt; (self.length()<span class="number">-1</span>):</span><br><span class="line">            self.append(item)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cur = self._head</span><br><span class="line">            count = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> count &lt; pos:</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">                cur = cur.next</span><br><span class="line">            <span class="comment"># 当循环退出后，cur指向pos位置</span></span><br><span class="line">            node = Node(item)</span><br><span class="line">            node.next = cur</span><br><span class="line">            node.prev = cur.prev</span><br><span class="line">            cur.prev.next = node</span><br><span class="line">            cur.prev = node</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">remove</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        <span class="string">"""删除节点"""</span></span><br><span class="line">        cur = self._head</span><br><span class="line">        <span class="keyword">while</span> cur != <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> cur.elem == item:</span><br><span class="line">                <span class="comment"># 先判断此结点是否是头节点</span></span><br><span class="line">                <span class="comment"># 头节点</span></span><br><span class="line">                <span class="keyword">if</span> cur == self._head:</span><br><span class="line">                    self._head = cur.next</span><br><span class="line">                    <span class="keyword">if</span> cur.next:</span><br><span class="line">                        <span class="comment"># 判断链表是否只有一个结点</span></span><br><span class="line">                        cur.next.prev = <span class="literal">None</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        self._rear = <span class="literal">None</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    cur.prev.next = cur.next</span><br><span class="line">                    <span class="keyword">if</span> cur.next:</span><br><span class="line">                        cur.next.prev = cur.prev</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        self._rear = cur.prev</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cur = cur.next</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>链接表的优点：</p><ul><li>由于表结构是由链接起来的结点形成，表结构很容易修改；</li><li>整个表由一些小的存储块构成，比较容易安排和管理，不需要连续的内存块。</li></ul><p>缺点：</p><ul><li>链表元素的定位需要线性的时间，比列表效率底；</li><li>链表的存储代价会比列表高。</li></ul>]]></content>
    
    <summary type="html">
    
      介绍的单链表，循环链表和双链表的实现过程，以及使用python对它们的代码实现。
    
    </summary>
    
    
      <category term="技术" scheme="https://codeeper.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="python" scheme="https://codeeper.com/tags/python/"/>
    
      <category term="数据结构" scheme="https://codeeper.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>在大数据下应用机器学习算法</title>
    <link href="https://codeeper.com/2020/02/09/tech/machine_learning/learn_large_datasets.html"/>
    <id>https://codeeper.com/2020/02/09/tech/machine_learning/learn_large_datasets.html</id>
    <published>2020-02-09T02:00:00.000Z</published>
    <updated>2020-02-14T05:44:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>大量的数据对机器学习算法的研究很有帮助，当我们直到算法有较高的方差（variance）时，增加m会有助于改善算法。<br>但是当m很大时，比如m=100,000,000时，在这种情况下，在计算时就会消耗更多的成本。</p><h3 id="使用随机梯度下降或小批量梯度下降"><a href="#使用随机梯度下降或小批量梯度下降" class="headerlink" title="使用随机梯度下降或小批量梯度下降"></a>使用随机梯度下降或小批量梯度下降</h3><p>在使用批量梯度下降时，假设有1亿的样本，就要对1亿的样本求和，这样的成本太高了，使用随机梯度下降，每次对一个样本，或者小批量梯度下降，对几十个样本求和，可以大大减小计算，加快速度。</p><h3 id="映射约减（Map-Reduce），并行化处理"><a href="#映射约减（Map-Reduce），并行化处理" class="headerlink" title="映射约减（Map Reduce），并行化处理"></a>映射约减（Map Reduce），并行化处理</h3><p>在设计网站时，当用户量增加时，一台服务器不足以承载这么多用户，我们会增加服务器，用负载均衡来解决。<br>同样的在解决大量数据的机器学习问题上，一台机器有时也不足以完成任务。</p><p>以线性回归为例，假设有400个样本，它的批量梯度下降的内部迭代为：<br>$$ \theta_j:=\theta_j−\alpha \left( \frac{1}{400} \sum_{i=1}^{400}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \right) $$</p><p>同时有四台机器，将样本分为四份：<br>机器1： $x^{(1)},x^{(2)},…,x^{(100)}$；<br>机器2： $x^{(101)},x^{(102)},…,x^{(200)}$；<br>机器3： $x^{(201)},x^{(202)},…,x^{(300)}$；<br>机器4： $x^{(301)},x^{(302)},…,x^{(400)}$。</p><p>分别将梯度下降大括号里的求和分成4在四个机器上运算：<br>机器1：$temp_j^{(1)}=\sum_{i=1}^{100}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$<br>机器2：$temp_j^{(2)}=\sum_{i=1}^{200}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$<br>机器3：$temp_j^{(3)}=\sum_{i=1}^{300}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$<br>机器4：$temp_j^{(4)}=\sum_{i=1}^{400}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$</p><p>然后将4份数据汇总到另一台机器上做求和运算。<br>$$\theta_j := \theta_j - \alpha \dfrac{1}{400}(temp_j^{(1)} + temp_j^{(2)} + temp_j^{(3)} + temp_j^{(4)})$$<br>这样能将原来的速度提升解决4倍，这是一个很大的提升。</p><p>将类似的求和运算分摊到多个机器上，它就是映射约减。<br>在线性回归和逻辑回归问题上很容易实现，而在神经网络上，也可以在多台机器上运行正向传播和反向传播，然后将结果汇总到主服务器上，加快运行速度。</p><p>处理使用多台计算机实现并行化，一台多核处理器的计算机也能实现类似的效果。</p><h3 id="在线学习（Online-Learning）"><a href="#在线学习（Online-Learning）" class="headerlink" title="在线学习（Online Learning）"></a>在线学习（Online Learning）</h3><p>在一个大型网站中，有不断的用户访问，数据源保持不断，这时就可以使用在线学习系统在解决一些问题。</p><p>与通常的机器学习问题使用固定的数据集来训练算法不同，在线学习每次处理单独的样本，在利用单独的样本更新完参数$\theta$，改善算法性能后就丢弃该样本。这与随机梯度下降的处理方式有类似之处，区别就是在线学习不会使用一个固定的数据集。</p><p>算法过程：<br>一直迭代下去 {<br>　　获取单独的数据$(x,y)$;<br>　　通过该样本更新$\theta$ {<br>　　　　$ \theta_j:=\theta_j−\alpha (h_\theta(x)-y)x_j $<br>　　}<br>}</p><p>这种做法的优点在于可以减少数据的存储空间和计算速度，在大型网站中，数据是海量的，要集中处理时占用的硬盘和内存空间非常大，也非常耗时。而当有连续的数据时，就可以使用在线学习来处理。<br>另一个优点是可以适应业务不同发展的需求和不同时段用户的需求，因为参数$\theta$会随着输入样本的不同而改变。</p>]]></content>
    
    <summary type="html">
    
      大量的数据对机器学习算法的研究很有帮助，当我们直到算法有较高的方差（variance）时，增加m会有助于改善算法。
    
    </summary>
    
    
      <category term="技术" scheme="https://codeeper.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="机器学习" scheme="https://codeeper.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>小批量梯度下降算法</title>
    <link href="https://codeeper.com/2020/02/08/tech/machine_learning/mini_batch_gradient_descent.html"/>
    <id>https://codeeper.com/2020/02/08/tech/machine_learning/mini_batch_gradient_descent.html</id>
    <published>2020-02-08T10:00:00.000Z</published>
    <updated>2020-02-14T05:44:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>批量梯度下降是每一次迭代中对所有样本求和后求梯度项，随机梯度下降是对每一个样本求梯度项，小批量梯度下降介于两者之间。</p><p>小批量梯度下降取的样本数一般在2-100之间：<br>例如样本总数m=1000，每次取样本b=10:</p><p>重复 {<br> 　　for i = 1,11,21,…,991 {<br>　　　　$ \theta_j:=\theta_j−\alpha \frac{1}{10}\sum_{k=i}^{i+9}(h_\theta(x^{(k)})-y^{(k)})x_j^{(k)} $<br>　　}<br>}</p><p>一次计算多个样本的优点是，多个样本能够以矢量化的方式进行，能够多个样本并行化处理，又不至于对所有样本求和那个耗费计算成本。<br>在某些情况下，小批量梯度下降的速度会优于随机梯度下降，但是在选择小批量样本数量b时会耗费一些时间。</p>]]></content>
    
    <summary type="html">
    
      批量梯度下降是每一次迭代中对所有样本求和后求梯度项，随机梯度下降是对每一个样本求梯度项，小批量梯度下降介于两者之间。
    
    </summary>
    
    
      <category term="技术" scheme="https://codeeper.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="机器学习" scheme="https://codeeper.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>随机梯度下降算法</title>
    <link href="https://codeeper.com/2020/02/08/tech/machine_learning/stochastic_gradient_descent.html"/>
    <id>https://codeeper.com/2020/02/08/tech/machine_learning/stochastic_gradient_descent.html</id>
    <published>2020-02-08T05:00:00.000Z</published>
    <updated>2020-02-14T05:46:37.000Z</updated>
    
    <content type="html"><![CDATA[<p>以线性回归为例：<br>预测函数为：<br>$$ h_\theta(x) = \theta^Tx $$<br>代价函数：<br>$$ J_{train}(\theta) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2 $$</p><p>重复：{<br>　　$ \theta_j:=\theta_j−\alpha \left( \frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \right) $<br>}</p><p>当数据量过大时，梯度下降的算法会变得很慢，因为要对所有的数据进行求和。因为每次重复梯度下降都是所有数据全部求和，所以梯度下降算法又称之为<strong>批量梯度下降（Batch Gradient Descent）</strong></p><h2 id="概念说明"><a href="#概念说明" class="headerlink" title="概念说明"></a>概念说明</h2><p>随机梯度下降在每一次迭代中，不用考虑全部的样本，只需要考虑一个训练样本。</p><p>针对一个样本，它的代价函数：<br>$$ cost(\theta, (x^{(i)},y^{(i)})) = \frac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2 $$<br>而针对所有样本的代价函数可以看作是对每个样本代价函数的平均：<br>$$ J_{train}(\theta) = \frac{1}{m}\sum_{i=1}^m cost(\theta, (x^{(i)},y^{(i)})) $$</p><p>随机梯度下降算法如下：<br>第一步，先随机打乱训练集样本。<br>第二步，进行梯度下降：<br>重复 {<br>　　循环所有样本 for i=1,2,3,…,m {<br>　　　　$ \theta_j:=\theta_j−\alpha (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}  $<br>　　}<br>}</p><p>一开始随机打乱数据是为了对样本集的访问是随机的，会让梯度下降的速度快一点。</p><p>该算法一次训练一个样本，对它的代价函数进行一小步梯度下降，修改参数$\theta$，使得它对该样本的拟合会好一点；然后再对下一个样本进行运算，直到扫描完所有的训练样本，最后外部在迭代这个过程。</p><p>跟批量梯度下降算法不同的是，随机梯度下降不需要等到所有样本求和来得到梯度项，而是在对每个样本就可以求出梯度项，在对每个样本扫描的过程中就已经在优化参数了。</p><p>在梯度下降过程中，批量梯度下降的过程趋向于一条直线，直接收敛到全局最小值；而随机梯度下降不太可能收敛到全局最小值，而是随机地在其周围震荡，但通常会很接近最小值。</p><p>随机梯度下降通常需要经过1-10次外部循环才能接近全局最小值。</p><h2 id="判断收敛"><a href="#判断收敛" class="headerlink" title="判断收敛"></a>判断收敛</h2><p>在批量梯度下降中，要判断是否收敛，需要在每一次迭代算法后计算$J_{train}$的值，根据值的变化来判断收敛。<br>在执行随机梯度下降时，不需要计算所有的样本的代价函数，只用在对某个样本进行梯度下降前计算该样本的代价函数$cost(\theta, (x^{(i)},y^{(i)})) $，为了判断是否收敛，可以计算多次迭代后$cost(\theta, (x^{(i)},y^{(i)})) $的平均值，例如1000次迭代，在每次更新$\theta$前，计算最后1000次的的cost的平均值。</p><p>选择每隔多少次计算成本函数对梯度下降的过程也有影响：<br><img width="40%" src="/images/ml44.png" alt=""><br>上图中蓝色曲线是每1000次迭代，红色的是每隔5000次迭代。<br>因为随机梯度下降时会出现震荡，当迭代次数少时发现下降的曲线起伏很多，而迭代次数变大时，曲线就会变得平滑许多。缺点是每隔5000个计算，会增加计算成本。</p><p>增加迭代次数可以判断算法是否正确：<br><img width="40%" src="/images/ml45.png" alt=""><br>上图蓝色的是1000个迭代次数，通过这条曲线，不能很好的判断成本函数是否在下降，这时就需要添加迭代次数，当增加到5000次，则可以通过平滑的曲线判断，当下滑曲线是红色的时，说明算法是有效的，代价函数值在下降；当是紫色的曲线时，可以看到是一个平坦的线，这时判断算法可能出现问题了。</p><p>在随机梯度下降中，学习率$\alpha$也会影响算法，当学习率减小时，下降曲线的震荡就会变小，而且会收敛到一个更好的解：<br><img width="40%" src="/images/ml46.png" alt=""><br>图中红色的曲线时学习率更小的一个，可以看到震荡变小，且下降到一个更小的值。</p><p>当看到曲线是上升的时候，可以尝试减小学习率看看效果。</p><p>在随机梯度下降中，如果想要收敛到全剧最小值，需要随着时间的变化减小学习率$\alpha$的值：<br>$$ \alpha = \frac{const1}{iterNumber + const2} $$<br>学习率等于一个常数除以迭代次数加另一个常数，随着迭代次数增大，学习率会减小；但这会造成常数1和常数2的选择问题。</p>]]></content>
    
    <summary type="html">
    
      当数据量过大时，梯度下降的算法会变得很慢，因为要对所有的数据进行求和。因为每次重复梯度下降都是所有数据全部求和，所以梯度下降算法又称之为批量梯度下降（Batch Gradient Descent），随机梯度下降在每一次迭代中，不用考虑全部的样本，只需要考虑一个训练样本。
    
    </summary>
    
    
      <category term="技术" scheme="https://codeeper.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="机器学习" scheme="https://codeeper.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统和协同过滤算法</title>
    <link href="https://codeeper.com/2020/02/08/tech/machine_learning/recommender_system.html"/>
    <id>https://codeeper.com/2020/02/08/tech/machine_learning/recommender_system.html</id>
    <published>2020-02-07T23:00:00.000Z</published>
    <updated>2020-02-14T05:46:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>推荐系统是目前非常流行的机器学习应用。<br>特征值对机器学习是非常重要的，而对特征值的选择会直接影响到算法的好坏，推荐系统能够自动帮助学习一些优良的特征值，帮助更好的实现算法。</p><h2 id="举例说明"><a href="#举例说明" class="headerlink" title="举例说明"></a>举例说明</h2><p>以电影评分和推荐电影为例</p><p>先定义几个变量：<br>$n_u$=用户人数<br>$n_m$=电影数量<br>$r(i,j)=1$ 表示用户$j$评价了电影$i$<br>$y(i,j)$= 用户$j$对电影$i$的评分，只有在$r(i,j)=1$的时候才会有</p><p>首先电影评分分为0-5星。我们有4个用户和5部电影：</p><table><thead><tr><th align="left">电影</th><th align="left">Alice(1)</th><th align="left">Bob(2)</th><th align="left">Carol(3)</th><th align="left">Dave(4)</th></tr></thead><tbody><tr><td align="left">Love at last(1)</td><td align="left">5</td><td align="left">5</td><td align="left">0</td><td align="left">0</td></tr><tr><td align="left">Romance forever(2)</td><td align="left">5</td><td align="left">?</td><td align="left">?</td><td align="left">0</td></tr><tr><td align="left">Cute puppies of love(3)</td><td align="left">?</td><td align="left">4</td><td align="left">0</td><td align="left">?</td></tr><tr><td align="left">Nonstop car chases(4)</td><td align="left">0</td><td align="left">0</td><td align="left">5</td><td align="left">4</td></tr><tr><td align="left">Swords vs. karate(5)</td><td align="left">0</td><td align="left">0</td><td align="left">5</td><td align="left">?</td></tr></tbody></table><p>上表中$n_u=4,n_m=5$，电影$i=1,2,3$为爱情片，$i=4,5$为动作片，打问号的表示没有评分。</p><p>上面的表格中可以看到Alice和Bob对爱情电影评分很高，对动作片评分很低，Carol和Dave则相反。</p><p>现在给每部电影添加两个特征值：$x_1$表示浪漫指数，$x_2$表示动作指数：</p><table><thead><tr><th>电影</th><th>Alice(1)</th><th>Bob(2)</th><th>Carol(3)</th><th>Dave(4)</th><th>$x_1$(浪漫)</th><th>$x_2$(动作)</th></tr></thead><tbody><tr><td>Love at last(1)</td><td>5</td><td>5</td><td>0</td><td>0</td><td>0.9</td><td>0</td></tr><tr><td>Romance forever(2)</td><td>5</td><td>?</td><td>?</td><td>0</td><td>1</td><td>0.01</td></tr><tr><td>Cute puppies of love(3)</td><td>?</td><td>4</td><td>0</td><td>?</td><td>0.99</td><td>0</td></tr><tr><td>Nonstop car chases(4)</td><td>0</td><td>0</td><td>5</td><td>4</td><td>0.1</td><td>1</td></tr><tr><td>Swords vs. karate(5)</td><td>0</td><td>0</td><td>5</td><td>?</td><td>0</td><td>1</td></tr></tbody></table><p>用矩阵的形式来表示每个电影的特征值:<br>$ x^{(1)}=\left[ \begin{matrix} 0 \\ 0.9 \\ 0  \end{matrix} \right],<br>  x^{(2)}=\left[ \begin{matrix} 0 \\ 1 \\ 0.01  \end{matrix} \right],<br>  x^{(3)}=\left[ \begin{matrix} 0 \\ 0.99 \\ 0  \end{matrix} \right],<br>  x^{(4)}=\left[ \begin{matrix} 0 \\ 0.1 \\ 1  \end{matrix} \right],<br>  x^{(5)}=\left[ \begin{matrix} 0 \\ 0 \\ 1  \end{matrix} \right] $</p><p>想要预测问号的值，这是一个线性回归的问题。<br>对于用户$j$来说，要预测他对电影$i$的评分值，应用线性回归的模型，当通过算法获得来一个参数$\theta^{(j)}$，通过这个参数，计算$(\theta^{(j)})^T \cdot x^{(i)}$，即可预测出评分值。</p><p>假设要预测用户1对电影3的评分：用户1的参数$\theta^{(1)} = \left[ \begin{matrix} 0 \\ 5 \\ 0  \end{matrix} \right]$，计算他对电影3的评分：$(\theta^{(1)})^T \cdot x^{(3)} = 4.95$，即可预测他的评分为5星。</p><p>下面就是对每个用户，应用线性回归模型即可预测出他们对电影的评分。</p><p>用公式来表示一下：<br>对一个用户$j$，他的线性回归公式：<br>$$ min_{\theta^{(j)}} = \frac{1}{2m^{(j)}} \sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2m^{(j)}}\sum_{k=1}^n(\theta_k^{(j)})^2 $$<br>这就是常用的线性回归模型。</p><p>下面在公式上约去常数$m^{(j)}$项，这并不影响最小化代价函数：<br>$$ min_{\theta^{(j)}} = \frac{1}{2} \sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2}\sum_{k=1}^n(\theta_k^{(j)})^2 $$</p><p>然后计算所有用户加在一起的代价函数公式：<br>$$ min_{\theta^{(1)},…,\theta^{(n_u)}} = \frac{1}{2} \sum_{j=1}^{n_u} \sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^n(\theta_k^{(j)})^2 $$</p><p>对该公式应用梯度下降求最小值：<br>当$k=0$：<br>$$ \theta_k^{(j)} := \theta_k^{(j)} - \alpha \left(  \sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)} - y^{(i,j)})x_k^{(i)} \right) $$</p><p>当$k \not= 0$：<br>$$ \theta_k^{(j)} := \theta_k^{(j)} - \alpha \left(  \sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)} - y^{(i,j)})x_k^{(i)} + \lambda\theta_k^{(j)} \right) $$</p><h2 id="协同过滤（Collaborative-Filtering）"><a href="#协同过滤（Collaborative-Filtering）" class="headerlink" title="协同过滤（Collaborative Filtering）"></a>协同过滤（Collaborative Filtering）</h2><p>在一个电影网站中，很难去获得一部电影的浪漫指数和动作指数是多少，这个参数很难人为的去判断。为了解决这个问题，可以使用特征寻找器（<em>feature finders.</em>）</p><p>现在我们不知道电影的特征值是多少,$ x^{(i)}=\left[ \begin{matrix} 0 \\ ? \\ ?  \end{matrix} \right]$，但是我们通过某种途径得知用户对各种类型电影的喜爱程度，是喜欢动作电影还是喜欢爱情电影。$\theta_1$表示喜欢爱情电影的参数，$\theta_2$表示喜欢动作电影的参数</p><table><thead><tr><th>电影</th><th>Alice(1)</th><th>Bob(2)</th><th>Carol(3)</th><th>Dave(4)</th></tr></thead><tbody><tr><td>Love at last(1)</td><td>5</td><td>5</td><td>0</td><td>0</td></tr><tr><td>Romance forever(2)</td><td>5</td><td>?</td><td>?</td><td>0</td></tr><tr><td>Cute puppies of love(3)</td><td>?</td><td>4</td><td>0</td><td>?</td></tr><tr><td>Nonstop car chases(4)</td><td>0</td><td>0</td><td>5</td><td>4</td></tr><tr><td>Swords vs. karate(5)</td><td>0</td><td>0</td><td>5</td><td>?</td></tr><tr><td>$\theta_1$(浪漫)</td><td>5</td><td>5</td><td>0</td><td>0</td></tr><tr><td>$\theta_2$(动作)</td><td>0</td><td>0</td><td>5</td><td>5</td></tr></tbody></table><p>用矩阵的形式来表示每个用户的关于电影特征的参数值:<br>$ \theta^{(1)}=\left[ \begin{matrix} 0 \\ 5 \\ 0  \end{matrix} \right],<br>  \theta^{(2)}=\left[ \begin{matrix} 0 \\ 5 \\ 0  \end{matrix} \right],<br>  \theta^{(3)}=\left[ \begin{matrix} 0 \\ 0 \\ 5  \end{matrix} \right],<br>  \theta^{(4)}=\left[ \begin{matrix} 0 \\ 0 \\ 5  \end{matrix} \right]$</p><p>对一个电影$i$，要获得它的特征值$ x^{(i)}=\left[ \begin{matrix} ? \\ ? \\ ?  \end{matrix} \right]$，也可以看作一个线性回归问题。<br>同样的预测函数可以写作：$h = (\theta^{(j)})^T \cdot x^{(i)} = (x^{(i)})^T \cdot \theta^{(j)} $</p><p>那么对一个电影$i$，它的代价函数则是：<br>$$ min_{x^{(i)}} = \frac{1}{2} \sum_{j:r(i,j)=1}((\theta^{(j)})^Tx^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2}\sum_{k=1}^n(x_k^{(i)})^2 $$</p><p>然后计算所有电影加在一起的代价函数公式：<br>$$ min_{x^{(1)},…,x^{(n_m)}} = \frac{1}{2}  \sum_{i=1}^{n_m} \sum_{j:r(i,j)=1}((\theta^{(j)})^Tx^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^n(x_k^{(i)})^2 $$</p><p>同样的应用梯度下降来最小化代价函数。</p><p>通过上面的说明：<br>当我们有电影的特征值$x$时，可以预测出用户的属性$\theta$；当有用户的属性$\theta$可以预测出电影的特征值$x$，这样交替运行，就可以使系统更加完善。这就是基本的协同过滤算法。</p><h2 id="算法公式"><a href="#算法公式" class="headerlink" title="算法公式"></a>算法公式</h2><p>将上面的两个式子合并，同时最小化特征值和参数：<br>$$ J(x,\theta)=\frac{1}{2} \sum_{(i,j):r(i,j)=1}((\theta^{(j)})^Tx^{(i)} - y^{(i,j)})^2 +  \frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^{n}(x_k^{(i)})^2  +  \frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}(\theta_k^{(j)})^2  $$</p><p><em>PS：该式子中的$x$和$\theta$都是n维的向量，它们的偏差单元$x_0$和$\theta_0$都被移除了。</em></p><h3 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h3><ol><li>随机初始化$x^{(1)},…,x^{(n_m)},\theta^{(1)},…,\theta^{(n_u)}$一个很小的值。</li><li>使用梯度下降算法来最小化代价函数$J$。</li></ol><p>$$ x_k^{(i)} :=  x_k^{(i)} - \alpha \frac{\partial}{\partial x_k^{(i)} }J(x,\theta) := x_k^{(i)} - \alpha \left(  \sum_{j:r(i,j)=1}((\theta^{(j)})^Tx^{(i)} - y^{(i,j)})x_k^{(i)} + \lambda x_k^{(i)} \right) $$</p><p>$$ \theta_k^{(j)} :=  x_k^{(i)} - \alpha \frac{\partial}{\partial \theta_k^{(j)} }J(x,\theta) := \theta_k^{(j)} - \alpha \left(  \sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)} - y^{(i,j)})x_k^{(i)} + \lambda\theta_k^{(j)} \right) $$</p><p>这样下来可以对用户尚未评分的电影，通过预测评分大小来推荐电影。<br>对用户已评分的电影，可以根据评分和用户的属性参数来获得更好的电影特征值。</p><h2 id="其他应用"><a href="#其他应用" class="headerlink" title="其他应用"></a>其他应用</h2><p>协同过滤算法还可以用来推荐相似的产品，假如当用户看了一个电影$i$之后，可以判断其他电影和该电影的相似度来推荐，相似度的公式为$||x^{(i)} - x^{(j)}||$，当该式子越小时相似度越高，就可以据此来推荐电影。</p><h2 id="其他事项"><a href="#其他事项" class="headerlink" title="其他事项"></a>其他事项</h2><p>在上述电影网站中，除了上面的四个用户，又有一个新用户加入，他没有对任何电影评分，要预测他对某一个电影的评分，通常采用的方法取评过该电影评分的平均值$\mu$来当作预测值。</p>]]></content>
    
    <summary type="html">
    
      推荐系统是目前非常流行的机器学习应用。特征值对机器学习是非常重要的，而对特征值的选择会直接影响到算法的好坏，推荐系统能够自动帮助学习一些优良的特征值，帮助更好的实现算法。
    
    </summary>
    
    
      <category term="技术" scheme="https://codeeper.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="机器学习" scheme="https://codeeper.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="非监督学习" scheme="https://codeeper.com/tags/%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>异常检测（Anomaly Detection）</title>
    <link href="https://codeeper.com/2020/02/07/tech/machine_learning/anomaly_detection.html"/>
    <id>https://codeeper.com/2020/02/07/tech/machine_learning/anomaly_detection.html</id>
    <published>2020-02-07T01:00:00.000Z</published>
    <updated>2020-02-14T05:42:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>异常检测（Anomaly Detection）是机器学习算法的一个常见应用。它主要用于非监督学习，但又类似一些监督学习问题。</p><p>异常检测常用在对网站异常用户的检测；还有在工程上一些零件，设备异常的检查；还有机房异常机器的监控等等</p><h2 id="概念说明"><a href="#概念说明" class="headerlink" title="概念说明"></a>概念说明</h2><p>假设有数据集$x^{(1)},x^{(2)},…,x^{(m)}$，当又有一个新的测试样本$x_{test}$；<br>想要知道这个新样本是否是异常的；<br>首先对x的分布概率建模p(x) ，用来说明这个例子不是异常的概率；<br>然后定一个阈值$\epsilon$，当$p(x)&lt;\epsilon$时说明是异常的。</p><img width="40%" src="/images/ml_33.jpg" alt=""><p>当出现在高概率分布的区域时，说明该例子时正常的；当出现在低概率的区域时，说明是异常的。</p><h2 id="高斯分布"><a href="#高斯分布" class="headerlink" title="高斯分布"></a>高斯分布</h2><p>高斯分布又被称之为正态分布，曲线呈钟型，两头低，中间高，左右对称因其曲线呈钟形，因此人们又经常称之为钟形曲线</p><img width="40%" src="/images/ml_34.jpg" alt=""><p>假设x是一个实数随机变量，如果它的概率分布为高斯分布，定义几个变量：<br>$\mu$=平均值<br>$\sigma$=标准差<br>$\sigma^2$=方差<br>那么x的概率分布可以用公式来表示：<br>$$x \sim \mathcal{N}(\mu, \sigma^2)$$</p><p>其平均值$\mu$决定了其位置，其标准差$σ\sigma$决定了分布的幅度</p><p>完整的高斯分布的概率公式为：<br>$$\large p(x;\mu,\sigma^2) = \dfrac{1}{\sigma\sqrt{(2\pi)}}exp(-\dfrac{1}{2}(\dfrac{x - \mu}{\sigma})^2)$$</p><p>当参数平均值$\mu$和标准差$\sigma$变化时：</p><div style="display:flex;"><img  src="/images/ml35.jpg" alt=""><img  src="/images/ml36.jpg" alt=""><img  src="/images/ml37.jpg" alt=""><img  src="/images/ml38.jpg" alt=""></div><p>关于平均值和方差的求解：<br>$$\mu=\frac{1}{m}\sum_{i=1}^m x^{(i)}, \sigma^2=\frac{1}{m}\sum_{i=1}^m (x^{(i)} - \mu)^2 $$</p><h2 id="异常检测算法"><a href="#异常检测算法" class="headerlink" title="异常检测算法"></a>异常检测算法</h2><p>在一个异常检测的例子中，有m个训练样本，每个样本的特征值数量有n个，那么某个样本的分布概率模型p(x)就可以用样本的每个特征值的概率分布来计算：<br>$$p(x) = p(x_1;\mu_1,\sigma_1^2)p(x_2;\mu_2,\sigma^2_2)\cdots p(x_n;\mu_n,\sigma^2_n)$$<br>上面的式子可以用更简洁的方式来表达<br>$$p(x) = \displaystyle \prod^n_{j=1} p(x_j;\mu_j,\sigma_j^2)$$</p><p>总计一下，异常检测的过程：</p><ol><li>在训练样本中提取觉得可能会导致异常的特征值$x_i$</li><li>计算训练样本的参数$\mu_1,…,\mu_n,\sigma_1^2,…,\sigma_n^2$<br>$$\mu_j=\frac{1}{m}\sum_{i=1}^m x_j^{(i)}, \sigma_j^2=\frac{1}{m}\sum_{i=1}^m (x_j^{(i)} - \mu_j)^2 $$</li><li>当有一个新的测试样本$x_{test}$时，计算它的分布概率：<br>$$p(x) = \displaystyle \prod^n_{j=1} p(x_j;\mu_j,\sigma_j^2) = \prod\limits^n_{j=1} \dfrac{1}{\sqrt{2\pi}\sigma_j}exp(-\dfrac{(x_j - \mu_j)^2}{2\sigma^2_j})$$</li><li>和阈值ϵ比较，当 p(x)&lt;ϵ 时，则为异常。</li></ol><h2 id="评估异常检测算法"><a href="#评估异常检测算法" class="headerlink" title="评估异常检测算法"></a>评估异常检测算法</h2><p>如何评估一个异常检测算法，以及如何开发一个关于异常检测的应用：</p><p>首先，在获取到的一堆数据中，取一大部分正常的（可能包含少部分异常）的数据用于训练集来训练分布概率公式p(x)。</p><p>然后，在交叉验证和测试集中使用包含正常和一定比例异常的数据，来通过查准率和召回率，以及F值公式来评价一个算法。</p><p><strong>举个例子</strong></p><p>假设有：</p><ul><li>10000个正常的样本</li><li>20个异常的样本</li></ul><p>下面分割一下训练集，交叉验证集和测试集：</p><ul><li>训练集：6000个正常的</li><li>交叉验证集：2000个正常的+10个异常的</li><li>测试集：2000个正常的+10个异常的</li></ul><p>在训练集上训练出概率分布函数p(x)<br>在交叉验证集上，预测y：</p><p>$y = \begin{cases} 1 　　 p(x) &lt; \epsilon \\  0 　　 p(x) \geq \epsilon  \end{cases}$</p><p>下面通过和真实标签的比较，可以计算出 查准率（Precision）和召回率（Recall），然后通过F值公式来得到一个数值。</p><p>我们可以通过改变不同的阈值$\epsilon$从而得到不同的评价系数来选取一个最佳的阈值。<br>当得到的评价系数不佳时，也可以通过改变特征值的种类和数量来获取理想的评价系数</p><h2 id="特征值的选择"><a href="#特征值的选择" class="headerlink" title="特征值的选择"></a>特征值的选择</h2><p>在使用异常检测时，对性能影响最大的因素是特征值的选择。</p><p>首先要对特征向量使用高斯分布来建模，通常情况下，我们得到的原始数据并没有呈现高斯分布，例如这种：</p><img width="40%" src="/images/ml39.jpg" alt=""><p>有几种方法可以实现：</p><ul><li>log(x)</li><li>log(x+c)</li><li>$\sqrt{x}$</li><li>$x^{\frac{1}{3}}$</li></ul><p>通过上述办法，可以将数据转换成高斯分布的形式。</p><p>异常检测有点类似监督学习中的二元分类问题。<br>我们的目标是使得p(x)对于正常的数据来说是大的，而对于异常的数据来说是很小的，而在异常检测中一个常见的问题是最终我们的到的p(x)对于正常和异常的都很大。<br>在这种情况下需要观察一下交叉验证集中的异常示例，尝试找出能更好区分数据的新特性。</p><p><strong>例子</strong></p><p>例如，有一个关于机房机器的样本示例，开始收集的样本示例中包含的特征值有关于cpu负载和网络流量的。</p><p>cpu负载和网络流量是呈线性关系的，当网络流量变大时，cpu也会相应增大。</p><p>现在有一个异常的示例是网络流量不大，cpu确负载很大。假如在只有这两个特征值的情况下运行异常检测算法得出的p(x)，可能就效果不佳。这时可以添加一个特征值，是流量和cpu的比例关系，这样就约束来上述的异常示例，通过这三个特征值得到的异常检测算法可能就会好一点。</p><h2 id="异常检测和监督学习的区别"><a href="#异常检测和监督学习的区别" class="headerlink" title="异常检测和监督学习的区别"></a>异常检测和监督学习的区别</h2><p>异常检测一般用于：<br>样本中$y=1$的数量非常少（0-50个），而$y=0$的非常多。这样由于样本数量的过少，达不到良好的训练效果，而在异常检测中确能够表现良好。<br>还有就是导致$y=1$的情况非常多，且有不可预见性。</p><p>监督学习一般用于：<br>样本中$y=1$和$y=0$的数量都非常多。这样就有足够的样本数量去训练算法。</p><h2 id="多元高斯分布（Multivariate-Gaussian-Distribution）"><a href="#多元高斯分布（Multivariate-Gaussian-Distribution）" class="headerlink" title="多元高斯分布（Multivariate Gaussian Distribution）"></a>多元高斯分布（Multivariate Gaussian Distribution）</h2><p>多元高斯分布是异常检测的一种推广，它可能会检测到更多的异常。</p><p>在原始高斯分布中，模型p(x)的搭建是通过分别计算$p(x_1),p(x_2),…,p(x_n)$来完成的，而多元高斯分布则是一步到位，直接计算出模型：<br>$$p(x;\mu,\Sigma) = \dfrac{1}{(2\pi)^{n/2} |\Sigma|^{1/2}} exp(-1/2(x-\mu)^T\Sigma^{-1}(x-\mu))$$<br><em>PS：$\Sigma$是一个协方差矩阵。</em></p><p>通过改变$\mu$和$\Sigma$可以得到不同的多元高斯分布图：</p><div style="display:flex;"><img  src="/images/ml40.jpg" alt=""><img  src="/images/ml41.jpg" alt=""><img  src="/images/ml42.jpg" alt=""><img  src="/images/ml43.jpg" alt=""></div><p>原始高斯分布模型，它的多个特征值之间的关系是轴对齐的（axis-aligned），两个或多个高斯分布之间没有相关性。<br>而多元高斯分布能够自动捕获x的不同特征之间的相关性。因此它在图像上会现实椭圆或有斜率的椭圆。</p><p>在平常的使用中，一般是使用原始高斯分布模型的，因为它的计算成本比较低。<br>在多元高斯分布中，因为要计算多个特征值之间的相关性，导致计算会慢很多，而且当特征值很多是，协方差矩阵就会很大，计算它的逆矩阵就会花费很多时间。</p><h3 id="何时使用多元高斯分布"><a href="#何时使用多元高斯分布" class="headerlink" title="何时使用多元高斯分布"></a>何时使用多元高斯分布</h3><p>要保证样本数量m大于特征值数量n，否则协方差矩阵会不可逆；<br>根据经验法则，当$m&gt;10n$时，多元高斯分布会表现良好。</p><p>在原始高斯分布模型中可以手动添加相关性高的特征值之间的关系，可以避免了使用多元高斯分布，减小计算成本。</p>]]></content>
    
    <summary type="html">
    
      异常检测（Anomaly Detection）是机器学习算法的一个常见应用。它主要用于非监督学习，但又类似一些监督学习问题。异常检测常用在对网站异常用户的检测；还有在工程上一些零件，设备异常的检查；还有机房异常机器的监控等等
    
    </summary>
    
    
      <category term="技术" scheme="https://codeeper.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="机器学习" scheme="https://codeeper.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="非监督学习" scheme="https://codeeper.com/tags/%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>PCA降维算法</title>
    <link href="https://codeeper.com/2020/02/06/tech/machine_learning/dimensionality_reduction.html"/>
    <id>https://codeeper.com/2020/02/06/tech/machine_learning/dimensionality_reduction.html</id>
    <published>2020-02-06T09:00:00.000Z</published>
    <updated>2020-02-14T05:43:17.000Z</updated>
    
    <content type="html"><![CDATA[<p>降维是机器学习中很重要的一种思想。在机器学习中经常会碰到一些高维的数据集，它们会占用计算机的内存和硬盘空间，而且在运算时会减缓速度。<br>降维能够使得数据量被压缩，加快运算速度，减小储存空间，以及方便可视化的观察数据特点。</p><p><em>PS：在降维中，我们减少的是特征种类而不是样本数量，样本数量m不变，特征值数量n会减少。</em></p><h2 id="主成分分析算法（PCA）"><a href="#主成分分析算法（PCA）" class="headerlink" title="主成分分析算法（PCA）"></a>主成分分析算法（PCA）</h2><p>一种常用的降维算法是主成分分析算法（Principal Component Analysis），简称<strong>PCA</strong>。</p><p>PCA是通过找到一个低维的线或面，然后将数据投影到线或面上去，然后通过减少投影误差（即每个特征到投影的距离的平均值）来实现降维。</p><img width="40%" src="/images/ml_32.jpg" alt=""><p>上图是一个包含二维特征值的样本集。黑色的叉代表样本，红色的线表示找到的低维的线，绿色的叉则是样本投影在线上的位置。而它们的投影距离就是PCA算法所需要考虑的。</p><p>通过上图可以看出PCA算法就是找出一个线，在数学上就是一个向量，使得其他样本投影到该向量上的距离最小。</p><p>推而广之：<br>一般情况下，将特征值的维度从n降到k，就是找到k个向量$u^{(1)},u^{(2)},…,u^{(k)}$，使得样本在这些向量上的投影最小。</p><p>例如，2维降到1维，就是找到1个向量，即一条线；3维降到2维，就是找到2向量，即一个平面。</p><h3 id="PCA和线性回归的区别"><a href="#PCA和线性回归的区别" class="headerlink" title="PCA和线性回归的区别"></a>PCA和线性回归的区别</h3><ul><li>在线性回归中，最小化的是没有样本到预测线的平方误差，它们之间的距离是垂直距离。</li><li>在PCA中，最小化的是投影距离，或者说是正交距离。</li></ul><h3 id="算法过程"><a href="#算法过程" class="headerlink" title="算法过程"></a>算法过程</h3><p><strong>数据处理</strong></p><p>假设有m个样本集：$x^{(1)},x^{(2)},…,x^{(m)}$<br>下面需要对数据做一下特征值缩放或者均值归一化。<br>先计算出平均值，然后用样本值减去平均值。<br>$\mu_i=\frac{1}{m}\sum_{i=1}^m x_j^{(i)}$<br>然后用$\frac{x_j^{(i)}-\mu_i}{s_j}$ 替换$ x_j^{(i)}$，$s_j$可以是数据最大值最小值的范围或者标准差。</p><p><strong>算法部分</strong></p><ol><li>计算协方差矩阵（covariance matrix）<br>$$\Sigma = \frac{1}{m}\sum^m_{i=1}(x^{(i)})(x^{(i)})^T$$</li></ol><p><em>PS：假设$x^{(i)}$为一个n维向量（n * 1）,$(x^{(i)})(x^{(i)})^T$则是一个n维方阵。</em></p><ol start="2"><li><p>计算协方差矩阵的特征向量（eigenvectors）<br>可以通过奇异值分解（SVD）来获得：</p><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="meta">U,S,V</span>] = svd(Sigma);</span><br></pre></td></tr></table></figure><p>我们需要的就是矩阵U，他是一个n维方阵$U \in \mathbb{R}^{n \times n}$，它的每一列就是我们需要的向量：<br>$$ U = \left[ \begin{matrix} u^{(1)} &amp; u^{(2)} &amp; … &amp; u^{(n)} \end{matrix} \right]$$</p></li><li><p>获取Ureduce矩阵<br>当要从n降维到k，从矩阵U中取前k个向量就可以了：<br>$$Ureduce = \left[ \begin{matrix} u^{(1)} &amp; u^{(2)} &amp; … &amp; u^{(k)} \end{matrix} \right]$$</p></li><li><p>计算投影后的矩阵<br>$$ z^{(i)} = Ureduce^T \cdot x^{(i)} $$</p></li></ol><p><em>PS：$Ureduce^T$ 是一个  k×n 维的矩阵，$x^{(i)}$ 是一个n×1维的向量，两者点乘后就是一个z×1的向量，这样就得到了我们需要的。</em></p><h3 id="回到原来的维度"><a href="#回到原来的维度" class="headerlink" title="回到原来的维度"></a>回到原来的维度</h3><p>使用$Ureduce$矩阵可以降维：<br>$$ z^{(i)} = Ureduce^T \cdot x^{(i)} $$<br>那么要回到原来的维度上去就需要：<br>$$ x_{approx}^{(i)} = Ureduce \cdot z^{(i)} $$</p><p><em>这里我们只能得到原来的近似值</em></p><h3 id="选择降维的维度"><a href="#选择降维的维度" class="headerlink" title="选择降维的维度"></a>选择降维的维度</h3><p>$x_{approx}^{(i)}$与$ x^{(i)}$ 近似相等，两者之间的差就是投影误差，或平均平方映射误差：<br>$$ \frac{1}{m}\sum_{i=1}^m || x^{(i)} - x_{approx}^{(i)} ||^2 $$</p><p>数据的总变差（total variation），即样本的长度平方的均值：<br>$$ \frac{1}{m}\sum_{i=1}^m || x^{(i)} ||^2 $$</p><p>选择维度k的最小值的方法：<br>$$ \frac{\frac{1}{m}\sum_{i=1}^m || x^{(i)} - x_{approx}^{(i)} ||^2}{\frac{1}{m}\sum_{i=1}^m || x^{(i)} ||^2} \leq 0.01 $$</p><p>表示平方投影误差除以总变差的值小于0.01，用PCA的语言称之为<strong>保留了99%的差异性</strong>。<br><em>PS：这个值是可以变化的，可以是95%，90%，85%等等。</em></p><p><strong>使用循环验证的办法：</strong><br> 初始化$k=1$，然后计算出$Ureduce$，通过$Ureduce$计算出$z^{(1)},z^{(2)},…,z^{(m)} $和$x_{approx}^{(1)},x_{approx}^{(2)},…,x_{approx}^{(m)}$，然后通过上方的公式计算出值是不是小于0.01。<br>如果不是，增加k值，直到获得最小的k值满足条件。</p><p><strong>快捷办法</strong></p><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="meta">U,S,V</span>] = svd(Sigma);</span><br></pre></td></tr></table></figure><p>通过奇异值分解的到的矩阵$S$是一个n维的对角矩阵：<br>$$ S = \left[ \begin{matrix} s_{11} &amp; 0  &amp; … &amp; 0 \\ 0 &amp; s_{22} &amp; … &amp; 0 \\ … &amp; … &amp; … &amp; … \\ 0 &amp; 0 &amp; … &amp; s_{nn}  \end{matrix} \right] $$</p><p>通过这个矩阵可以来计算：<br>$$ 1 - \frac{\sum_{i=1}^k S_{ii}}{\sum_{i=1}^n S_{ii}}  \leq 0.01 $$<br>也可以用下面的式子：<br>$$ \frac{\sum_{i=1}^k S_{ii}}{\sum_{i=1}^n S_{ii}}  \geq 0.99 $$<br>这种方法就非常快捷高效。</p><h2 id="使用PCA的建议"><a href="#使用PCA的建议" class="headerlink" title="使用PCA的建议"></a>使用PCA的建议</h2><p>我们在训练集上通过PCA获得矩阵$Ureduce$，在交叉验证集和测试集上就不能再使用PCA来计算矩阵了，而是直接用训练集里的矩阵来映射交叉验证集和测试集上的数据。</p><p>PCA最常用的就是压缩数据，加速算法的学习，或者可视化数据。</p><ul><li>压缩数据<br>将数据压缩到合适的维度，用来加速算法。</li><li>数据可视化<br>将数据的维度降到2或3维以便画出图像。</li></ul><p><strong>PCA的错误用法，用来防止算法过拟合</strong><br>算法过拟合的原因之一是算法过于复杂，特征值的维度过高，使用PCA可以降低维度，看起来会有效，但是实际上效果很差。防止算法过拟合还是使用正则化的方法来实现。</p><p>还有一个注意点。就是在设计一个机器学习算法时，不用一开始就考虑降维，先在不使用PCA的条件下设计算法，当算法出现问题，例如，算法计算过慢，占用大量内存…，之后当确定需要使用PCA的时候再继续使用。</p>]]></content>
    
    <summary type="html">
    
      降维是机器学习中很重要的一种思想。在机器学习中经常会碰到一些高维的数据集，它们会占用计算机的内存和硬盘空间，而且在运算时会减缓速度。降维能够使得数据量被压缩，加快运算速度，减小储存空间，以及方便可视化的观察数据特点。
    
    </summary>
    
    
      <category term="技术" scheme="https://codeeper.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="机器学习" scheme="https://codeeper.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="非监督学习" scheme="https://codeeper.com/tags/%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>K均值算法介绍</title>
    <link href="https://codeeper.com/2020/02/06/tech/machine_learning/k_means_intro.html"/>
    <id>https://codeeper.com/2020/02/06/tech/machine_learning/k_means_intro.html</id>
    <published>2020-02-06T05:00:00.000Z</published>
    <updated>2020-02-14T05:43:54.000Z</updated>
    
    <content type="html"><![CDATA[<p>从没有标记过的数据中学习称之为非监督学习。<br>在非监督学习中，通过算法来定义一些数据的结构，将数据分别聚合到这些子集中，这种算法称之为聚类算法。</p><p>K均值 (K-means) 算法是最常用的一种聚类算法。</p><h2 id="K-means算法"><a href="#K-means算法" class="headerlink" title="K-means算法"></a>K-means算法</h2><p>$[x^{(1)},x^{(2)},x^{(3)},…,x^{(m)}]$<br>假设有如上的数据集，可以看到只有输入$x$，没有输出$y$。</p><p>下面说明一下K均值算法的过程</p><ol><li>首先确认聚类的簇的个数，$K=?$</li><li>随机初始化K个聚类的中心点，$\mu_1,\mu_2,…,\mu_k$</li><li>集群分配：将所有的数据集分配到里离它最近的聚类中心，并用$c^{(i)}$表示聚类中心的索引。例如第3个样本$x^{(3)}$，离它最近的一个聚类中心是第2个($\mu_2$)，那个$c^{(3)}=2$</li></ol><p><em>通过比较$||x^{(i)}-\mu_k||^2$的大小，算出$x^{(i)}$距离最近的中心点。</em><br>4. 移动质心：每个聚类中心所分配到的样本，求出这些样本的平均值，然后将原来的聚类中心移到新的平均值点上。<br>5. 重复3和4步骤，直到找到正确的聚类的簇。</p><h2 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h2><h3 id="参数说明："><a href="#参数说明：" class="headerlink" title="参数说明："></a>参数说明：</h3><ul><li>$c^{(i)}$=样本$x^{(i)}$所分配到的聚类中心点的索引</li><li>$\mu_k$=第k个聚类中心</li><li>$\mu_{c^{(i)}}$=样本$x^{(i)}$所分配到的聚类中心点</li></ul><p>K均值算法的代价函数为：<br>$$ J(c^{(1)},…,c^{(m)},\mu_1,…\mu_k) = \frac{1}{m}\sum_{i=1}^m ||x^{(i)}-\mu_{c^{(i)}}||^2 $$</p><p>优化目标就是使用上面的代价函数最小化所有参数。</p><p>上述步骤中<br>第3步集群分配，是通过找到离样本最近的聚类中心点来最小化代价函数；<br>第4步移动质心，是通过改变样本和聚类中心点的距离来最小代价函数。<br><strong>在K均值算法中，代价函数是一直下降的，不可能出现上升的情况。</strong></p><h2 id="初始化聚类中心"><a href="#初始化聚类中心" class="headerlink" title="初始化聚类中心"></a>初始化聚类中心</h2><p>聚类中心的个数 $K$一般都是小于样本数量$m$的，因此可以随机取$K$个样本来作为聚类中心。</p><p>步骤</p><ul><li>确保聚类中心个数小于样本数量；</li><li>随机取K个样本；</li><li>将K个样本分配到聚类中心$\mu_1,…\mu_k$.</li></ul><p>这样做的优点是方便快捷，缺点是不一定能够找到最佳的聚类中心，容易陷入局部最优。<br>这种陷入局部最优的情况在聚类中心过少时一般会出现，一般在$K&lt;10$的情况下，解决办法是多次执行该步骤，比较代价函数的值，取最小值。</p><h2 id="聚类中心数量的选择"><a href="#聚类中心数量的选择" class="headerlink" title="聚类中心数量的选择"></a>聚类中心数量的选择</h2><p>聚类中心数量的选择没有固定的方法，跟主观上的判断有很大关系，也跟业务，以及一些客观条件，以及使用K均值算法的目标有关。</p>]]></content>
    
    <summary type="html">
    
      了解非监督学习的概念，以及聚类算法中常用的K均值算法的使用。
    
    </summary>
    
    
      <category term="技术" scheme="https://codeeper.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="机器学习" scheme="https://codeeper.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="非监督学习" scheme="https://codeeper.com/tags/%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>核函数的概念以及在SVM上的应用</title>
    <link href="https://codeeper.com/2020/02/01/tech/machine_learning/svm_kernels.html"/>
    <id>https://codeeper.com/2020/02/01/tech/machine_learning/svm_kernels.html</id>
    <published>2020-02-01T00:00:00.000Z</published>
    <updated>2020-02-14T05:47:29.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="高斯核函数"><a href="#高斯核函数" class="headerlink" title="高斯核函数"></a>高斯核函数</h2><p>激励函数中$\theta^Tx=\theta_0 + \theta_1x_1 + \theta_2x_2 + … + \theta_nx_n$</p><p>现在准备用新的特征值$f_1,f_2,…$ 来替换 $x_1,x_2,…$</p><p>将$f$定义为两个向量的相似度：</p><p>例如，有一个标记向量$l^{(i)}$，某个样本的特征向量$x$和其的相似度为：<br>$$ f_i = similarity(x, l^{(i)}) = exp(-\frac{|| x-l^{(i)} ||^2}{2\sigma^2})$$</p><p><em>PS：$|| x ||$ 是 x向量的范数。</em></p><p>该核函数又称之为<strong>高斯核函数（Gaussian Kernels）</strong>。</p><p>当$x$和$l^{(i)}$很相似时，$f_i \approx 1$；当两者差距很大时，$f_i \approx 0$。</p><p>高斯核函数中有个关键参数$\sigma$，它的大小决定了，该函数值的变化速度。当$\sigma$很小时，$f_i$的变化就会很快，两个向量一点细微的差距就会本放大。当$\sigma$变大时，则相反：</p><div style="display:flex;"><img  src="/images/ml_28.jpg" alt=""><img  src="/images/ml_29.jpg" alt=""><img  src="/images/ml_30.jpg" alt=""></div><p>举个例子，假设设计三个新的特征变量$f_1,f_2,f_3$：<br>当$\theta_0 + \theta_1f_1 + \theta_2f_2 + \theta_3f_3 \geq 0$时，通过预测函数$h(\theta)$预测为1。</p><p>我们假设$\theta_0=-0.5, \theta_1=1, \theta_2=1, \theta_3=0$</p><p>当样本x接近$l_1$时，$f_1 \approx 1$，$f_2 \approx 0$，$f_3 \approx 0$，可以得出上述表达式为$-0.5 + 1 + 0 + 0 = 0.5$，可以预测该样本的输出为1，以此类推，可以得出在标记向量$l_1,l_2$附近的向量即预测为1，远离的预测为0：</p><img width="40%" src="/images/ml_31.jpg" alt=""><h2 id="应用到支持向量机上"><a href="#应用到支持向量机上" class="headerlink" title="应用到支持向量机上"></a>应用到支持向量机上</h2><h3 id="选取标记向量"><a href="#选取标记向量" class="headerlink" title="选取标记向量"></a>选取标记向量</h3><p>在实际应用中，将每个样本作为标记向量。<br>假设有样本$x^{(1)},x^{(2)},…,x^{(m)}$，同样的，将每个样本都定义为标记点，$l^{(1)},l^{(2)},…,l^{(m)}$，即：$x^{(1)}=l^{(1)},x^{(2)}=l^{(2)},…,x^{(m)}=l^{(m)}$。</p><p>对于某一个样本$(x^{(i)},y^{(i)})$来说：<br>$f_1^{(i)}$ = similarity$(x^{(i)}, l^{(1)})$<br>$f_2^{(i)}$ = similarity$(x^{(i)}, l^{(2)})$<br>…<br>$f_i^{(i)}$ = similarity$(x^{(i)}, l^{(i)})=1$<br>…<br>$f_m^{(i)}$ = similarity$(x^{(i)}, l^{(m)})$</p><p><em>PS：在这其中，第$i$个样本向量和第$i$个标记向量是同一个，所以值为1。</em></p><p>对于某一个样本的新的特征向量$f^{(i)}=\left[ \begin{matrix} f^{(i)}_0 \\ f^{(i)}_1 \\ f^{(i)}_2 \\ … \\ f^{(i)}_m \end{matrix}\right]$，同样的$f^{(i)}_0$始终为1，$f$是一个m+1的向量。</p><p>当使用新的特征值后，当$\theta^Tf \geq 0$时，即可预测输出值为1。</p><p>支持向量机的代价函数为：<br>$$ J(\theta)=-C \sum_{i=1}^m [y^{(i)} cost_1(\theta^Tx) + (1-y^{(i)}) cost_0(\theta^Tx)] + \frac{1}{2} \sum_{j=1}^n \theta_j^2 $$</p><p>将新的特征值替换到支持向量机的代价函数中去：</p><p>$$ J(\theta)=-C \sum_{i=1}^m [y^{(i)} cost_1(\theta^Tf^{(i)}) + (1-y^{(i)}) cost_0(\theta^Tf^{(i)})] + \frac{1}{2} \sum_{j=1}^m \theta_j^2 $$</p><p>可以看到将$x^{(i)}$替换成了$f^{(i)}$，$\sum_{j=1}^m \theta_j^2$中是统计从1到m的参数了，因为，新的特征向量是m+1维向量，而且$\theta_0$不用修正。</p><p>核函数也可以用到逻辑回归上，但是这样计算成本会很高，速度会慢许多。而在SVM上，有许多针对核函数的优化方法，使得核函数在SVM上运行良好。</p><h2 id="SVM使用事项"><a href="#SVM使用事项" class="headerlink" title="SVM使用事项"></a>SVM使用事项</h2><p>在使用SVM时一般都是使用第三方包提供的SVM优化算法，我们仅需提供：</p><ul><li>参数C</li><li>选择核函数</li></ul><h3 id="参数C"><a href="#参数C" class="headerlink" title="参数C"></a>参数C</h3><p>参数C可以看作$\frac{1}{\lambda}$，与$\lambda$相关的：</p><ul><li>C过大时，会导致高方差，过拟合的问题；</li><li>C过小时，会导致高偏差，欠拟合的问题。</li></ul><p>因此需要选取折中的C值，可以通过选取多个C值，然后在交叉验证集上简直多个C值的误差，选择最小的。</p><h3 id="核函数选择"><a href="#核函数选择" class="headerlink" title="核函数选择"></a>核函数选择</h3><p>在核函数的选择上，一般有两种，一个是不用核函数，一个是高斯核函数。</p><p>不用核函数也称为线性核函数。它和逻辑回归的算法效果类似。当训练集有大量的特征值，但是样本数量不是太多时，这时拟合一个线性的边界条件会有比较好的效果，也即一般会不使用核函数或使用线性核函数。 </p><p>高斯核函数 则需要去选择$\sigma^2$</p><p><strong>参数$\sigma^2$</strong></p><ul><li>当$\sigma^2$过小时，$f$核函数的变化速度会很剧烈，会导致高方差的现象；</li><li>当$\sigma^2$过大时，$f$核函数的变化速度会很平缓，会导致高偏差的现象。</li></ul><p>当训练集的特征值不是特别多，而且有大量的样本数量时可以选择高斯核函数。</p><p>当使用高斯核函数时，一个很重要的操作就是特征值的缩放，因为计算相似度时，会计算$|| x-l^{(i)} ||^2$，当每个特征值的取值范围相差很大时，求得的范数会受范围大的特征值的影响。为了避免这种情况，需要缩放到相近的范围内。</p><h2 id="多分类问题"><a href="#多分类问题" class="headerlink" title="多分类问题"></a>多分类问题</h2><p>在处理多元分类问题时，与逻辑分类和神经网络类似，也是训练多个SVM训练器，然后比较取性能最好的一个。<br>更方便的方式是使用第三方的库函数来实现多元分类问题。 </p><h2 id="逻辑回归和SVM"><a href="#逻辑回归和SVM" class="headerlink" title="逻辑回归和SVM"></a>逻辑回归和SVM</h2><p>定义：n = 样本的特征值数量，m = 样本的数量。</p><ul><li><p>当n相对于m来时很大时，比如n=10000，m=1000时，这时选择逻辑回归或者不使用核函数的SVM；</p></li><li><p>当n比较小，m中等大小，比如n=1000，m=10000时，选择SVM（使用高斯核函数）的效果会比较好； </p></li><li><p>当n比较小，m非常大时，比如n=1000，m=50000+时，这时通常手动添加更多的特征值，然后使用逻辑回归或者不使用核函数的SVM来处理。</p></li></ul><p>而神经网络算法，通常都会比这两者来的慢，且需要良好的设计才能取得较好的效果。而且使用SVM时不用担心遇到局部最优的问题。</p>]]></content>
    
    <summary type="html">
    
      了解核函数的概念，高斯核函数的公式，以及核函数在支持向量机算法上的应用。
    
    </summary>
    
    
      <category term="技术" scheme="https://codeeper.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="机器学习" scheme="https://codeeper.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="SVM" scheme="https://codeeper.com/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>支持向量机算法（SVM）介绍</title>
    <link href="https://codeeper.com/2020/01/31/tech/machine_learning/svm_intro.html"/>
    <id>https://codeeper.com/2020/01/31/tech/machine_learning/svm_intro.html</id>
    <published>2020-01-31T03:00:00.000Z</published>
    <updated>2020-02-14T05:47:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>在解决复杂的非线性分类问题时，除了逻辑回归和神经网络，还有一种更为强大的算法：叫做<strong>支持向量机（Support Vector Machines）</strong>，简称<strong>SVM</strong>。</p><h2 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h2><p>在分类问题上，常用的激励函数为：<br>$$ h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}} $$</p><p>逻辑回归的cost函数：<br>$$ Cost(h_\theta(x), y) = -y \log(h_\theta(x)) - (1-y) \log(1-h_\theta(x)) $$</p><p>代入激励函数，得到：<br>$$ Cost(h_\theta(x), y) = -y \log(\frac{1}{1+e^{-\theta^Tx}}) - (1-y) \log(1-\frac{1}{1+e^{-\theta^Tx}}) $$</p><p>上述式子中：<br>当样本的输出$y$为1时：$Cost(h_\theta(x), y) = -\log(\frac{1}{1+e^{-z}}), z=\theta^Tx$</p><p>当样本的输出$y$为0时：$Cost(h_\theta(x), y) = -\log(1-\frac{1}{1+e^{-z}}), z=\theta^Tx$</p><p>该式子的值随着z的变化曲线：</p><div style="display:flex;"><img  src="/images/ml_25.jpg" alt=""><img  src="/images/ml_26.jpg" alt=""></div><p>可以看到当$y=1$时，该式子的值随着z值的增大而减小；当$y=0$时，该式子的值随着z值的增大而增大。</p><p>下面开始构建向量机<br>将上面的变化曲线做一下简化：</p><p><img src="/images/ml_27.jpg" alt=""></p><ul><li>当$y=1$时，以$z=1$为分界点，当$z\geq 1$，式子的值为0，当小于1时，是一条线性变化的直线；<br>用 <strong>$cost_1(z)$</strong>来表示该曲线。</li><li>当$y=0$时，以$z=-1$为分界点，当$z\leq -1$，式子的值为0，当大于-1时，是一条线性变化的直线；<br>用 <strong>$cost_0(z)$</strong>来表示该曲线。</li></ul><p>逻辑回归的代价函数为：<br>$$ J(\theta)=-\frac{1}{m} \sum_{i=1}^m [y^{(i)} \log(h_\theta(x^{(i)})) + (1-y^{(i)}) \log(1-h_\theta(x^{(i)}))] + \frac{\lambda}{2m} \sum_{j=1}^n \theta_j^2  $$</p><p>将$cost_1(z)$和$cost_0(z)$代入上述式子：</p><p>$$ J(\theta)=-\frac{1}{m} \sum_{i=1}^m [y^{(i)} cost_1(\theta^Tx) + (1-y^{(i)}) cost_0(\theta^Tx)] + \frac{\lambda}{2m} \sum_{j=1}^n \theta_j^2 $$</p><p>为了方便计算，将式子乘以$m$；再将$\lambda$化去，用$C$代替：</p><p>$$ J(\theta)=-C \sum_{i=1}^m [y^{(i)} cost_1(\theta^Tx) + (1-y^{(i)}) cost_0(\theta^Tx)] + \frac{1}{2} \sum_{j=1}^n \theta_j^2 $$</p><p>上述便是SVM的代价函数。<br>$C$值是一种控制参数权重的方法，和逻辑分类的$\lambda$的作用类似。区别是更关心前一个式子的优化还是后一个式子的优化。为了方便，可以简单的把C和$\lambda$的关系比做$C=\frac{1}{\lambda}$。</p><p>通过代价函数 和 $cost_1(z)$和$cost_0(z)$ 的变化曲线，可以得知：</p><ul><li>当$y=1$：$\theta^Tx \geq  1$条件下，代价函数的值趋向于0；</li><li>当$y=0$：$\theta^Tx \leq -1$条件下，代价函数的值趋向于0；</li></ul><p>上述条件比起在逻辑回归中，$\theta^Tx \geq 0$时预测值为1，$\theta^Tx &lt; 0$预测值为0，支持向量机的要求更加严格。</p><h2 id="决策边界"><a href="#决策边界" class="headerlink" title="决策边界"></a>决策边界</h2><p>支持向量机在选择决策边界时会选择一个离样本间距最大的边界，因此支持向量机有时被称为<strong>大间距分类器</strong>。</p>]]></content>
    
    <summary type="html">
    
      了解分类算法中一种强大的算法，支持向量机。以及它的代价函数说明。
    
    </summary>
    
    
      <category term="技术" scheme="https://codeeper.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="机器学习" scheme="https://codeeper.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="SVM" scheme="https://codeeper.com/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>查准率（Precision）和召回率（Recall）</title>
    <link href="https://codeeper.com/2020/01/27/tech/machine_learning/precision_and_recall.html"/>
    <id>https://codeeper.com/2020/01/27/tech/machine_learning/precision_and_recall.html</id>
    <published>2020-01-27T06:00:00.000Z</published>
    <updated>2020-02-14T05:46:19.000Z</updated>
    
    <content type="html"><![CDATA[<p>在处理分类问题时，会遇到一种情况：</p><p>假设一个二元分类问题：假设我们的预测算法是：$h_\theta(x)=0$，这个算法忽略特征值$x$，不管任何情况下都是预测$h_\theta(x)$等于0。</p><p>毫无疑问这是一个糟糕的算法，但是在测试集中，99%的样本输出$y=1$，1%的样本输出$y=0$，这样计算预测算法的误差率的时候，会的到1%的误差率，这就是很糟糕的情况，一个完全错误的算法得到了一个正确率很高的测试结果。</p><p>这种情况称之为<strong>偏斜类（Skewed Classes）</strong>的问题。</p><h2 id="解决问题的办法"><a href="#解决问题的办法" class="headerlink" title="解决问题的办法"></a>解决问题的办法</h2><p>处理这种情况，需要参考查准率和召回率</p><table border="1">    <tr>        <th colspan="2" rowspan="2"></th>        <th colspan="2" align="center">真实结果</th>    </tr>    <tr>        <td>1</td>        <td>0</td>    </tr>    <tr><th rowspan="4">预测结果</th></tr>    <tr>        <td>1</td>        <td>TP(真阳性)</td>        <td>FP(假阳性)</td>    </tr>    <tr>        <td>0</td>        <td>FN(假阴性)</td>        <td>TN(真阴性)</td>    </tr></table><p>上图的表格中，提到了四个概念。<br><strong>TP(真阳性)</strong> 预测为真的样本中确实为真的数量。<br><strong>FP(假阳性)</strong> 预测为真的样本中确实为假的数量。<br><strong>FN(假阴性)</strong> 预测为假的样本中确实为真的数量。<br><strong>TN(真阴性)</strong> 预测为假的样本中确实为假的数量。</p><p>举个例子来说明</p><p>预测某些病人有没有得癌症。<br>假设有100个样本，真实情况是有10个得癌症的，通过预测函数遇到到了有12个得了癌症，其中有8个是真实得癌症的。<br>这种情况下:<br>TP=8<br>FP=12-8=4<br>FN=10-8=2<br>TN=(100-12)-2=86</p><h2 id="准确率-Accuracy"><a href="#准确率-Accuracy" class="headerlink" title="准确率 Accuracy"></a>准确率 Accuracy</h2><p>正确预测为1，正确预测为0的样本比率，公式为：$\frac{TP+TN}{ALL}$<br>上例中准确率为 $\frac{8+86}{100}=0.94$</p><h2 id="查准率-Precision"><a href="#查准率-Precision" class="headerlink" title="查准率 Precision"></a>查准率 Precision</h2><p>查准率是指在所有预测为1的样本中预测正确的比率，公式为：$\frac{TP}{TP+FP}$<br>上例中查准率为 $\frac{8}{8+4}=0.667$</p><h2 id="召回率-Recall"><a href="#召回率-Recall" class="headerlink" title="召回率 Recall"></a>召回率 Recall</h2><p>召回率是指在所有真正为1的样本中预测正确的比率，公式为：$\frac{TP}{TP+FN}$<br>上例中召回率为 $\frac{8}{8+2}=0.8$</p><p>在最开始偏斜类问题中 TP=0，召回率为0，因此那个预测算法是错误的。</p><h2 id="查准率和召回率的关系"><a href="#查准率和召回率的关系" class="headerlink" title="查准率和召回率的关系"></a>查准率和召回率的关系</h2><p>在分类问题中，$h_\theta(x) \geq 0.5$ 是我们就预测为1，$h_\theta(x) &lt; 0.5$ 是我们就预测为0；</p><p>边界条件就是0.5<br>当提高边界值时，即$h_\theta(x) \geq 0.7$，查准率会提高，召回率会下降；<br>当减小边界值时，即$h_\theta(x) \geq 0.3$，召回率会提高，查准率会下降。</p><img width="50%" src="/images/ml_23.jpg" alt=""><p>查准率和召回率之间的变化关系和上图类似，变化的曲线可能不是上图的平滑关系。大方向两者是相反的增长。</p><h2 id="判断一个学习算法的性能"><a href="#判断一个学习算法的性能" class="headerlink" title="判断一个学习算法的性能"></a>判断一个学习算法的性能</h2><p><img src="/images/ml_24.jpg" alt=""></p><p>要判断一个学习学习算法需要综合考虑查准率和召回率，可以使用 <strong>F值（F-Score）</strong> 来综合评价。<br>公式为：2$\frac{P*R}{P+R}$</p><p>通过上面，可以得出算法1的性能比较好。</p>]]></content>
    
    <summary type="html">
    
      如何解决偏斜类问题，查准率和召回率的定义以及使用，以及使用F值来判断一个学习算法的性能。
    
    </summary>
    
    
      <category term="技术" scheme="https://codeeper.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="机器学习" scheme="https://codeeper.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>评估学习算法</title>
    <link href="https://codeeper.com/2020/01/26/tech/machine_learning/evaluating_learning_algorithm.html"/>
    <id>https://codeeper.com/2020/01/26/tech/machine_learning/evaluating_learning_algorithm.html</id>
    <published>2020-01-26T14:00:00.000Z</published>
    <updated>2020-02-14T12:50:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>在使用学习算法解决机器学习问题时，可能预测函数的误差很小，但是这个学习算法不确定是不是准确的，因为可能出现过拟合的情况。</p><p>因为仅仅用一个训练集来判断学习算法是否准确是不行的，为此我们需要将原始数据集拆分成三个：</p><ul><li>训练集 (Training set) 占60%</li><li>交叉验证集 (Cross validation set) 占20%</li><li>测试集 (Test set) 占20%</li></ul><h2 id="学习模型的选择"><a href="#学习模型的选择" class="headerlink" title="学习模型的选择"></a>学习模型的选择</h2><p>例如在线性回归问题中，训练集有一个特征值$x$，在选择模型时，选择几次项：<br>1 .  $h_\theta(x)=\theta_0+\theta_1x$<br>2 .  $h_\theta(x)=\theta_0+\theta_1x+\theta_2x^2$<br>3 .  $h_\theta(x)=\theta_0+\theta_1x+…+\theta_3x^3$<br>…<br>10 . $h_\theta(x)=\theta_0+\theta_1x+…+\theta_{10}x^{10}$</p><p>首先分成三个样本集：<br>训练集：$(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), …, (x^{(m)},y^{(m)})$<br>交叉验证集：$(x_{cv}^{(1)},y_{cv}^{(1)}), (x_{cv}^{(2)},y_{cv}^{(2)}), …, (x_{cv}^{(m_{cv})},y_{cv}^{(m_{cv})})$<br>测试集：$(x_{test}^{(1)},y_{test}^{(1)}), (x_{test}^{(2)},y_{test}^{(2)}), …, (x_{test}^{(m_{test})},y_{test}^{(m_{test})})$</p><p>分开的三个样本集上的误差：<br>$J_{train}(\Theta)=\frac{1}{2m} \sum_{i=0}^m(h_\theta(x^{(i)})-y^{(i)})^2$<br>$J_{cv}(\Theta)=\frac{1}{2m_{cv}} \sum_{i=0}^{m_{cv}}(h_\theta(x_{cv}^{(i)})-y_{cv}^{(i)})^2$<br>$J_{test}(\Theta)=\frac{1}{2m_{test}} \sum_{i=0}^{m_{test}}(h_\theta(x_{test}^{(i)})-y_{test}^{(i)})^2$</p><p>选择模型的步骤：</p><ul><li>将上面十个模型在训练集上优化每个模型的参数$\theta$；</li><li>在交叉验证集上找出一个误差最小的模型；</li><li>在测试集上用$J_{test}(\Theta)$来估计泛化误差。</li></ul><p>这样的操作是的测试集与训练集，交叉验证集完全分离开，互不影响。在测试集上的泛化误差比较准确。</p><h2 id="偏差（Bias）和方差（Variance）"><a href="#偏差（Bias）和方差（Variance）" class="headerlink" title="偏差（Bias）和方差（Variance）"></a>偏差（Bias）和方差（Variance）</h2><p>当一个算法在使用时并不理想，会有两种情况，高偏差（欠拟合）或高方差（过拟合）。</p><p>还是上面的线性回归例子。通过选择不同模型，比较它们在训练集和交叉验证集上的表现结果：</p><img width="50%" src="/images/ml_19.jpg" alt=""><p>上图中，x轴是多项式的次数，可以看作是函数的复杂程度，y轴是代价函数的值。</p><p><strong>在训练集上：</strong>当函数简单时，代价函数值很大，预测函数效果不好，随着多项式的增加，函数复杂度的增加，拟合效果越来越小，代价趋向于零。<br><strong>在交叉验证集上：</strong>当函数简单时，代价函数值很大，预测函数效果不好，多项式在增加到一定程度前，代价一直在下降，当下降到一定程度是，代价函数又会上升，拟合效果又变差了。</p><p><strong>高偏差（Bias）：</strong>$J_{train}(\Theta)$和$J_{cv}(\Theta)$都很高，而且两者非常接近，$J_{train}(\Theta)\approx J_{cv}(\Theta)$<br><strong>高方差（Variance）：</strong>$J_{train}(\Theta)$很小，$J_{cv}(\Theta)$远远大于$J_{train}(\Theta)$</p><p>在实际使用时，需要在中间找一个平衡点，避免高偏差和高方差</p><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>通过正则化能够解决过拟合的问题，但是它在欠拟合也就是高偏差情况下效果并不明显。</p><p>上面的线性回归例子，选择一个模型：<br>$$h_\theta(x)=\theta_0+\theta_1x+\theta_2x^2+\theta_3x^3+\theta_4x^4$$<br>它的正则化后的代价函数：<br>$$ J(\theta)= \frac{1}{2m} \sum_{i=0}^m (h_\theta(x^{(i)})-y^{(i)})^2 + \frac{\lambda}{2m} \sum_{j=1}^n\theta_j^2 $$</p><p><em>PS: 上面的$J_{train}(\Theta),J_{cv}(\Theta),J_{test}(\Theta)$定义里面没有加上正则化，所以上面的训练集，交叉验证集，测试集都是在没有正则化的情况的误差</em></p><p>现在我们要做的是先选择一系列的$\lambda$值，比如:<br>0, 0.01, 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1.28, 2.56, 5.12, 10.24<br>通过正则化得出优化的参数$\theta$。</p><p>然后使用这些优化参数得出$J_{train}(\Theta),J_{cv}(\Theta)$<br>$J_{train}(\Theta)=\frac{1}{2m} \sum_{i=0}^m(h_\theta(x^{(i)})-y^{(i)})^2$<br>$J_{cv}(\Theta)=\frac{1}{2m_{cv}} \sum_{i=0}^{m_{cv}}(h_\theta(x_{cv}^{(i)})-y_{cv}^{(i)})^2$<br><strong>注意，这里的误差都没有加上正则化项目</strong></p><p>比较每个$\lambda$值下的代价函数的曲线：</p><img width="50%" src="/images/ml20.png" alt=""><p><strong>在训练集上：</strong> 随着$\lambda$的增大，代价函数的值越来越大，效果越来越差。<br><strong>在交叉验证集上：</strong> 先随着$\lambda$在一定范围下增大，代价函数的值在减小，超过一个程度后，代价函数会增大。</p><p>当$\lambda$为0，或很小时，是高方差；当$\lambda$过大时，是高偏差。</p><p>同样的需要找一个$\lambda$使得代价函数在训练集和交叉验证集上平衡的。</p><h2 id="学习曲线（Learning-Curves）"><a href="#学习曲线（Learning-Curves）" class="headerlink" title="学习曲线（Learning Curves）"></a>学习曲线（Learning Curves）</h2><p>画出随着样本集数量的增加，代价函数的值的变化曲线。<br>训练集和交叉验证集的误差：<br>$J_{train}(\Theta)=\frac{1}{2m} \sum_{i=0}^m(h_\theta(x^{(i)})-y^{(i)})^2$<br>$J_{cv}(\Theta)=\frac{1}{2m_{cv}} \sum_{i=0}^{m_{cv}}(h_\theta(x_{cv}^{(i)})-y_{cv}^{(i)})^2$</p><p>当样本集数量很少时，训练集的误差很小，因为两三个样本总能够用一个直线达到不错的拟合效果，但是在交叉验证集上体现的并不好。<br>随着样本数量的增大，训练集的误差增大，而在交叉验证集上的体现会改善，误差会减小。</p><p><strong>高偏差</strong><br>在高偏差情况下，随着样本数量的增大，训练集和验证集的误差趋向于相等，但是都会偏高，与预期效果不符。</p><img width="50%" src="/images/ml_21.jpg" alt="高偏差学习曲线"><p><strong>高方差</strong><br>在高方差下，随着样本增加，训练集的误差会增加，但是增加的幅度很小；在验证集上，误差会减小，但是仍然大于训练集误差。</p><img width="50%" src="/images/ml_22.jpg" alt="高方差学习曲线"><p>可以看出，在高方差下，增加样本数量会有助于算法的优化。但是在高偏差下，增加样本数量并没有多少用处。</p><p>通过以上方法，在解决机器学习问题中，有以下方法可以帮助优化算法：</p><ul><li>获取更多样本：优化高方差</li><li>减少特征值的数量：优化高方差</li><li>添加特征值：优化高偏差</li><li>添加多项式次数，复杂的预测函数：优化高偏差</li><li>减小$\lambda$：解决高偏差</li><li>增加$\lambda$：解决高方差</li></ul>]]></content>
    
    <summary type="html">
    
      了解如何选择学习算法，以及对学习算法的优化，在使用学习曲线的情况下，判断高偏差和高方差，以及对其的解决办法。
    
    </summary>
    
    
      <category term="技术" scheme="https://codeeper.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="机器学习" scheme="https://codeeper.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="算法优化" scheme="https://codeeper.com/tags/%E7%AE%97%E6%B3%95%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>神经网络的反向传播算法的使用</title>
    <link href="https://codeeper.com/2020/01/20/tech/machine_learning/neural_network_backpropagation.html"/>
    <id>https://codeeper.com/2020/01/20/tech/machine_learning/neural_network_backpropagation.html</id>
    <published>2020-01-20T13:00:00.000Z</published>
    <updated>2020-02-14T05:45:20.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="参数展开"><a href="#参数展开" class="headerlink" title="参数展开"></a>参数展开</h2><p>在神经网络中，我们有一些参数矩阵，例如：<br>$\Theta^{(1)},\Theta^{(2)},\Theta^{(3)}…$<br>$D^{(1)},D^{(2)},D^{(3)}…$</p><p>有时为了使用方便，需要将这些矩阵统一到一个向量中去</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">thetaVector = [ Theta1(:); Theta2(:); Theta3(:); ]</span><br><span class="line">deltaVector = [ D1(:); D2(:); D3(:) ]</span><br></pre></td></tr></table></figure><p>这样就合并了三个矩阵，<br>假如$\Theta^{(1)}$ 是一个$5 \times 6$的矩阵，$\Theta^{(2)}$是一个$5 \times 6$的矩阵，$\Theta^{(3)}$是一个$1 \times 6$的矩阵<br>则 thetaVector 是一个$66 \times 1$的矩阵</p><p>当要使用时，同样要将这一个向量，拆分成三个矩阵：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Theta1 = <span class="built_in">reshape</span>(thetaVector(<span class="number">1</span>:<span class="number">30</span>),<span class="number">5</span>,<span class="number">6</span>)</span><br><span class="line">Theta2 = <span class="built_in">reshape</span>(thetaVector(<span class="number">31</span>:<span class="number">60</span>),<span class="number">5</span>,<span class="number">6</span>)</span><br><span class="line">Theta3 = <span class="built_in">reshape</span>(thetaVector(<span class="number">61</span>:<span class="number">66</span>),<span class="number">1</span>,<span class="number">6</span>)</span><br></pre></td></tr></table></figure><h2 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h2><p>在逻辑回归里，使用梯度下降算法时，需要初始化$\Theta$，这里可以设置成零向量$n\times 1$，元素全为0的向量。<br>但是在神经网络里，这样做不行。</p><p>举例说明，假设$\Theta_{i,j}^{(l)}=0$<br>$ a_1^{(2)} = g(\theta_{10}^{(1)}x_0+\theta_{11}^{(1)}x_1+\theta_{12}^{(1)}x_2+\theta_{13}^{(1)}x_3)=g(0) $<br>$ a_2^{(2)} = g(\theta_{20}^{(1)}x_0+\theta_{21}^{(1)}x_1+\theta_{22}^{(1)}x_2+\theta_{23}^{(1)}x_3)=g(0) $<br>…</p><p>这样$a_1^{(2)}=a_2^{(2)}=a_{s_l}^{(2)}$</p><p>当反向传播时$\delta^{(l)}$会发现每层的每个节点都是一样的，这样算法就会出现对称问题。<br>因此初始化的时候要随机一些初始值。初始化不同的随机址会打破对称问题。</p><p>将$\Theta_{i,j}^{(l)}$内每个值都随机在$[-\epsilon, \epsilon]$ 之间。（$-\epsilon \leq \Theta_{i,j}^{(l)} \leq \epsilon$）</p><p>例如:<br>Theta1 = rand(10,11) * (2<em>init_epsilon) - init_epsilon;<br>Theta2 = rand(1,11) * (2</em>init_epsilon) - init_epsilon;</p><p>rand(x,y)是一个初始化一个$x \times y$ 维的，其内之在0, 1之间的随机值。</p><h2 id="梯度检验（Gradient-Checking）"><a href="#梯度检验（Gradient-Checking）" class="headerlink" title="梯度检验（Gradient Checking）"></a>梯度检验（Gradient Checking）</h2><p>在使用反向传播算法时，因为反向传播有很多复杂的细节，这些细节会导致一些bug，虽然代价函数的值在减小，但是可能的结果和实际的还是有很大误差。<br>梯度检验可以减少这种错误的概率。</p><p>在数学上<br>$\frac{\partial}{\partial \Theta} J(\Theta) \approx \frac{J(\Theta+\epsilon) + J(\Theta-\epsilon)}{2\epsilon}$</p><p><em>PS：$\epsilon$这个和初始化随机的参数不一样</em></p><p>$\epsilon$ 是一个很小的值，例如$10^{-4}$</p><p>对于多个$\Theta$</p><p>$\frac{\partial}{\partial \Theta_j^{(l)}} J(\Theta) \approx \frac{J(\Theta_1,…,\Theta_j+\epsilon,…,\Theta_n) + J(\Theta_1,…,\Theta_j-\epsilon,…,\Theta_n)}{2\epsilon}$</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">epsilon = <span class="number">1e-4</span>;</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:n,</span><br><span class="line">  thetaPlus = theta;</span><br><span class="line">  thetaPlus(<span class="built_in">i</span>) += epsilon;</span><br><span class="line">  thetaMinus = theta;</span><br><span class="line">  thetaMinus(<span class="built_in">i</span>) -= epsilon;</span><br><span class="line">  gradApprox(<span class="built_in">i</span>) = (J(thetaPlus) - J(thetaMinus))/(<span class="number">2</span>*epsilon)</span><br><span class="line"><span class="keyword">end</span>;</span><br></pre></td></tr></table></figure><p>将反向传播计算的DVec和梯度校验计算的gradApprox比较一下，两者一致则说明算法没有问题。但是在最终使用反向传播取训练样本集时，关掉梯度校验，因为这个非常耗时。</p><h2 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h2><p>每个隐藏层的单元数理论上越多准确度越好，但是实际上为了和计算成本平衡，需要选择何时的隐藏层单元数量。一般隐藏层单元数目稍大于输入层的特征值数。<br>隐藏层的数量越多，算法准确度越高，相应的计算成本也会增高，一般来说1个隐藏层。如果有多个隐藏层，建议在每个隐藏层中使用相同数量的单元。</p>]]></content>
    
    <summary type="html">
    
      在使用神经网络时的一些操作，比如参数展开，还有随机初始化参数以及使用梯度校验验证反向传播的正确性。
    
    </summary>
    
    
      <category term="技术" scheme="https://codeeper.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="机器学习" scheme="https://codeeper.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://codeeper.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>神经网络代价函数和反向传播算法</title>
    <link href="https://codeeper.com/2020/01/18/tech/machine_learning/neural_network_cost_function.html"/>
    <id>https://codeeper.com/2020/01/18/tech/machine_learning/neural_network_cost_function.html</id>
    <published>2020-01-18T11:00:00.000Z</published>
    <updated>2020-02-14T12:54:47.000Z</updated>
    
    <content type="html"><![CDATA[<img width="50%" src="/images/ml_18.jpg" alt="多层神经网络"><p>定义一些参数：<br>$L$ = 神经网络的层数<br>$s_l$ = $l$层的单元数，但不包括偏差单元<br>$K$ = 输出层的单元数，二元分类是1，大于二的就是k</p><p>逻辑回归的代价函数为：<br>$$ J(\theta)=-\frac{1}{m} \sum_{i=1}^m [y^{(i)} \log(h_\theta(x^{(i)})) + (1-y^{(i)}) \log(1-h_\theta(x^{(i)}))] + \frac{\lambda}{2m} \sum_{j=1}^n \theta_j^2 $$</p><p>神经网络的代价函数：<br>$h_\theta(x^{(i)})$ 是一个K维向量。$(h_\theta(x))_i$是输出向量的第i个单元。</p><p>$$ J(\theta)=-\frac{1}{m} \sum_{i=1}^{m}\sum_{k=1}^{K} [y_k^{(i)}\log((h_\theta(x^{(i)}))^k) + (1-y_k^{(i)})\log(1-(h_\theta(x^{(i)}))^k) ] + \frac{\lambda}{2m} \sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}(\theta_{ji}^{(l)})^2 $$</p><p><em>PS：$(h_\theta(x^{(i)}))^k$ 这里的$k$应该是下标，这样写的原因是markdown解析不出来，囧～～</em></p><p>左侧两个求和的是将输出的每个单元的回归代价相加，右侧的三次和是将所有的$\theta$单元的平方相加</p><h2 id="最小化-J-theta"><a href="#最小化-J-theta" class="headerlink" title="最小化$J(\theta)$"></a>最小化$J(\theta)$</h2><p>想要计算神经网络的代价函数$J(\theta)$的最小化，需要计算$\frac{\partial}{\partial \theta_{i,j}^{(l)}} J(\theta)$</p><p>在神经网络算法中，最小化代价函数一般是使用<strong>反向传播算法（Backpropagation）</strong>来计算</p><p>定义一个参数：<br>$a_j^{(l)}$ = 第$l$层，第$j$个节点的激励函数返回值。<br>$\delta_j^{(l)}$ = 第$l$层，第$j$个节点的误差。</p><h3 id="计算过程"><a href="#计算过程" class="headerlink" title="计算过程"></a>计算过程</h3><ol><li><h4 id="初始化-Delta-i-j-l-0-，每一层的每一个元素都是0；"><a href="#初始化-Delta-i-j-l-0-，每一层的每一个元素都是0；" class="headerlink" title="初始化$\Delta_{i,j}^{(l)}=0$，每一层的每一个元素都是0；"></a>初始化$\Delta_{i,j}^{(l)}=0$，每一层的每一个元素都是0；</h4></li><li><h4 id="设置-a-1-x-i-；"><a href="#设置-a-1-x-i-；" class="headerlink" title="设置 $a^{(1)}=x^{(i)}$；"></a>设置 $a^{(1)}=x^{(i)}$；</h4></li><li><h4 id="正向计算每层的单元-a-l-，-l-2-3-…-L"><a href="#正向计算每层的单元-a-l-，-l-2-3-…-L" class="headerlink" title="正向计算每层的单元$a^{(l)}$，$l$=2,3,…,L"></a>正向计算每层的单元$a^{(l)}$，$l$=2,3,…,L</h4></li></ol><p>最上面的神经网络模型为例：<br>$a^{(1)}=x$<br>$z^{(2)}=\Theta^{(1)}a^{(1)}$<br>$a^{(2)}=g(z^{(2)})$，添加$a_0^{(2)}=1$<br>$z^{(3)}=\Theta^{(2)}a^{(2)}$<br>$a^{(3)}=g(z^{(3)})$，添加$a_0^{(3)}=1$<br>$z^{(4)}=\Theta^{(3)}a^{(3)}$<br>$a^{(4)}=h_\theta(x)=g(z^{(4)})$</p><ol start="4"><li><h4 id="计算L层（输出层）误差-delta-L-a-L-y-i"><a href="#计算L层（输出层）误差-delta-L-a-L-y-i" class="headerlink" title="计算L层（输出层）误差$\delta^{(L)}=a^{(L)}-y^{(i)}$"></a>计算L层（输出层）误差$\delta^{(L)}=a^{(L)}-y^{(i)}$</h4></li></ol><p>$y^{(i)}$是训练集的真实输出值。上例中，$\delta^{(4)}=a^{(4)}-y^{(i)}$</p><ol start="5"><li><h4 id="计算L-1-L-2-…-2层误差，-delta-L-1-delta-L-2-…-delta-2-；"><a href="#计算L-1-L-2-…-2层误差，-delta-L-1-delta-L-2-…-delta-2-；" class="headerlink" title="计算L-1,L-2,…,2层误差，$\delta^{(L-1)},\delta^{(L-2)},…,\delta^{(2)}$；"></a>计算L-1,L-2,…,2层误差，$\delta^{(L-1)},\delta^{(L-2)},…,\delta^{(2)}$；</h4></li></ol><p>L层之前的层的误差计算公式<br>$\delta^{(l)}=(\Theta^{(l)})^T\delta^{(l+1)} \cdot*g\prime(z^{(l)}),l \in [2,3,…L-1]$</p><p>其中$g\prime(z^{(l)})=a^{(l)} \cdot* (1-a^{(l)})$<br>因此误差计算公式等于：<br>$$ \delta^{(l)}=(\Theta^{(l)})^T\delta^{(l+1)} \cdot* a^{(l)} \cdot* (1-a^{(l)}),l \in [2,3,…L-1] $$<br>这里没有第一层误差，因为第一层就是输入值，不存在误差。</p><ol start="6"><li><h4 id="计算-Delta-，-Delta-i-j-l-Delta-i-j-l-a-j-i-delta-i-l-1-，用向量方式表示-Delta-l-Delta-l-delta-l-1-a-l-T-，其中-l-in-1-2-3-…-L-1"><a href="#计算-Delta-，-Delta-i-j-l-Delta-i-j-l-a-j-i-delta-i-l-1-，用向量方式表示-Delta-l-Delta-l-delta-l-1-a-l-T-，其中-l-in-1-2-3-…-L-1" class="headerlink" title="计算$\Delta$，$\Delta_{i,j}^{(l)} := \Delta_{i,j}^{(l)} + a_j^{(i)}\delta_i^{(l+1)}$，用向量方式表示$\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T $，其中$l \in [1,2,3,…,L-1]$"></a>计算$\Delta$，$\Delta_{i,j}^{(l)} := \Delta_{i,j}^{(l)} + a_j^{(i)}\delta_i^{(l+1)}$，用向量方式表示$\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T $，其中$l \in [1,2,3,…,L-1]$</h4></li><li><h4 id="在所有的样本集中，重复2-6步骤，计算出-Delta-l"><a href="#在所有的样本集中，重复2-6步骤，计算出-Delta-l" class="headerlink" title="在所有的样本集中，重复2-6步骤，计算出$\Delta^{(l)}$;"></a>在所有的样本集中，重复2-6步骤，计算出$\Delta^{(l)}$;</h4></li><li><h4 id="这里要加上正则化的过程："><a href="#这里要加上正则化的过程：" class="headerlink" title="这里要加上正则化的过程："></a>这里要加上正则化的过程：</h4></li></ol><p>$D_{i,j}^{(l)} = \frac{1}{m}\Delta_{i,j}^{(l)}, j=0$<br>$D_{i,j}^{(l)} = \frac{1}{m}(\Delta_{i,j}^{(l)} + \lambda\Theta_{i,j}^{(l)}), j\neq 0$</p><p>最终获得的代价函数的导数$\frac{\partial}{\partial \theta_{i,j}^{(l)}} J(\Theta)=D_{i,j}^{(l)}$</p>]]></content>
    
    <summary type="html">
    
      了解神经网络的代价函数，还有对代价函数求导的方法，反向传播算法的使用过程。
    
    </summary>
    
    
      <category term="技术" scheme="https://codeeper.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="机器学习" scheme="https://codeeper.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://codeeper.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>神经网络算法介绍</title>
    <link href="https://codeeper.com/2020/01/16/tech/machine_learning/neural_network_intro.html"/>
    <id>https://codeeper.com/2020/01/16/tech/machine_learning/neural_network_intro.html</id>
    <published>2020-01-16T02:00:00.000Z</published>
    <updated>2020-02-14T12:55:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>在解决分类问题时，可以用逻辑回归算法，但当解决复杂的非线性分类器时，这并不是一个好的选择。<br>如果用逻辑回归来解决，首先要构造一个包含很多非线性项的逻辑回归函数。使用逻辑回归会构造一个s型函数$g$。当多项式足够多，足够复杂，会有一个非常扭曲的决策边界。这可能会出现过拟合的情况。<br>另一个问题，复杂的非线性分类器会包含有很多的特征项，要包含所有的特征项时很困难的事情，而且计算成本过大。</p><p>比如要识别一个图像是不是汽车，就要检测图像的每一个像素，这是一个非常大的计算量。因此神经网络是一个很好的选择。</p><h2 id="神经网络模型"><a href="#神经网络模型" class="headerlink" title="神经网络模型"></a>神经网络模型</h2><img width="50%" src="/images/ml_16.jpg" alt="逻辑单元"><p>这是一个最简单的神经网络的模型。左侧的是三个特征值的输入，右侧是一个输出，这是一个二元问题的神经网络模型。</p><p>一般在处理神经网络时，和逻辑回归一样，需要添加一个$x_0$的默认特征项。在神经网络里，这称之为偏置单位<br>$$ x=\left[ \begin{matrix}<br>x_0 \\ x_1 \\ x_2 \\ x_3<br>\end{matrix} \right], \theta=\left[ \begin{matrix}<br>\theta_0 \\ \theta_1 \\ \theta_2 \\ \theta_3<br>\end{matrix} \right]$$<br>还有关于$h(\theta)$的函数<br>$$ h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}} $$<br>在神经网络里这个成为激励函数，这是神经网络的术语，它是和逻辑回归里相同的函数。在这种情况下，激励函数的参数$\theta$称之为权重。</p><p>看一个复杂一点的模型：</p><img width="50%" src="/images/ml_17.jpg" alt="多层神经网络"><p>上图中，第一层是<strong>输入层</strong>，然后进入第二层，最后输出预测函数，也就是第三层，<strong>输出层</strong>。<br>输入层和输出层之间的节点层，称之为<strong>隐藏层</strong>。上图中有一个隐藏层。<br>隐藏层中的$a_i^{(j)}$是第$j$层的第$i$个单元。和输入层一样，在计算时会添加一个偏置单元$a_0^{(j)}=1$<br>$$ a^{(j)}=\left[ \begin{matrix}<br>a_0^{(j)} \\ a_1^{(j)} \\ a_2^{(j)} \\  a_3^{(j)}<br>\end{matrix} \right]$$</p><p>$\theta^{(j)}$是从第$j$层到第$j+1$层的映射参数矩阵。</p><h2 id="计算过程"><a href="#计算过程" class="headerlink" title="计算过程"></a>计算过程</h2><p>隐藏层的激活节点和输出层的输出函数计算过程如下：<br>$ a_0^{(2)} = 1 $<br>$ a_1^{(2)} = g(\theta_{10}^{(1)}x_0+\theta_{11}^{(1)}x_1+\theta_{12}^{(1)}x_2+\theta_{13}^{(1)}x_3) $<br>$ a_2^{(2)} = g(\theta_{20}^{(1)}x_0+\theta_{21}^{(1)}x_1+\theta_{22}^{(1)}x_2+\theta_{23}^{(1)}x_3) $<br>$ a_3^{(2)} = g(\theta_{30}^{(1)}x_0+\theta_{31}^{(1)}x_1+\theta_{32}^{(1)}x_2+\theta_{33}^{(1)}x_3) $<br>$ h_\theta(x) = a_1^{(3)} = g(\theta_{10}^{(2)}a_0^{(2)}+\theta_{11}^{(2)}a_1^{(2)}+\theta_{12}^{(2)}a_2^{(2)}+\theta_{13}^{(2)}a_3^{(2)})$</p><p>$\theta^{(j)}$是一个矩阵，关于它的维度：<br>在第$j$层，该层有$s_j$个单元，而第$j+1$层有$s_k$个单元，则$\theta^{(j)}$会是一个$s_k\times s_j + 1$维矩阵</p><p>举个例子，假如有一个三层的神经网络，第一层有2个输入特征值，第二层是3个单元，最后第三层输出1个预测结果：<br>$\theta^{(1)}$s是一个$3\times 3$的矩阵<br>$\theta^{(2)}$s是一个$1\times 4$的矩阵</p><p>为了方便，将激励函数中的参数用变量$z$替换：<br>$ a_0^{(2)} = 1 $<br>$ a_1^{(2)} = g(z_1^{(2)}) $<br>$ a_2^{(2)} = g(z_2^{(2)}) $<br>$ a_3^{(2)} = g(z_3^{(2)}) $</p><p>其中$z$为：<br>$ z_k^{(2)}= \theta_{k,0}^{(1)}x_0 + \theta_{k,1}^{(1)}x_1 + … + \theta_{k,n}^{(1)}x_n $</p><h3 id="向量形式"><a href="#向量形式" class="headerlink" title="向量形式"></a>向量形式</h3><p>用向量的形式来表示：</p><p>$x=\left[ \begin{matrix}<br>x_0 \\ x_1 \\ x_2 \\ x_3<br>\end{matrix} \right], z^{(2)}= \left[ \begin{matrix}<br>z_1^{(2)} \\ z_2^{(2)} \\ z_2^{(2)}<br>\end{matrix} \right]$</p><p>$z^{(2)} = \theta^{(1)}x$<br>$a^{(2)} = g(z^{(2)})$</p><p>第三层：</p><p>$z^{(3)} = \theta^{(2)}z^{(2)}$</p><p>类推到通常情况：<br>$z^{(j)} = \theta^{(j-1)}a^{(j-1)}$<br>$a^{(j)} = g(z^{(j)})$<br>最后一步<br>$h_\theta(x) = a^{(j+1)} = g(z^{(j+1)})$</p><h2 id="举例说明"><a href="#举例说明" class="headerlink" title="举例说明"></a>举例说明</h2><h3 id="1-预测“与”-AND"><a href="#1-预测“与”-AND" class="headerlink" title="1.预测“与” AND"></a>1.预测“与” AND</h3><p>$$ \left[ \begin{matrix}<br>x_0 \\ x_1 \\ x_2<br>\end{matrix} \right] → \left[ \begin{matrix}<br>g(z^{(2)})<br>\end{matrix} \right] → h_\theta(x) $$</p><p>其中$x_0 = 1$</p><p>我们要计算“与”，其中$x1,x2 \in [0,1]$，$y=x_1$ &amp;&amp; $x_2$<br>设置$\theta^{(1)} = \left[ \begin{matrix}<br>-30 &amp; 20 &amp; 20<br>\end{matrix} \right]$</p><p>通过上面的公式<br>$h_\theta(x) = \theta^{(1)}x = g(-30+20x_1+20x_2)$</p><p>$x_1=0,x_2=0$，$h_\theta(x)=g(-30)\approx 0$<br>$x_1=0,x_2=1$，$h_\theta(x)=g(-10)\approx 0$<br>$x_1=1,x_2=0$，$h_\theta(x)=g(-10)\approx 0$<br>$x_1=1,x_2=1$，$h_\theta(x)=g(10)\approx 1$</p><p>满足”与“的逻辑。</p><h3 id="2-预测“或”-OR"><a href="#2-预测“或”-OR" class="headerlink" title="2.预测“或” OR"></a>2.预测“或” OR</h3><p>与 预测“AND”的神经网络模型一样，我们只是调整一下：$\theta^{(1)} = \left[ \begin{matrix}<br>-10 &amp; 20 &amp; 20<br>\end{matrix} \right]$</p><p>$h_\theta(x) = \theta^{(1)}x = g(-10+20x_1+20x_2)$</p><p>$x_1=0,x_2=0$，$h_\theta(x)=g(-10)\approx 0$<br>$x_1=0,x_2=1$，$h_\theta(x)=g(10)\approx 1$<br>$x_1=1,x_2=0$，$h_\theta(x)=g(10)\approx 1$<br>$x_1=1,x_2=1$，$h_\theta(x)=g(30)\approx 1$</p><p>满足”或“的逻辑。</p><h3 id="3-预测“异或”-XOR"><a href="#3-预测“异或”-XOR" class="headerlink" title="3.预测“异或” XOR"></a>3.预测“异或” XOR</h3><p>$$ \left[ \begin{matrix}<br>x_0 \\ x_1 \\ x_2<br>\end{matrix} \right] → \left[ \begin{matrix}<br>a_0^{(2)} \\ a_1^{(2)} \\ a_2^{(2)}<br>\end{matrix} \right] → \left[ \begin{matrix}<br>g(z^{(3)})<br>\end{matrix} \right] → h_\theta(x) $$</p><p>其中$x_0=1,a_0^{(2)}=1$</p><p>$\theta^{(1)} = \left[ \begin{matrix}<br>-10 &amp; 20 &amp; 20 \\ 40 &amp; -30 &amp; -30<br>\end{matrix} \right], \theta^{(2)} = \left[ \begin{matrix}<br>-30 &amp; 20 &amp; 20<br>\end{matrix} \right] $</p><p>$a^{(2)} = \theta^{(1)}x$，$h_\theta(x)=a^{(3)}=\theta^{(2)}a^{(2)}$</p><p>$x_1=0,x_2=0$，$a^{(2)}=\left[ \begin{matrix}<br>1 \\ g(-10)\\ g(40)<br>\end{matrix} \right]\approx\left[ \begin{matrix}<br>1 \\ 0 \\ 1<br>\end{matrix} \right]$，$h_\theta(x)=g(-10)\approx 0$</p><p>$x_1=0,x_2=1$，$a^{(2)}=\left[ \begin{matrix}<br>1 \\ g(10) \\ g(10)<br>\end{matrix} \right]\approx\left[ \begin{matrix}<br>1 \\ 1 \\ 1<br>\end{matrix} \right]$，$h_\theta(x)=g(10)\approx 1$</p><p>$x_1=1,x_2=0$，$a^{(2)}=\left[ \begin{matrix}<br>1 \\ g(10) \\ g(10)<br>\end{matrix} \right]\approx\left[ \begin{matrix}<br>1 \\ 1 \\ 1<br>\end{matrix} \right]$，$h_\theta(x)=g(10)\approx 1$</p><p>$x_1=1,x_2=1$，$a^{(2)}=\left[ \begin{matrix}<br>1 \\ g(30) \\ g(-20)<br>\end{matrix} \right]\approx\left[ \begin{matrix}<br>1 \\ 1 \\ 0<br>\end{matrix} \right]$，$h_\theta(x)=g(-10)\approx 0$</p><p>这符合“异或”的逻辑。</p>]]></content>
    
    <summary type="html">
    
      在解决分类问题时，可以用逻辑回归算法，但当解决复杂的非线性分类器时，这并不是一个好的选择。如果用逻辑回归来解决，首先要构造一个包含很多非线性项的逻辑回归函数。使用逻辑回归会构造一个s型函数$g$。当多项式足够多，足够复杂，会有一个非常扭曲的决策边界。这可能会出现过拟合的情况。
    
    </summary>
    
    
      <category term="技术" scheme="https://codeeper.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="机器学习" scheme="https://codeeper.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="https://codeeper.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>分类算法中多元分类</title>
    <link href="https://codeeper.com/2020/01/15/tech/machine_learning/one_vs_all.html"/>
    <id>https://codeeper.com/2020/01/15/tech/machine_learning/one_vs_all.html</id>
    <published>2020-01-15T12:00:00.000Z</published>
    <updated>2020-02-14T05:45:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>判断垃圾邮件，这个分类问题的输出是[0, 1]，只有固定的两个输出值，这称之为二元分类问题。<br>在生活中有许多另外的分类问题，给定一张数字的图像，判断是数字几；自动将邮件归类，归为工作，朋友，家人等等，这就是一个多元分类问题了。</p><p>原来的二元分类$y\in[0,1]$，多元分类就需要扩展一下$y\in[1,2,3,…,n]$</p><h2 id="问题说明"><a href="#问题说明" class="headerlink" title="问题说明"></a>问题说明</h2><p>假如我们有一组训练集$X$，输出有三项$y\in[1,2,3]$，我们可以将问题拆分成3个二元问题，在每一个问题中，预测$y$的输出值的概率；<br>即先预测为1的概率，再预测为2的概率，再预测为3的概率，比较三个概率的大小，取其中最大的概率，就是$y$最可能的输出值。</p><p>再一般化一点：<br>$y\in[1,2,3,…,n]$<br>y有n个输出，所以要分成n个二元问题，n个预测函数：<br>$h_\theta^{(1)}(x)=P(y=1|x; \theta)$ ；y=1的概率<br>$h_\theta^{(2)}(x)=P(y=2|x; \theta)$ ；y=2的概率<br>…<br>$h_\theta^{(n)}(x)=P(y=n|x; \theta)$ ；y=n的概率<br>然后比较这n个预测函数的输出值，取其最大概率的一个。</p><h3 id="多元问题的输出"><a href="#多元问题的输出" class="headerlink" title="多元问题的输出"></a>多元问题的输出</h3><p>在实际应用上，多元问题的输出并不是一个值，而是一个向量。<br>例如：$y\in[1,2,3]$，$y$的输出值有三个可能，因此，$y$的输出是一个三维的向量$\left[ \begin{matrix} y_1 \\ y_2 \\ y_3 \end{matrix} \right]$，其中$y_1,y_2,y_3\in[0,1]$，向量的三个值都是取0和1这两个值。<br>比如说$y=\left[ \begin{matrix} 1 \\ 0 \\ 0 \end{matrix} \right]$，就是说明y最大概率的值是1；$y=\left[ \begin{matrix} 0 \\ 0 \\ 1 \end{matrix} \right]$，说明最大概率的值是3。</p><h3 id="举例说明"><a href="#举例说明" class="headerlink" title="举例说明"></a>举例说明</h3><p>在处理多元问题时，处理训练集的输出$y$时，应当将输出的实际标量转换成向量的形式：<br>例如，$y\in[1,2,3]$，$y$在1,2,3中选择。<br>我们有5个样本集，忽略输入，观察到输出为：<br>$y=\left[ \begin{matrix}<br>2 \\ 1 \\ 1 \\ 3 \\ 2<br>\end{matrix} \right]$<br>要方便计算，将向量的各个值在转换一下：<br>$y=\left[ \begin{matrix}<br>0&amp;1&amp;0 \\ 1&amp;0&amp;0 \\ 1&amp;0&amp;0 \\ 0&amp;0&amp;1 \\ 0&amp;1&amp;0<br>\end{matrix} \right]$<br>可以看出原来是一个五维的向量，每个值是标量，经过转换后每一个标量都转成一个向量，形成的一个 $5\times3$ 的矩阵，可以看到，第一行第二列的元素为1，其他为0，对应这原来y内的第一个标量2。以此类推，一一对应。</p><p>这是训练集的输出，再看一下预测值的处理，经过分类算法的计算，得出的5个样本的预测函数值为：<br>$h_\theta(x)=\left[ \begin{matrix}<br>0.2&amp;0.9&amp;0.1 \\ 0.78&amp;0.13&amp;0.3 \\ 0.88&amp;0.3&amp;0.2 \\ 0.2&amp;0.1&amp;0.92 \\ 0.2&amp;0.98&amp;0.1<br>\end{matrix} \right]$<br>比较这个矩阵的每一行中三列，这三列就是三个预测函数的预测值。比较后记录下最大的值是第几列：<br>$h_\theta(x)=\left[ \begin{matrix}<br>2 \\ 1 \\ 1 \\ 3 \\ 2<br>\end{matrix} \right]$<br>这样就训练好了一个多元分类的样本集。</p>]]></content>
    
    <summary type="html">
    
      判断垃圾邮件，这个分类问题的输出是[0, 1]，只有固定的两个输出值，这称之为二元分类问题。在生活中有许多另外的分类问题，给定一张数字的图像，判断是数字几；自动将邮件归类，归为工作，朋友，家人等等，这就是一个多元分类问题了。
    
    </summary>
    
    
      <category term="技术" scheme="https://codeeper.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="机器学习" scheme="https://codeeper.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="非监督学习" scheme="https://codeeper.com/tags/%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>过拟合（Overfitting）和正则化（Regularized）</title>
    <link href="https://codeeper.com/2020/01/14/tech/machine_learning/overfitting_and_regularized.html"/>
    <id>https://codeeper.com/2020/01/14/tech/machine_learning/overfitting_and_regularized.html</id>
    <published>2020-01-14T11:00:00.000Z</published>
    <updated>2020-02-14T05:50:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>在应用线性回归和逻辑回归去解决某些机器学习的实际应用时，它们可能会出现一种叫做“过度拟合”（Overfitting）的问题。这会使得学习的结果很糟糕。</p><h2 id="什么是过拟合"><a href="#什么是过拟合" class="headerlink" title="什么是过拟合"></a>什么是过拟合</h2><p>用预测房价的线性回归的例子：</p><p><img src="/images/ml_15.jpg" alt="线性回归图表"></p><p>最左边一张图片用一条直线来拟合数据，这种情况下随着x轴的变大，房价增长的幅度一直不变，这样的结果误差很大，这意味这该算法没有很好的拟合训练数据，<br>这种称之为 <strong>“欠拟合”</strong>；</p><p>中间一张图用的二次曲线来拟合数据，拟合情况良好，稍有偏差；</p><p>右边一张图用了四次项，拟合的更好，在图中，它贯穿了所有样本，但是它的曲线很扭曲，这不是一个很好的预测房价的模型，这称之为 <strong>“过拟合”</strong>；</p><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p><strong>过拟合：</strong> 假如有很多特征值，且学习算法能够很好的拟合训练集，但是在新的样本上却表现的性能很差。</p><p>数据拟合有两个极端，当预测函数过于简单或者使用的相关特征值过少，就会出现<strong>欠拟合（高偏差high bias）</strong>的情况。<br>另一个极端，当函数过于复杂，使用的非常多的特征值，使得学习算法在训练样本上非常适合，但是不能推广到新的样本集上，这就是<strong>过拟合（高方差high variance）</strong></p><p>过拟合问题除了出现在线性回归问题上，也会出现在逻辑回归上。</p><h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><ol><li>减少特征值的数量<br>可以通过观察手动删掉一些特征值，或者用模型选择算法来达到目的。</li><li>正则化<br>保留所有特征值，但是减少参数$\theta_j$的大小，因为似乎每个特征值都或多或少的在预测函数起了作用。</li></ol><p>这两种方法的本质类似，就是弱化不必要的特征值，从而解决过拟合。</p><h2 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h2><p>假设线性回归的预测函数为：<br>$$ h(\theta) = \theta_0+\theta_1x+\theta_2x^2+\theta_3x^3+\theta_4x^4 $$</p><p>这是一个四次项的公式，通过上面的可以知道这是一个过拟合的预测函数，需要解决过拟合，需要忽略后面的三次和四次项，这要改一下代价函数：</p><p>$$ J(\theta) = \frac{1}{2m} \sum_{i=0}^m(h_\theta(x^{(i)})-y^{(i)})^2 +1000\cdot \theta_3^2 +1000\cdot \theta_4^2 $$</p><p>在原油的代价函数上我们添加了$1000\cdot \theta_3^2$和$1000\cdot \theta_4^2$这两个，为了使代价函数尽量小，我们就需要让$\theta_3$和$\theta_4$尽量小，接近于零。</p><p>这样就给了一个正则化的思路。将关联小的特征值的参数趋向于0，使得预测函数从过拟合的状态修正过来。</p><p>正则化后的代价函数为：<br>$$ J(\theta)= \frac{1}{2m} \sum_{i=0}^m (h_\theta(x^{(i)})-y^{(i)})^2 + \lambda \sum_{j=1}^n\theta_j^2 ] $$</p><p>其中$\lambda$为正则化参数，为了平衡代价函数。$\lambda$需要选择合适的值。</p><p>可以看到，在代价函数后面添加了$\lambda \sum_{j=1}^n\theta_j^2$ 这一项。注意，这一项中的$j$ 是从1到m的，因为$x_0$固定为0，不需要修正，所以参数$\theta_0$，不需要添加进来。</p><h2 id="线性回归正则化"><a href="#线性回归正则化" class="headerlink" title="线性回归正则化"></a>线性回归正则化</h2><p>梯度下降算法：<br>重复 {<br>$$ \theta_0 = \theta_0-\alpha\frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})x_0^{(i)} $$<br>$$ \theta_j = \theta_j -\alpha [\frac{1}{m}\sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} +\frac{\lambda}{m}\theta_j ], j \in [1,2,3,…,n] $$<br>}</p><p>同样的$\theta_0$不需要修正</p><p>下面的式子可以稍作调整：<br>$$ \theta_j = \theta_j(1-\alpha\frac{\lambda}{m}) -\alpha\frac{1}{m}\sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}, j \in 1,2,3,…,n] $$</p><p>可以看出$\theta_j$是在原来梯度下降的基础上有减去了$\alpha\frac{\lambda}{m}\theta_j$</p><h2 id="逻辑回归正则化"><a href="#逻辑回归正则化" class="headerlink" title="逻辑回归正则化"></a>逻辑回归正则化</h2><p>代价函数：<br>$$ J(\theta)=-\frac{1}{m} \sum_{i=1}^m [y^{(i)} \log(h_\theta(x^{(i)})) + (1-y^{(i)}) \log(1-h_\theta(x^{(i)}))] + \frac{\lambda}{2m} \sum_{j=1}^n \theta_j^2 $$</p><p>同样的$\theta_0$不需要修正</p><p>它的梯度下降算法是：</p><p>重复 {<br>$$ \theta_0 = \theta_0-\alpha\frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})x_0^{(i)} $$<br>$$ \theta_j = \theta_j -\alpha [\frac{1}{m}\sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} +\frac{\lambda}{m}\theta_j ], j \in [1,2,3,…,n] $$<br>}</p><h2 id="正规方程（Normal-Equation）"><a href="#正规方程（Normal-Equation）" class="headerlink" title="正规方程（Normal Equation）"></a>正规方程（Normal Equation）</h2><p>在求解线性回归时，除了梯度下降，还有一种正规方程的方式直接求解出$\theta$。使用正规方程的也需要正则化。</p><p>正规方程的公式为：<br>$$ \theta=(X^TX)^{-1}X^Ty $$</p><p>需要作出修改：<br>$$ \theta=(X^TX + \lambda\cdot L)^{-1}X^Ty $$</p><p>其中L是一个$n+1$的方阵 $L=\left[ \begin{matrix}<br>0&amp;&amp;&amp;&amp; \\ &amp;1&amp;&amp;&amp; \\ &amp;&amp;1&amp;&amp; \\ &amp;&amp;&amp;…&amp; \\ &amp;&amp;&amp;&amp;1<br>\end{matrix} \right]$，L是一个对角线矩阵，对角线第一个元素为0。</p>]]></content>
    
    <summary type="html">
    
      了解什么是欠拟合和过拟合，以及使用正则化来解决过拟合问题，正则化后的的代价函数和梯度下降算法的使用。
    
    </summary>
    
    
      <category term="技术" scheme="https://codeeper.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="机器学习" scheme="https://codeeper.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="算法优化" scheme="https://codeeper.com/tags/%E7%AE%97%E6%B3%95%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>关于逻辑回归模型的梯度下降算法</title>
    <link href="https://codeeper.com/2020/01/11/tech/machine_learning/classification_gradient_descent.html"/>
    <id>https://codeeper.com/2020/01/11/tech/machine_learning/classification_gradient_descent.html</id>
    <published>2020-01-11T02:00:00.000Z</published>
    <updated>2020-02-14T05:43:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>逻辑回归的代价函数：</p><p>$$ J(\theta)=-\frac{1}{m} \sum_1^m [y^{(i)} \log(h_\theta(x^{(i)})) + (1-y^{(i)}) \log(1-h_\theta(x^{(i)}))]  $$</p><p>与线性回归一样，它的梯度下降算法类似：</p><p>重复直到$J(\theta)$收敛 {<br>$  \theta_j:=\theta_j−\alpha \frac{\partial}{\partial \theta_j} J(\theta) $<br>}</p><p>计算$\frac{\partial}{\partial \theta_j} J(\theta)$后会得到：<br>$$ \theta_j:=\theta_j−\alpha \frac{1}{m} \sum_{i=0}^m((h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}), j \epsilon \left(0, 1, 2,…,n\right) $$</p><p>计算后得到的和线性回归的看上去没有区别，但是两者的$h_\theta(x)$不同。<br>线性回归的是：$h_\theta(x) = \theta^Tx$<br>逻辑回归的是：$h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}} $</p><p>因为预测函数改变了，所以两者的梯度下降算法是不一样的。</p><h2 id="举例说明"><a href="#举例说明" class="headerlink" title="举例说明"></a>举例说明</h2><p>假设数据集有5个样本，每个样本有2个特征值，即$m=5,n=2$如下：</p><p>$$ X = \left[ \begin{matrix}<br>34.62 &amp; 78.02 \\ 30.29 &amp; 3.89 \\ 35.85 &amp; 72.9 \\ 60.18 &amp; 86.31 \\ 79.03 &amp; 75.34<br>\end{matrix} \right], y = \left[ \begin{matrix}<br>0 \\ 0 \\ 0 \\ 1 \\ 1<br>\end{matrix} \right]$$</p><p>初始化$\theta=\left[ \begin{matrix} 0\\0\\0 \end{matrix} \right]$，然后在输入矩阵加上一列$x_0=1$，<br>$ X = \left[ \begin{matrix}<br>1 &amp; 34.62 &amp; 78.02 \\ 1 &amp; 30.29 &amp; 3.89 \\ 1 &amp; 35.85 &amp; 72.9 \\ 1 &amp; 60.18 &amp; 86.31 \\ 1 &amp; 79.03 &amp; 75.34<br>\end{matrix} \right]$</p><p>先计算预测函数$h$：<br>$h=g(X\theta)$<br>$X\theta = \left[ \begin{matrix}<br>1 &amp; 34.62 &amp; 78.02 \\ 1 &amp; 30.29 &amp; 3.89 \\ 1 &amp; 35.85 &amp; 72.9 \\ 1 &amp; 60.18 &amp; 86.31 \\ 1 &amp; 79.03 &amp; 75.34<br>\end{matrix} \right] \left[ \begin{matrix}<br>0 \\ 0 \\ 0<br>\end{matrix} \right] = \left[ \begin{matrix}<br>0 \\ 0 \\ 0<br>\end{matrix} \right]$<br>代入$g$函数，可以的得到预测结果$h=\left[ \begin{matrix}<br>\frac{1}{1+e^0} \\ \frac{1}{1+e^0} \\ \frac{1}{1+e^0}\\ \frac{1}{1+e^0}\\ \frac{1}{1+e^0}<br>\end{matrix} \right]=\left[ \begin{matrix}<br>0.5 \\ 0.5 \\ 0.5 \\ 0.5 \\ 0.5<br>\end{matrix} \right]$</p><p>代入J的公式<br>$$ J(\theta)=\frac{1}{m} \cdot (-y^T \log(h) -(1-y)^T \log(1-h)) $$<br>此时代价函数$J$的值为：$\color{red}{0.69315}$</p><p>下面计算第一次梯度下降的梯度值，代入下面的公式<br>$$ \theta = \theta - \frac{\alpha}{m} X^T(h-y) $$</p><p>即：<br>$ \theta = \theta - \alpha\frac{1}{5} \cdot \left[ \begin{matrix}<br>1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\ 34.62 &amp; 30.29 &amp; 35.85 &amp; 60.18 &amp; 79.03 \\ 78.02 &amp; 3.89 &amp; 72.9 &amp; 86.31 &amp; 75.34<br>\end{matrix} \right] \left[ \begin{matrix}<br>0.5 - 0 \\ 0.5-0 \\ 0.5-0 \\ 0.5-1 \\ 0.5-1<br>\end{matrix} \right] = \left[ \begin{matrix}<br>0 \\ 0 \\ 0<br>\end{matrix} \right] - \alpha \cdot \left[ \begin{matrix}<br>0.1 \\ -3.846 \\ 3.317<br>\end{matrix} \right]$</p><p>假设$\alpha=0.01$，则$\theta=\left[ \begin{matrix}<br>0.001 \\ 0.03846 \\ -0.03317<br>\end{matrix} \right]$</p><p>再次计算代价函数$J$为：$\color{red}{0.51494}$</p><p>这个例子用矩阵和向量来进行了代价函数和梯度下降的计算。</p>]]></content>
    
    <summary type="html">
    
      了解逻辑回归的梯度下降算法和公式，以及用向量化的方式来求解。
    
    </summary>
    
    
      <category term="技术" scheme="https://codeeper.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="机器学习" scheme="https://codeeper.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="监督学习" scheme="https://codeeper.com/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="逻辑回归" scheme="https://codeeper.com/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
</feed>
